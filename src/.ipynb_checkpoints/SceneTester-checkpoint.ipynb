{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_FOLDER = \"/data/students_home/amoscatelli/Desktop/actionAnalysis\"\n",
    "SCENE_FOLDER_PATH = PROJECT_FOLDER + \"/scenes/\"\n",
    "SCENE_POSES_FOLDER_PATH = SCENE_FOLDER_PATH + \"poses/\"\n",
    "SCENE_DATASET_FOLDER_PATH = SCENE_FOLDER_PATH + \"dataset/\"\n",
    "INPUT_SCENE_FOLDER_PATH = SCENE_FOLDER_PATH + \"input_video/\"\n",
    "OUTPUT_SCENE_FOLDER_PATH = SCENE_FOLDER_PATH + \"output_video/\"\n",
    "INFERENCE_MODEL_FOLDER_PATH = SCENE_FOLDER_PATH + \"model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, isdir, join, exists\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# extracting the frames of the passed video file name in output_folder\n",
    "def extractFrames(filePath, output_folder, REQUESTED_FRAMES_PER_SECONDS = 9999999, rotate = 0):\n",
    "\n",
    "    # Opens the Video file\n",
    "    video = cv2.VideoCapture(filePath)\n",
    "\n",
    "    fps = round(video.get(cv2.CAP_PROP_FPS))\n",
    "    length = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width  = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(\"Original FPS: {}\".format(int(fps)))\n",
    "    print(\"Requested FPS: {}\".format(int(REQUESTED_FRAMES_PER_SECONDS)))\n",
    "    print(\"length: {} width: {} height: {}\".format(length, width, height))\n",
    "    frame_ratio_to_keep = max(round(fps/REQUESTED_FRAMES_PER_SECONDS),1)\n",
    "\n",
    "    print(\"pick 1 frame every {}\".format(int(frame_ratio_to_keep)))\n",
    "\n",
    "    try:\n",
    "        os.mkdir(output_folder)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    !rm $output_folder/* # cleaning output_folder\n",
    "\n",
    "    i = -1\n",
    "    while(video.isOpened()):\n",
    "        i+=1\n",
    "        ret, frame = video.read()\n",
    "        if(i%frame_ratio_to_keep!=0):\n",
    "            # print(\"DISCARDED\")\n",
    "            continue\n",
    "        # print(\"PASSED\")\n",
    "        if ret == False:\n",
    "            # print(\"FINISHED\")\n",
    "            break\n",
    "        outputFramePath = join(output_folder,\"frame{:04d}.jpg\".format(i))\n",
    "        cv2.imwrite(outputFramePath, frame)\n",
    "        if rotate > 0:\n",
    "            rotated = Image.open(outputFramePath).rotate(rotate)\n",
    "            rotated.save(outputFramePath)\n",
    "\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sceneFileList = [f for f in scandir(INPUT_SCENE_FOLDER_PATH)]\n",
    "# temp_folder = SCENE_FOLDER_PATH + \"temp_extraction_frames/\"\n",
    "# filePath = sceneFileList[5].path\n",
    "# filePath\n",
    "# # extractFrames(filePath, temp_folder)  \n",
    "# # # extractFrames(sceneFile.path,temp_folder,rotate=180)\n",
    "\n",
    "# # Opens the Video file\n",
    "# video = cv2.VideoCapture(filePath)\n",
    "\n",
    "# fps = round(video.get(cv2.CAP_PROP_FPS))\n",
    "# length = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# width  = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "# height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# print(\"Original FPS: {}\".format(int(fps)))\n",
    "# print(\"Requested FPS: {}\".format(int(REQUESTED_FRAMES_PER_SECONDS)))\n",
    "# print(\"length: {} width: {} height: {}\".format(length, width, height))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sceneFileList[5].path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting poses from scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0+cu100\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch, torchvision\n",
    "print(torch.__version__)\n",
    "# You may need to restart your runtime prior to this, to let your installation take effect\n",
    "# Some basic setup\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import listdir, scandir\n",
    "# sceneFileList = [f for f in scandir(INPUT_SCENE_FOLDER_PATH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sceneFile = sceneFileList[0]\n",
    "# sceneFile.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sceneFileList[0].path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_folder = SCENE_FOLDER_PATH + \"temp_extraction_frames/\"\n",
    "# framePaths = [f.path for f in os.scandir(temp_folder) if f.is_file() and f.path.endswith('.jpg')]\n",
    "# framePaths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### 0 / 19 - 1-8.mp4 ###\n",
      "#### 1 / 19 - 1-11.mp4 ###\n",
      "#### 2 / 19 - 1-7.mp4 ###\n",
      "#### 3 / 19 - 1-4.mp4 ###\n",
      "#### 4 / 19 - 1-3.mp4 ###\n",
      "#### 5 / 19 - 3-4.mov ###\n",
      "#### 6 / 19 - 1-5.mp4 ###\n",
      "#### 7 / 19 - 3-3.mov ###\n",
      "#### 8 / 19 - 2-1.mov ###\n",
      "#### 9 / 19 - 1-2.mp4 ###\n",
      "#### 10 / 19 - 1-9.mp4 ###\n",
      "#### 11 / 19 - 1-10.mp4 ###\n",
      "#### 12 / 19 - 1-1.mp4 ###\n",
      "#### 13 / 19 - 2-3.mov ###\n",
      "#### 14 / 19 - 3-1.mov ###\n",
      "#### 15 / 19 - 2-2.mov ###\n",
      "#### 16 / 19 - 3-2.mov ###\n",
      "#### 17 / 19 - 3-5.mov ###\n",
      "#### 18 / 19 - 1-6.mp4 ###\n"
     ]
    }
   ],
   "source": [
    "# import sys \n",
    "from os import listdir, scandir\n",
    "\n",
    "sceneFileList = [f for f in scandir(INPUT_SCENE_FOLDER_PATH)]\n",
    "\n",
    "#### PoseNet config #### \n",
    "poseNetModel = 101 #50, 75, 100, 101\n",
    "\n",
    "#### Detectron2 config #### \n",
    "cfg = get_cfg()\n",
    "detectron2ConfigName = \"/detectron2_repo/configs/COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml\"\n",
    "modelWeightName = \"detectron2://COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x/139686956/model_final_5ad38f.pkl\"\n",
    "# detectron2ConfigName = \"/detectron2_repo/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"\n",
    "# modelWeightsName = \"detectron2://COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x/137849621/model_final_a6e10b.pkl\"\n",
    "cfg.merge_from_file(PROJECT_FOLDER+detectron2ConfigName)\n",
    "cfg.MODEL.WEIGHTS = modelWeightName\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.4  # set threshold for this model\n",
    "\n",
    "predictor = DefaultPredictor(cfg) # Detectron2 predictor\n",
    "\n",
    "for idx_scene,sceneFile in enumerate(sceneFileList):\n",
    "    print(\"#### {} / {} - {} ###\".format(idx_scene+1,len(sceneFileList),sceneFile.name))    \n",
    "    video_points_folder_name = SCENE_POSES_FOLDER_PATH+sceneFile.name[:-4]\n",
    "    \n",
    "    if not exists(video_points_folder_name):\n",
    "        os.mkdir(video_points_folder_name)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "#     ######### Extracting frames #########\n",
    "    temp_folder = SCENE_FOLDER_PATH + \"temp_extraction_frames/\"\n",
    "    extractFrames(sceneFile.path,temp_folder,rotate=180) # extract the video frames in temp_folder\n",
    "    framePaths = [f.path for f in os.scandir(temp_folder) if f.is_file() and f.path.endswith('.jpg')]\n",
    "    \n",
    "    ######### POSENET #########\n",
    "    poseNetFolder = video_points_folder_name+\"/PoseNet-\"+str(poseNetModel)\n",
    "    if not exists(poseNetFolder):\n",
    "        os.mkdir(poseNetFolder)\n",
    "    #         !export CUDA_VISIBLE_DEVICES=$GPU_CARD && \n",
    "        !cd $PROJECT_FOLDER/posenet-python && python keyPointsLogger.py \\\n",
    "            --model $poseNetModel \\\n",
    "            --image_dir $temp_folder \\\n",
    "            --output_dir $poseNetFolder\n",
    "    else:\n",
    "        print(\"######  video already parsed with PoseNet #####\")\n",
    "\n",
    "    print(\"######  PoseNet done  #####\")\n",
    "\n",
    "\n",
    "    ######### Detectron2 #########\n",
    "    print(\"######  parsing with Detectron2.... #####\")\n",
    "    detectron2Model = detectron2ConfigName.split(\"/\")[-1][:-5]\n",
    "    detectron2Folder = video_points_folder_name+\"/Detectron2-\"+str(detectron2Model)\n",
    "    if not exists(detectron2Folder):\n",
    "        os.mkdir(detectron2Folder)\n",
    "        for frame_path in framePaths:\n",
    "            outputsFile = frame_path.split(\"/\")[-1][:-4]+\".pickle\" # e.g.: ./temp/frame0010.jpg --> frame0010.pickle\n",
    "            im = cv2.imread(frame_path)      \n",
    "            outputs = predictor(im)\n",
    "            with open(detectron2Folder+\"/\"+outputsFile, 'wb') as file_out:\n",
    "                pickle.dump(outputs, file_out, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        print(\"######  video already parsed with Detectron2 #####\")\n",
    "\n",
    "    print(\"######  Detectron2 done! #####\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def extractBestPosesAlongTheVideo(pose_scores_video, keypoint_coords_video, numberOfPosesToExtract):\n",
    "    numberOfDetectedPeople = max([np.count_nonzero(i) for i in pose_scores_video])\n",
    "    alignedCoords = np.zeros((len(pose_scores_video), numberOfDetectedPeople, 17, 2))\n",
    "    alignedPoseScores = np.zeros((len(pose_scores_video), numberOfDetectedPeople))\n",
    "    #     peopleLastCoords = keypoint_coords_video[0][:numberOfDetectedPeople]\n",
    "    peopleLastCoords = np.zeros((numberOfDetectedPeople,17,2))\n",
    "    for frame_idx, frame in enumerate(keypoint_coords_video):\n",
    "        coordDistances = [] #triplet list -> distance, id_new_coords, id_person\n",
    "        for newCoords_idx, newCoords in enumerate(frame):\n",
    "            if np.count_nonzero(newCoords) == 0: # skip all the coords which are just zeros\n",
    "                continue\n",
    "            for person_idx, personLastCoords in enumerate(peopleLastCoords):\n",
    "                coordinatesDistance = np.mean([distance.euclidean(newCoords[i],personLastCoords[i]) \n",
    "                                           for i in range(len(newCoords))\n",
    "                                           if np.count_nonzero(newCoords[i]) == 2 \n",
    "                                           and np.count_nonzero(personLastCoords[i]) == 2])\n",
    "                # in case of poses which are all 0's\n",
    "                if np.isnan(coordinatesDistance): \n",
    "                    coordinatesDistance = float(\"inf\")\n",
    "\n",
    "                coordDistances.append((coordinatesDistance,newCoords_idx,person_idx))\n",
    "\n",
    "        # sort distances from te closest to the farest\n",
    "        coordDistances.sort(key=lambda x : x[0]) \n",
    "\n",
    "        idNewCoords = [None for i in range(numberOfDetectedPeople)]\n",
    "        for distance_triplet in coordDistances:\n",
    "            person_idx = distance_triplet[2]\n",
    "            newCoords_idx = distance_triplet[1]\n",
    "            # check that the person doesn't have already a new coord and that the new coords are not already used  \n",
    "            if idNewCoords[person_idx] == None and newCoords_idx not in idNewCoords:\n",
    "                # assign new closest coords to people\n",
    "                idNewCoords[person_idx] = newCoords_idx\n",
    "                alignedCoords[frame_idx][person_idx] = frame[newCoords_idx]\n",
    "                peopleLastCoords[person_idx] = frame[newCoords_idx]\n",
    "                alignedPoseScores[frame_idx][person_idx] = pose_scores_video[frame_idx][newCoords_idx]\n",
    "\n",
    "\n",
    "        # for the people who didn't find a match in the new coordinates we add a bunch of zeros in the result\n",
    "        # without updating their last coordinates\n",
    "        for person_idx, value in enumerate(idNewCoords):\n",
    "            if value == None:\n",
    "                alignedCoords[frame_idx][person_idx] = np.zeros((17,2))\n",
    "                alignedPoseScores[frame_idx][person_idx] = 0\n",
    "\n",
    "\n",
    "    # select the best poses\n",
    "    scorePosesRank = np.mean(alignedPoseScores, axis=0)\n",
    "    posesIndexRank = scorePosesRank.argsort()[::-1]\n",
    "\n",
    "    isSecondPersonDetected = True\n",
    "    if numberOfPosesToExtract == 1:\n",
    "        bestPoseKeypointCoords = alignedCoords[:,posesIndexRank[0]]\n",
    "    elif numberOfPosesToExtract == 2:\n",
    "        if len(posesIndexRank) == 1:\n",
    "            isSecondPersonDetected = False\n",
    "            bestPoseKeypointCoords = np.concatenate((\n",
    "                                                    alignedCoords[:,posesIndexRank[0]], \n",
    "                                                    np.zeros( #concatenate it with a bunch of zeros whit the same shape\n",
    "                                                        alignedCoords[:,posesIndexRank[0]].shape\n",
    "                                                    )),axis=1)\n",
    "        else:\n",
    "            bestPoseKeypointCoords = np.concatenate((\n",
    "                                                    alignedCoords[:,posesIndexRank[0]], \n",
    "                                                    alignedCoords[:,posesIndexRank[1]]\n",
    "                                                    ),axis=1)\n",
    "    else:\n",
    "        raise Exception(\"So far, it's possible to extract maximum 2 people from the video.\") \n",
    "\n",
    "    return bestPoseKeypointCoords, isSecondPersonDetected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PoseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import scandir, listdir\n",
    "from os.path import basename\n",
    "\n",
    "def getPoseNetResult(inputFolder = \"/data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/poses/\", poseNetModelName = \"PoseNet-101\"):\n",
    "    print(\"retrieving PoseNet points from input folders...\")\n",
    "    videoFeaturesList = []\n",
    "    videoNameList = []\n",
    "\n",
    "    videoFoldersToAnalyse = [f.path for f in scandir(inputFolder) if isdir(join(inputFolder, f))]\n",
    "\n",
    "    print(\"found {} folders for {}\".format(len(videoFoldersToAnalyse), poseNetModelName))\n",
    "    for i,avf in enumerate(videoFoldersToAnalyse):\n",
    "        print(\"{}/{} - {}\".format(i+1, len(videoFoldersToAnalyse),basename(avf)))\n",
    "\n",
    "        poseNet_folder = avf + \"/\"+ poseNetModelName\n",
    "        pose_scores_video = []\n",
    "    #     keypoint_scores_video = [] # ignoring for now\n",
    "        keypoint_coords_video = []\n",
    "        framesInFolder = [f for f in listdir(poseNet_folder) if f.endswith(\".pickle\")]\n",
    "        framesInFolder.sort() # must be sorted to have the frame in the correct order   \n",
    "        for frame_file in framesInFolder:\n",
    "            with open(poseNet_folder+\"/\"+frame_file,'rb') as file_in:\n",
    "                pose_scores, keypoint_scores, keypoint_coords = pickle.load(file_in)\n",
    "            pose_scores_video.append(pose_scores)\n",
    "    #         keypoint_scores_video.append(keypoint_scores) # for now ignoring the keypoints Scores\n",
    "            keypoint_coords_video.append(keypoint_coords)\n",
    "\n",
    "#         numberOfPosesToExtract = 1 if target < 50 else 2\n",
    "        numberOfPosesToExtract = 2\n",
    "\n",
    "        bestPosesKeypointCoords, isSecondPersonDetected = extractBestPosesAlongTheVideo(pose_scores_video, \n",
    "                                                                keypoint_coords_video, \n",
    "                                                                numberOfPosesToExtract)\n",
    "\n",
    "        if not isSecondPersonDetected:\n",
    "            print(\"The second person is not recognized in the whole video:\", basename(avf))\n",
    "\n",
    "\n",
    "        videoFeaturesList.append(bestPosesKeypointCoords)\n",
    "        videoNameList.append(basename(avf))\n",
    "\n",
    "\n",
    "    # converting to ndarray\n",
    "    videoFeatures = np.asarray(videoFeaturesList)\n",
    "    videoNames = np.asarray(videoNameList)\n",
    "\n",
    "    # final assertion (just to be sure)\n",
    "    assert len(videoFeatures) == len(videoNames)\n",
    "\n",
    "    return videoFeatures,videoNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/poses/3-4'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputFolder = \"/data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/poses/\"\n",
    "# [f.path for f in scandir(inputFolder) if isdir(join(inputFolder, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetectronResult(inputFolder = \"/data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/poses/\", detectron2ModelName = \"keypoint_rcnn_X_101_32x8d_FPN_3x\"):\n",
    "    print(\"retrieving detectron points from input folders...\")\n",
    "    videoFeaturesList = []\n",
    "    videoNameList = []\n",
    "\n",
    "    videoFoldersToAnalyse = [f.path for f in scandir(inputFolder) if isdir(join(inputFolder, f))]\n",
    "\n",
    "    print(\"found {} folders for {}\".format(len(videoFoldersToAnalyse), detectron2ModelName))\n",
    "\n",
    "    for i, avf in enumerate(videoFoldersToAnalyse):\n",
    "        print(\"{}/{} - {}\".format(i+1, len(videoFoldersToAnalyse), basename(avf)))\n",
    "\n",
    "        detectron2_folder = avf + \"/Detectron2-\"+ detectron2ModelName\n",
    "        framesInFolder = [f for f in listdir(detectron2_folder) if f.endswith(\".pickle\")]\n",
    "        framesInFolder.sort() # must be sorted to have the frame in the correct order   \n",
    "        keypoint_coords_video = []\n",
    "        pose_scores_video = []\n",
    "\n",
    "        for fIdx,frame_file in enumerate(framesInFolder):\n",
    "            with open(detectron2_folder+\"/\"+frame_file,'rb') as file_in:\n",
    "                outputsRead = pickle.load(file_in)\n",
    "\n",
    "            if len(outputsRead['instances']) == 0:\n",
    "                # in case of no pose is detected we fill the features with zeros\n",
    "                keypoints_frame = np.zeros((17,2))\n",
    "                score_poses = [0]\n",
    "                print(\" for video {}-{} the frame {} is filled with zeros\".format(i,basename(avf),fIdx))\n",
    "            else:\n",
    "                instancesDict = outputsRead['instances'].get_fields()\n",
    "                keypoints_frame = np.array(instancesDict['pred_keypoints'].cpu()) \n",
    "                keypoints_frame = np.delete(keypoints_frame, 2, 2) #removing the third value of each keypoint (which I still don't understand what it means)\n",
    "                score_poses = np.array(instancesDict['scores'].cpu()) \n",
    "\n",
    "            keypoint_coords_video.append(keypoints_frame)\n",
    "            pose_scores_video.append(score_poses)\n",
    "\n",
    "\n",
    "#         numberOfPosesToExtract = 1 if target < 50 else 2\n",
    "        numberOfPosesToExtract = 2\n",
    "        \n",
    "        bestPoses, isSecondPersonDetected = extractBestPosesAlongTheVideo(\n",
    "            pose_scores_video, keypoint_coords_video, numberOfPosesToExtract\n",
    "        )\n",
    "        \n",
    "        if not isSecondPersonDetected:\n",
    "            print(\"The second person is not recognized in the whole video:\", basename(avf))\n",
    "            \n",
    "\n",
    "        videoFeaturesList.append(bestPoses)\n",
    "        videoNameList.append(basename(avf))\n",
    "\n",
    "    # converting to ndarray\n",
    "    videoFeatures = np.asarray(videoFeaturesList)\n",
    "    videoNames = np.asarray(videoNameList)\n",
    "\n",
    "    #final assertion (just to be sure)\n",
    "    assert len(videoFeatures) == len(videoNames)\n",
    "\n",
    "    return videoFeatures,videoNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detectron2_folder = \"/data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/poses/\"+ \"3-3\" + \"/Detectron2-\"+ \"keypoint_rcnn_X_101_32x8d_FPN_3x\"\n",
    "# framesInFolder = [f for f in listdir(detectron2_folder) if f.endswith(\".pickle\")]\n",
    "# framesInFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving detectron points from input folders...\n",
      "found 19 folders for keypoint_rcnn_X_101_32x8d_FPN_3x\n",
      "1/19 - 3-3\n",
      "2/19 - 3-1\n",
      "3/19 - 2-1\n",
      "4/19 - 1-5\n",
      "5/19 - 2-3\n",
      "6/19 - 1-10\n",
      "7/19 - 3-2\n",
      "8/19 - 1-9\n",
      "9/19 - 2-2\n",
      "10/19 - 1-8\n",
      "11/19 - 1-3\n",
      "12/19 - 3-5\n",
      "13/19 - 1-1\n",
      "14/19 - 1-4\n",
      "15/19 - 1-11\n",
      "16/19 - 1-2\n",
      "17/19 - 3-4\n",
      "18/19 - 1-7\n",
      "19/19 - 1-6\n",
      "Dumping the result in  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "retrieving PoseNet points from input folders...\n",
      "found 19 folders for PoseNet-101\n",
      "1/19 - 3-3\n",
      "2/19 - 3-1\n",
      "3/19 - 2-1\n",
      "4/19 - 1-5\n",
      "5/19 - 2-3\n",
      "6/19 - 1-10\n",
      "7/19 - 3-2\n",
      "8/19 - 1-9\n",
      "9/19 - 2-2\n",
      "10/19 - 1-8\n",
      "11/19 - 1-3\n",
      "12/19 - 3-5\n",
      "13/19 - 1-1\n",
      "14/19 - 1-4\n",
      "15/19 - 1-11\n",
      "16/19 - 1-2\n",
      "17/19 - 3-4\n",
      "18/19 - 1-7\n",
      "19/19 - 1-6\n",
      "Dumping the result in  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/PoseNet-101-SCENES-dataset.pickle\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "modelNames = [\"keypoint_rcnn_X_101_32x8d_FPN_3x\"\n",
    "              ,\"PoseNet-101\"\n",
    "             ]\n",
    "for modelName in modelNames:\n",
    "    if modelName == \"PoseNet-101\":\n",
    "        videoFeatures,videoNames = getPoseNetResult()\n",
    "    elif modelName == \"keypoint_rcnn_X_101_32x8d_FPN_3x\":\n",
    "        videoFeatures,videoNames = getDetectronResult()\n",
    "    else:\n",
    "        raise Exception(\"model name {} unknown\".format(modelName)) \n",
    "        \n",
    "    datasetResultName = SCENE_DATASET_FOLDER_PATH+modelName+\"-SCENES-dataset.pickle\"\n",
    "    print(\"Dumping the result in \",datasetResultName)\n",
    "    with open(datasetResultName, 'wb') as handle:\n",
    "        pickle.dump((videoFeatures,videoNames), handle, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Force first pose to be always filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(datasetToLoad,'rb') as file_in:\n",
    "#     readFeatures, readNames = pickle.load(file_in)\n",
    "    \n",
    "# for features in readFeatures:\n",
    "#     for frame in features:\n",
    "#         if np.count_nonzero(frame[0:17]) == 0:\n",
    "#             frame[0:17] = frame[17:34]\n",
    "#             frame[17:34] = np.zeros((17,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DirEntry '1-9.mp4'>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [f for f in scandir(INPUT_SCENE_FOLDER_PATH) if f.name == \"1-9.mp4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See poses on video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/PoseNet-101-SCENES-dataset.pickle\n",
      "Retrieving features for  1-9.mp4\n",
      "Original FPS: 20\n",
      "Requested FPS: 9999999\n",
      "length: 542 width: 1280 height: 720\n",
      "pick 1 frame every 1\n",
      "0/535\n",
      "30/535\n",
      "60/535\n",
      "90/535\n",
      "120/535\n",
      "150/535\n",
      "180/535\n",
      "210/535\n",
      "240/535\n",
      "270/535\n",
      "300/535\n",
      "330/535\n",
      "360/535\n",
      "390/535\n",
      "420/535\n",
      "450/535\n",
      "480/535\n",
      "510/535\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from os import scandir # listdir\n",
    "import matplotlib.pyplot as plt\n",
    "# from random import randrange\n",
    "# import pickle\n",
    "# import parse\n",
    "\n",
    "\n",
    "file_name = \"1-9.mp4\"\n",
    "ROTATE = 180\n",
    "\n",
    "sceneFile = [f for f in scandir(INPUT_SCENE_FOLDER_PATH) if f.name == file_name][0]\n",
    "# sceneFile = sceneFileList[0]\n",
    "\n",
    "# modelToLoad = \"keypoint_rcnn_X_101_32x8d_FPN_3x\"\n",
    "modelToLoad = \"PoseNet-101\"\n",
    "\n",
    "\n",
    "\n",
    "datasetToLoad = SCENE_DATASET_FOLDER_PATH + modelToLoad + \"-SCENES-dataset.pickle\"\n",
    "plt.rcParams[\"figure.figsize\"] = (19,15)\n",
    "\n",
    "print(\"Reading \", datasetToLoad)\n",
    "with open(datasetToLoad,'rb') as file_in:\n",
    "    readFeatures, readNames = pickle.load(file_in)\n",
    "\n",
    "\n",
    "print(\"Retrieving features for \", file_name)    \n",
    "idx = np.where(readNames == file_name[:-4])[0][0]\n",
    "features = readFeatures[idx]\n",
    "\n",
    "    \n",
    "# # Force first pose to be always filled\n",
    "# for frame in features:\n",
    "#     if np.count_nonzero(frame[0:17]) == 0:\n",
    "#         frame[0:17] = frame[17:34]\n",
    "#         frame[17:34] = np.zeros((17,2))\n",
    "\n",
    "# extracting the frames of the passed video \n",
    "tempFrameFolder = SCENE_FOLDER_PATH+\"tempDataVisualizerFrames/\"\n",
    "extractFrames(sceneFile.path,tempFrameFolder,rotate=ROTATE) \n",
    "    \n",
    "framesInFolder = [f.path for f in scandir(tempFrameFolder)]\n",
    "framesInFolder.sort()\n",
    "\n",
    "for i,framePath in enumerate(framesInFolder):\n",
    "    if i % 30 == 0:\n",
    "        print(\"{}/{}\".format(i,len(framesInFolder)))\n",
    "    plt.axis(\"off\")\n",
    "    im = plt.imread(framePath)\n",
    "    implot = plt.imshow(im)\n",
    "    \n",
    "    #read dataset for that picture\n",
    "    if \"PoseNet\" in datasetToLoad:\n",
    "        x = [x[1] for x in readFeatures[idx][i]]\n",
    "        y = [x[0] for x in readFeatures[idx][i]]\n",
    "    else:\n",
    "        x = [x[0] for x in readFeatures[idx][i]]\n",
    "        y = [x[1] for x in readFeatures[idx][i]]\n",
    "        \n",
    "    \n",
    "    categories = np.zeros(17).astype(int)\n",
    "    if len(x) == 34:  \n",
    "        categories = np.concatenate((categories,np.ones(17))).astype(int)\n",
    "\n",
    "    colormap = np.array(['r', 'b'])    \n",
    "    \n",
    "    plt.scatter(x=x, y=y, \n",
    "                c=colormap[categories], \n",
    "                s = 40)\n",
    "    \n",
    "#     plt.plot([70, 70], [100, 250], 'k-', lw=2) # draw skeleton lines\n",
    "  \n",
    "    plt.savefig(framePath, bbox_inches='tight')\n",
    "    plt.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [f for f in scandir(INPUT_SCENE_FOLDER_PATH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    " \n",
    "# tempFrameFolder = SCENE_FOLDER_PATH+\"tempDataVisualizerFrames/\"\n",
    "img_array = []\n",
    "frames = glob.glob(tempFrameFolder+\"*.jpg\")\n",
    "frames.sort()\n",
    "for filename in frames:\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)\n",
    "    \n",
    "tempFilePath = tempFrameFolder+\"tempVideo.mp4\"\n",
    "    \n",
    "# out = cv2.VideoWriter('project.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
    "out = cv2.VideoWriter(tempFilePath,cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
    " \n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()\n",
    "\n",
    "\n",
    "VideoFileClip(tempFilePath).ipython_display(width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build video with inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/data/students_home/amoscatelli/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/students_home/amoscatelli/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/students_home/amoscatelli/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/students_home/amoscatelli/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/students_home/amoscatelli/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/students_home/amoscatelli/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "#### LOADING MODEL #############################\n",
      "WARNING:tensorflow:From /data/students_home/amoscatelli/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "#### LOADING FUNCTIONS #############################\n",
      "#### 0/57 - 1-8.mp4 - 2 seconds\n",
      "Already done!\n",
      "#### 1/57 - 1-8.mp4 - 3 seconds\n",
      "Already done!\n",
      "#### 2/57 - 1-8.mp4 - 4 seconds\n",
      "Already done!\n",
      "#### 3/57 - 1-11.mp4 - 2 seconds\n",
      "Already done!\n",
      "#### 4/57 - 1-11.mp4 - 3 seconds\n",
      "Already done!\n",
      "#### 5/57 - 1-11.mp4 - 4 seconds\n",
      "Already done!\n",
      "#### 6/57 - 1-7.mp4 - 2 seconds\n",
      "Already done!\n",
      "#### 7/57 - 1-7.mp4 - 3 seconds\n",
      "Already done!\n",
      "#### 8/57 - 1-7.mp4 - 4 seconds\n",
      "Already done!\n",
      "#### 9/57 - 1-4.mp4 - 2 seconds\n",
      "Already done!\n",
      "#### 10/57 - 1-4.mp4 - 3 seconds\n",
      "Already done!\n",
      "#### 11/57 - 1-4.mp4 - 4 seconds\n",
      "Already done!\n",
      "#### 12/57 - 1-3.mp4 - 2 seconds\n",
      "Already done!\n",
      "#### 13/57 - 1-3.mp4 - 3 seconds\n",
      "Already done!\n",
      "#### 14/57 - 1-3.mp4 - 4 seconds\n",
      "Already done!\n",
      "#### 15/57 - 3-4.mov - 2 seconds\n",
      "Already done!\n",
      "#### 16/57 - 3-4.mov - 3 seconds\n",
      "Already done!\n",
      "#### 17/57 - 3-4.mov - 4 seconds\n",
      "Already done!\n",
      "#### 18/57 - 1-5.mp4 - 2 seconds\n",
      "Already done!\n",
      "#### 19/57 - 1-5.mp4 - 3 seconds\n",
      "Already done!\n",
      "#### 20/57 - 1-5.mp4 - 4 seconds\n",
      "Already done!\n",
      "#### 21/57 - 3-3.mov - 2 seconds\n",
      "Already done!\n",
      "#### 22/57 - 3-3.mov - 3 seconds\n",
      "Already done!\n",
      "#### 23/57 - 3-3.mov - 4 seconds\n",
      "###################   EXTRACTING FRAMES  #######################\n",
      "Original FPS: 30\n",
      "Requested FPS: 9999999\n",
      "length: 948 width: 1920 height: 1080\n",
      "pick 1 frame every 1\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  3-3.mov\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 29\n",
      "### SLICING_WINDOW_SIZE: 116\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 115, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 115, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (826, 115, 34, 2)\n",
      "test set zero elements (after padding): 11730 (0.18%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/941\n",
      "50/941\n",
      "100/941\n",
      "150/941\n",
      "200/941\n",
      "250/941\n",
      "300/941\n",
      "350/941\n",
      "400/941\n",
      "450/941\n",
      "500/941\n",
      "550/941\n",
      "600/941\n",
      "650/941\n",
      "700/941\n",
      "750/941\n",
      "800/941\n",
      "850/941\n",
      "900/941\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 24/57 - 2-1.mov - 2 seconds\n",
      "###################   EXTRACTING FRAMES  #######################\n",
      "Original FPS: 30\n",
      "Requested FPS: 9999999\n",
      "length: 846 width: 1920 height: 1080\n",
      "pick 1 frame every 1\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  2-1.mov\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 29\n",
      "### SLICING_WINDOW_SIZE: 58\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 57, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 57, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (783, 57, 34, 2)\n",
      "test set zero elements (after padding): 0 (0.00%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/840\n",
      "50/840\n",
      "100/840\n",
      "150/840\n",
      "200/840\n",
      "250/840\n",
      "300/840\n",
      "350/840\n",
      "400/840\n",
      "450/840\n",
      "500/840\n",
      "550/840\n",
      "600/840\n",
      "650/840\n",
      "700/840\n",
      "750/840\n",
      "800/840\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 25/57 - 2-1.mov - 3 seconds\n",
      "######  Frames already extracted  #######\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  2-1.mov\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 29\n",
      "### SLICING_WINDOW_SIZE: 87\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 86, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 86, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (754, 86, 34, 2)\n",
      "test set zero elements (after padding): 5848 (0.13%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/840\n",
      "50/840\n",
      "100/840\n",
      "150/840\n",
      "200/840\n",
      "250/840\n",
      "300/840\n",
      "350/840\n",
      "400/840\n",
      "450/840\n",
      "500/840\n",
      "550/840\n",
      "600/840\n",
      "650/840\n",
      "700/840\n",
      "750/840\n",
      "800/840\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 26/57 - 2-1.mov - 4 seconds\n",
      "######  Frames already extracted  #######\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  2-1.mov\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 29\n",
      "### SLICING_WINDOW_SIZE: 116\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 115, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 115, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (725, 115, 34, 2)\n",
      "test set zero elements (after padding): 3910 (0.07%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/840\n",
      "50/840\n",
      "100/840\n",
      "150/840\n",
      "200/840\n",
      "250/840\n",
      "300/840\n",
      "350/840\n",
      "400/840\n",
      "450/840\n",
      "500/840\n",
      "550/840\n",
      "600/840\n",
      "650/840\n",
      "700/840\n",
      "750/840\n",
      "800/840\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 27/57 - 1-2.mp4 - 2 seconds\n",
      "###################   EXTRACTING FRAMES  #######################\n",
      "Original FPS: 20\n",
      "Requested FPS: 9999999\n",
      "length: 655 width: 1280 height: 720\n",
      "pick 1 frame every 1\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  1-2.mp4\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 20\n",
      "### SLICING_WINDOW_SIZE: 40\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 39, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 39, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (615, 39, 34, 2)\n",
      "test set zero elements (after padding): 0 (0.00%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/654\n",
      "50/654\n",
      "100/654\n",
      "150/654\n",
      "200/654\n",
      "250/654\n",
      "300/654\n",
      "350/654\n",
      "400/654\n",
      "450/654\n",
      "500/654\n",
      "550/654\n",
      "600/654\n",
      "650/654\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 28/57 - 1-2.mp4 - 3 seconds\n",
      "######  Frames already extracted  #######\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  1-2.mp4\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 20\n",
      "### SLICING_WINDOW_SIZE: 60\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 59, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 59, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (595, 59, 34, 2)\n",
      "test set zero elements (after padding): 0 (0.00%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/654\n",
      "50/654\n",
      "100/654\n",
      "150/654\n",
      "200/654\n",
      "250/654\n",
      "300/654\n",
      "350/654\n",
      "400/654\n",
      "450/654\n",
      "500/654\n",
      "550/654\n",
      "600/654\n",
      "650/654\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 29/57 - 1-2.mp4 - 4 seconds\n",
      "######  Frames already extracted  #######\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  1-2.mp4\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 20\n",
      "### SLICING_WINDOW_SIZE: 80\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 79, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 79, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (575, 79, 34, 2)\n",
      "test set zero elements (after padding): 0 (0.00%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/654\n",
      "50/654\n",
      "100/654\n",
      "150/654\n",
      "200/654\n",
      "250/654\n",
      "300/654\n",
      "350/654\n",
      "400/654\n",
      "450/654\n",
      "500/654\n",
      "550/654\n",
      "600/654\n",
      "650/654\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 30/57 - 1-9.mp4 - 2 seconds\n",
      "###################   EXTRACTING FRAMES  #######################\n",
      "Original FPS: 20\n",
      "Requested FPS: 9999999\n",
      "length: 542 width: 1280 height: 720\n",
      "pick 1 frame every 1\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  1-9.mp4\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 20\n",
      "### SLICING_WINDOW_SIZE: 40\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 39, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 39, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (496, 39, 34, 2)\n",
      "test set zero elements (after padding): 1326 (0.10%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/535\n",
      "50/535\n",
      "100/535\n",
      "150/535\n",
      "200/535\n",
      "250/535\n",
      "300/535\n",
      "350/535\n",
      "400/535\n",
      "450/535\n",
      "500/535\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 31/57 - 1-9.mp4 - 3 seconds\n",
      "######  Frames already extracted  #######\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  1-9.mp4\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 20\n",
      "### SLICING_WINDOW_SIZE: 60\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 59, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 59, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (476, 59, 34, 2)\n",
      "test set zero elements (after padding): 0 (0.00%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/535\n",
      "50/535\n",
      "100/535\n",
      "150/535\n",
      "200/535\n",
      "250/535\n",
      "300/535\n",
      "350/535\n",
      "400/535\n",
      "450/535\n",
      "500/535\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 32/57 - 1-9.mp4 - 4 seconds\n",
      "Already done!\n",
      "#### 33/57 - 1-10.mp4 - 2 seconds\n",
      "###################   EXTRACTING FRAMES  #######################\n",
      "Original FPS: 20\n",
      "Requested FPS: 9999999\n",
      "length: 641 width: 1280 height: 720\n",
      "pick 1 frame every 1\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  1-10.mp4\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 20\n",
      "### SLICING_WINDOW_SIZE: 40\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 39, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 39, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (581, 39, 34, 2)\n",
      "test set zero elements (after padding): 2652 (0.17%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/620\n",
      "50/620\n",
      "100/620\n",
      "150/620\n",
      "200/620\n",
      "250/620\n",
      "300/620\n",
      "350/620\n",
      "400/620\n",
      "450/620\n",
      "500/620\n",
      "550/620\n",
      "600/620\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### 34/57 - 1-10.mp4 - 3 seconds\n",
      "######  Frames already extracted  #######\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  1-10.mp4\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 20\n",
      "### SLICING_WINDOW_SIZE: 60\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 59, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 59, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (561, 59, 34, 2)\n",
      "test set zero elements (after padding): 2006 (0.09%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/620\n",
      "50/620\n",
      "100/620\n",
      "150/620\n",
      "200/620\n",
      "250/620\n",
      "300/620\n",
      "350/620\n",
      "400/620\n",
      "450/620\n",
      "500/620\n",
      "550/620\n",
      "600/620\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 35/57 - 1-10.mp4 - 4 seconds\n",
      "######  Frames already extracted  #######\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  1-10.mp4\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 20\n",
      "### SLICING_WINDOW_SIZE: 80\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 79, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 79, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (541, 79, 34, 2)\n",
      "test set zero elements (after padding): 0 (0.00%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/620\n",
      "50/620\n",
      "100/620\n",
      "150/620\n",
      "200/620\n",
      "250/620\n",
      "300/620\n",
      "350/620\n",
      "400/620\n",
      "450/620\n",
      "500/620\n",
      "550/620\n",
      "600/620\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 36/57 - 1-1.mp4 - 2 seconds\n",
      "###################   EXTRACTING FRAMES  #######################\n",
      "Original FPS: 20\n",
      "Requested FPS: 9999999\n",
      "length: 655 width: 1280 height: 720\n",
      "pick 1 frame every 1\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  1-1.mp4\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 20\n",
      "### SLICING_WINDOW_SIZE: 40\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 39, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 39, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (591, 39, 34, 2)\n",
      "test set zero elements (after padding): 5304 (0.34%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/630\n",
      "50/630\n",
      "100/630\n",
      "150/630\n",
      "200/630\n",
      "250/630\n",
      "300/630\n",
      "350/630\n",
      "400/630\n",
      "450/630\n",
      "500/630\n",
      "550/630\n",
      "600/630\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 37/57 - 1-1.mp4 - 3 seconds\n",
      "######  Frames already extracted  #######\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  1-1.mp4\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 20\n",
      "### SLICING_WINDOW_SIZE: 60\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 59, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 59, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (571, 59, 34, 2)\n",
      "test set zero elements (after padding): 4012 (0.18%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/630\n",
      "50/630\n",
      "100/630\n",
      "150/630\n",
      "200/630\n",
      "250/630\n",
      "300/630\n",
      "350/630\n",
      "400/630\n",
      "450/630\n",
      "500/630\n",
      "550/630\n",
      "600/630\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 38/57 - 1-1.mp4 - 4 seconds\n",
      "######  Frames already extracted  #######\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  1-1.mp4\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 20\n",
      "### SLICING_WINDOW_SIZE: 80\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 79, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 79, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (551, 79, 34, 2)\n",
      "test set zero elements (after padding): 0 (0.00%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/630\n",
      "50/630\n",
      "100/630\n",
      "150/630\n",
      "200/630\n",
      "250/630\n",
      "300/630\n",
      "350/630\n",
      "400/630\n",
      "450/630\n",
      "500/630\n",
      "550/630\n",
      "600/630\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 39/57 - 2-3.mov - 2 seconds\n",
      "###################   EXTRACTING FRAMES  #######################\n",
      "Original FPS: 30\n",
      "Requested FPS: 9999999\n",
      "length: 965 width: 1920 height: 1080\n",
      "pick 1 frame every 1\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  2-3.mov\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 29\n",
      "### SLICING_WINDOW_SIZE: 58\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set shape: (2, 57, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 57, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (894, 57, 34, 2)\n",
      "test set zero elements (after padding): 1938 (0.06%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n",
      "0/951\n",
      "50/951\n",
      "100/951\n",
      "150/951\n",
      "200/951\n",
      "250/951\n",
      "300/951\n",
      "350/951\n",
      "400/951\n",
      "450/951\n",
      "500/951\n",
      "550/951\n",
      "600/951\n",
      "650/951\n",
      "700/951\n",
      "750/951\n",
      "800/951\n",
      "850/951\n",
      "900/951\n",
      "950/951\n",
      "############### SAVING VIDEO WITH INFERENCE #####################\n",
      "#### 40/57 - 2-3.mov - 3 seconds\n",
      "######  Frames already extracted  #######\n",
      "###################   LOADING DATASET  #######################\n",
      "Loading  /data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/dataset/keypoint_rcnn_X_101_32x8d_FPN_3x-SCENES-dataset.pickle\n",
      "Retrieving features for  2-3.mov\n",
      "###################   INFERENCE  #######################\n",
      "### FPS: 29\n",
      "### SLICING_WINDOW_SIZE: 87\n",
      "### predicting slices of video ###\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "classes order: ['0']\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "removing zeros from dataset\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "Adapting the data to the TOP-MIDDLE-BOTTOM centers of each video\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "normalising EACH VIDEO, considering x and y TOGETHER\n",
      "train set shape: (2, 86, 34, 2)\n",
      "train set zero elements (after padding): 0 (0.00%)\n",
      "val set shape: (2, 86, 34, 2)\n",
      "val set zero elements (after padding): 0 (0.00%)\n",
      "test set shape: (865, 86, 34, 2)\n",
      "test set zero elements (after padding): 0 (0.00%)\n",
      "################## PREPARING FRAMES WITH INFERENCE #######################\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9f3280969398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# cleaning the new video frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mnewVideoFrameFolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSCENE_FOLDER_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"tempDataVisualizerFrames/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm $newVideoFrameFolder*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2444\u001b[0m         \u001b[0;31m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m         \u001b[0;31m# Instead, we store the exit_code in user_ns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msystem_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawnb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pexpect-U\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Vanilla Pexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<pexpect factory incomplete>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_poll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_poll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         self.ptyproc = self._spawnpty(self.args, env=self.env,\n\u001b[0;32m--> 304\u001b[0;31m                                      cwd=self.cwd, **kwargs)\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptyproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_spawnpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mptyprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPtyProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/ptyprocess/ptyprocess.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_native_pty_fork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;31m# Use internal fork_pty, for Solaris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pty.py\u001b[0m in \u001b[0;36mfork\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmaster_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslave_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCHILD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Establish a new session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "from os import scandir\n",
    "import pickle\n",
    "import dill\n",
    "import numpy as np\n",
    "from keras.models import load_model \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import basename\n",
    "from PIL import ImageFont \n",
    "import glob\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "\n",
    "# SLICING_WINDOW_SIZE = 80 # frames\n",
    "\n",
    "modelToLoad = \"keypoint_rcnn_X_101_32x8d_FPN_3x\"\n",
    "# modelToLoad = \"PoseNet-101\"\n",
    "\n",
    "seconds_slice_values = [2,3,4]\n",
    "\n",
    "\n",
    "#################### PROCESS SETTINGS ###################################\n",
    "# label_order = ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', \n",
    "#                '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', \n",
    "#                '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', \n",
    "#                '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', \n",
    "#                '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', \n",
    "#                '6', '60', '7', '8', '9']\n",
    "    \n",
    "label_order = ['beve', 'applaude', 'legge', 'scrive', 'strappa un foglio', 'si veste', 'si spoglia', 'si infila le scarpe', 'si leva le scarpe', 'si mette gli occhiali', 'si leva gli occhiali', \n",
    "               'mangia', 'si mette il cappello', 'si leva il cappello', 'esulta', 'saluta', 'calcia qualcosa', 'mette qualcosa in tasca', 'salta su un piede', 'salta', 'telefona', 'gioca col telefono', \n",
    "               'si lava i denti', 'scrive sulla tastiera', 'indica qualcosa', 'si fa un selfie', 'controlla l\\'orologio', 'si strofina le mani', 'si inchina', 'squote la testa', 'si strofina la faccia', 'fa il saluto militare', 'unisce i palmi delle mani', \n",
    "               'si spazzola i capelli', 'mette le braccia ad X', 'starnutisce', 'barcolla', 'cade', 'si tocca la testa', 'si tocca la pancia', 'si tocca la schiena', 'si tocca il collo', 'ha la nausea', 'si sventola', \n",
    "               'fa cadere qualcosa', 'colpisce una persona', 'calcia una persona', 'spinge una persona', 'da una pacca sulla spalla', 'indica una persona', 'si abbracciano', 'da qualcosa ad una persona', 'tocca la tasca di qualcuno', 'si danno la mano', 'si avvicinano', \n",
    "               'raccoglie qualcosa', 'si allontanano', 'lancia qualcosa', 'si siede', 'si alza']\n",
    "\n",
    "modelLength = 300\n",
    "ROTATE = 180\n",
    "plt.rcParams[\"figure.figsize\"] = (19,15)\n",
    "actionFontSize = 40\n",
    "font = ImageFont.truetype('Pillow/Tests/fonts/Arial.ttf', actionFontSize)\n",
    "fontPlt = {'family' : 'Arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : actionFontSize}\n",
    "\n",
    "print(\"#### LOADING MODEL #############################\")\n",
    "modelToLoadPath = [f.path for f in scandir(INFERENCE_MODEL_FOLDER_PATH) if f.name.endswith(\".h5\")][0]\n",
    "modelToLoadAccessoriesPath = [f.path for f in scandir(INFERENCE_MODEL_FOLDER_PATH) if f.name.endswith(\".pickle\")][0]\n",
    "\n",
    "model = load_model(modelToLoadPath)\n",
    "with open(modelToLoadAccessoriesPath,'rb') as file_in:\n",
    "    accessories = pickle.load(file_in)\n",
    "\n",
    "print(\"#### LOADING FUNCTIONS #############################\")\n",
    "envFunctions = [dill.loads(x) for x in accessories[\"env_fun_DILL\"]]\n",
    "specificFunctionsList = [dill.loads(x) for x in accessories[\"spec_fun_DILL\"]]\n",
    "\n",
    "one_hot_encoding = envFunctions[0]\n",
    "normaliseBeforePadding = envFunctions[1]\n",
    "paddingTrainValTest = envFunctions[2]\n",
    "getClosestNonZeroCoordinate = envFunctions[3]\n",
    "removeZerosFromVideo = envFunctions[4]\n",
    "getZeroStatsForDataset = envFunctions[5]\n",
    "preprocessData = envFunctions[6]\n",
    "############################################\n",
    "\n",
    "        \n",
    "\n",
    "file_name_list = [f.name for f in scandir(INPUT_SCENE_FOLDER_PATH)]\n",
    "\n",
    "numberOfVideos = len(file_name_list)*len(seconds_slice_values)\n",
    "counter = 0\n",
    "for file_name in file_name_list:    \n",
    "    firstFrameExtraction = True    \n",
    "    for secondsSlice in seconds_slice_values:\n",
    "        print(\"#### {}/{} - {} - {} seconds\".format(counter,numberOfVideos,file_name,secondsSlice))\n",
    "        counter += 1\n",
    "        \n",
    "        outputAnalysedVideoPath = OUTPUT_SCENE_FOLDER_PATH+file_name[:-4]+\"_\"+str(secondsSlice)+\"s\"+file_name[-4:]\n",
    "        if exists(outputAnalysedVideoPath):\n",
    "            print(\"Already done!\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if firstFrameExtraction:\n",
    "            print(\"###################   EXTRACTING FRAMES  #######################\")\n",
    "            firstFrameExtraction = False\n",
    "            sceneFile = [f for f in scandir(INPUT_SCENE_FOLDER_PATH) if f.name == file_name][0]\n",
    "            # extracting the frames of the passed video \n",
    "            extractionFrameFolder = SCENE_FOLDER_PATH+\"temp_extraction_frames/\"\n",
    "        #     print(\"#### NOT EXTRACTING FRAMES!!! #####\")\n",
    "            extractFrames(sceneFile.path,extractionFrameFolder,rotate=ROTATE) \n",
    "            sceneFrames = [f.path for f in scandir(extractionFrameFolder)]\n",
    "            sceneFrames.sort()\n",
    "        else:\n",
    "            print(\"######  Frames already extracted  #######\")   \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"###################   LOADING DATASET  #######################\")\n",
    "        datasetToLoad = SCENE_DATASET_FOLDER_PATH + modelToLoad + \"-SCENES-dataset.pickle\"\n",
    "\n",
    "        print(\"Loading \", datasetToLoad)\n",
    "        with open(datasetToLoad,'rb') as file_in:\n",
    "            readFeatures, readNames = pickle.load(file_in)\n",
    "\n",
    "        # file_name = readNames[0]+\".mp4\"\n",
    "\n",
    "        print(\"Retrieving features for \", file_name)    \n",
    "        idx_dataset = np.where(readNames == file_name[:-4])[0][0]\n",
    "        features = readFeatures[idx_dataset]\n",
    "\n",
    "        # Force first pose to be always filled\n",
    "        # print(\"FORCE first pose to be never ZERO\")\n",
    "        # for frame in features:\n",
    "        #     if np.count_nonzero(frame[0:17]) == 0:\n",
    "        #         frame[0:17] = frame[17:34]\n",
    "        #         frame[17:34] = np.zeros((17,2))\n",
    "\n",
    "\n",
    "        print(\"###################   INFERENCE  #######################\")\n",
    "        \n",
    "        fps = 20 if file_name.endswith(\"mp4\") else 29\n",
    "        print(\"### FPS:\", fps)\n",
    "        SLICING_WINDOW_SIZE = fps*secondsSlice\n",
    "        print(\"### SLICING_WINDOW_SIZE:\", SLICING_WINDOW_SIZE)\n",
    "        \n",
    "        slicingSequences = list()\n",
    "\n",
    "        for i in range(SLICING_WINDOW_SIZE-1,len(features)):\n",
    "            slicingSequences.append(features[i+1-SLICING_WINDOW_SIZE:i])\n",
    "\n",
    "        slicingSequences = np.asarray(slicingSequences)\n",
    "\n",
    "        \n",
    "        # specificFunctionsList = specificFunctionsList[1:] # remove 'remove-0'\n",
    "\n",
    "\n",
    "        fakeTrainSet = [slicingSequences[0:2],[0]]\n",
    "        fakeValSet = [slicingSequences[0:2],[0]]\n",
    "        testSet = [slicingSequences,[0]]\n",
    "\n",
    "        print(\"### predicting slices of video ###\")\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = preprocessData(fakeTrainSet, fakeValSet, testSet, specificFunctionsList)\n",
    "\n",
    "        X_test = pad_sequences(X_test, maxlen=modelLength, dtype='float32', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "        y_test_pred = [label_order[i] for i in model.predict_classes(X_test)]\n",
    "#         y_test_pred_proba = [model.predict_proba(i) for i in X_test]\n",
    "\n",
    "        print(\"################## PREPARING FRAMES WITH INFERENCE #######################\")\n",
    "\n",
    "        # # Force first pose to be always filled\n",
    "        # for frame in features:\n",
    "        #     if np.count_nonzero(frame[0:17]) == 0:\n",
    "        #         frame[0:17] = frame[17:34]\n",
    "        #         frame[17:34] = np.zeros((17,2))\n",
    "\n",
    "\n",
    "        # cleaning the new video frames\n",
    "        newVideoFrameFolder = SCENE_FOLDER_PATH+\"tempDataVisualizerFrames/\"\n",
    "        !rm $newVideoFrameFolder*\n",
    "\n",
    "\n",
    "    #     singleFrameToAnalyse = 115\n",
    "        # for i,framePath in enumerate(sceneFrames[singleFrameToAnalyse-1:singleFrameToAnalyse]):\n",
    "        #     i=singleFrameToAnalyse\n",
    "        for idx_sceneFrame, framePath in enumerate(sceneFrames):\n",
    "            if idx_sceneFrame % 50 == 0:\n",
    "                print(\"{}/{}\".format(idx_sceneFrame,len(sceneFrames)))\n",
    "            plt.axis(\"off\")\n",
    "            im = plt.imread(framePath)\n",
    "            implot = plt.imshow(im)\n",
    "\n",
    "            #read dataset for that picture\n",
    "            if modelToLoad == \"PoseNet-101\":\n",
    "                x = [x[1] for x in readFeatures[idx_dataset][idx_sceneFrame]]\n",
    "                y = [x[0] for x in readFeatures[idx_dataset][idx_sceneFrame]]\n",
    "            else:\n",
    "                x = [x[0] for x in readFeatures[idx_dataset][idx_sceneFrame]]\n",
    "                y = [x[1] for x in readFeatures[idx_dataset][idx_sceneFrame]]\n",
    "\n",
    "\n",
    "            if np.count_nonzero(x[17:34]) == 0:\n",
    "                x = x[:17]\n",
    "                y = y[:17]\n",
    "\n",
    "            # to avoid to plot outside the frame\n",
    "            x = [max(p,7) for p in x]\n",
    "            y = [max(p,7) for p in y]\n",
    "            x = [min(p,im.shape[1]-7) for p in x]\n",
    "            y = [min(p,im.shape[0]-7) for p in y]\n",
    "\n",
    "            categories = np.zeros(17).astype(int)\n",
    "            if len(x) == 34:  \n",
    "                categories = np.concatenate((categories,np.ones(17))).astype(int)\n",
    "\n",
    "            colormap = np.array(['r', 'b'])    \n",
    "\n",
    "            plt.scatter(x=x, y=y, \n",
    "                        c=colormap[categories], \n",
    "                        s = 40)\n",
    "\n",
    "            message = \"         ...         \"\n",
    "            if idx_sceneFrame >= SLICING_WINDOW_SIZE-1:\n",
    "                message = y_test_pred[idx_sceneFrame+1-SLICING_WINDOW_SIZE]    \n",
    "\n",
    "            w, h = font.getsize(message)\n",
    "            (text_x, text_y) = (im.shape[1]*0.5-(w/2), im.shape[0]*0.9)\n",
    "\n",
    "            t =  plt.text(text_x, text_y, message, fontdict = fontPlt)\n",
    "            t.set_bbox(dict(facecolor='white', alpha=0.5, edgecolor='black')) \n",
    "        #     plt.show()\n",
    "\n",
    "        #     plt.plot([70, 70], [100, 250], 'k-', lw=2) # draw skeleton lines\n",
    "\n",
    "            newFramePath = newVideoFrameFolder+basename(framePath)\n",
    "            plt.savefig(newFramePath, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"############### SAVING VIDEO WITH INFERENCE #####################\")\n",
    "        img_array = []\n",
    "        frames = glob.glob(newVideoFrameFolder+\"*.jpg\")\n",
    "        frames.sort()\n",
    "        for idx_frame,filename in enumerate(frames):\n",
    "            img = cv2.imread(filename)\n",
    "            img_array.append(img)\n",
    "\n",
    "\n",
    "        height, width, layers = img.shape\n",
    "        size = (width,height)\n",
    "\n",
    "        out = cv2.VideoWriter(outputAnalysedVideoPath,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
    "\n",
    "        for i in range(len(img_array)):\n",
    "            out.write(img_array[i])\n",
    "        out.release()\n",
    "\n",
    "\n",
    "    #     VideoFileClip(tempFilePath).ipython_display(width=500)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "readyVideo = [f for f in scandir(OUTPUT_SCENE_FOLDER_PATH)]\n",
    "videoPath = OUTPUT_SCENE_FOLDER_PATH+readyVideo[0].name\n",
    "print(\"Loading\", videoPath)\n",
    "VideoFileClip(videoPath).ipython_display(width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(673, 673, 692)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_test_pred[i+1-SLICING_WINDOW_SIZE]\n",
    "i+1-SLICING_WINDOW_SIZE,len(y_test_pred),i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/output_video/3-4_1s.mov'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_SCENE_FOLDER_PATH+file_name[:-4]+\"_\"+str(secondsSlice)+\"s\"+file_name[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/output_video/3-4_1s.mov'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondsSlice = 1\n",
    "OUTPUT_SCENE_FOLDER_PATH+file_name[:-4]+\"_\"+str(secondsSlice)+\"s\"+file_name[-4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show inference on video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/data/students_home/amoscatelli/Desktop/actionAnalysis/scenes/tempDataVisualizerFrames/*': No such file or directory\n",
      "#### NOT EXTRACTING FRAMES!!! #####\n",
      "0/941\n",
      "30/941\n",
      "60/941\n",
      "90/941\n",
      "120/941\n",
      "150/941\n",
      "180/941\n",
      "210/941\n",
      "240/941\n",
      "270/941\n",
      "300/941\n",
      "330/941\n",
      "360/941\n",
      "390/941\n",
      "420/941\n",
      "450/941\n",
      "480/941\n",
      "510/941\n",
      "540/941\n",
      "570/941\n",
      "600/941\n",
      "630/941\n",
      "660/941\n",
      "690/941\n",
      "720/941\n",
      "750/941\n",
      "780/941\n",
      "810/941\n",
      "840/941\n",
      "870/941\n",
      "900/941\n",
      "930/941\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os.path import basename\n",
    "from PIL import ImageFont\n",
    "\n",
    "ROTATE = 180\n",
    "plt.rcParams[\"figure.figsize\"] = (19,15)\n",
    "\n",
    "# # Force first pose to be always filled\n",
    "# for frame in features:\n",
    "#     if np.count_nonzero(frame[0:17]) == 0:\n",
    "#         frame[0:17] = frame[17:34]\n",
    "#         frame[17:34] = np.zeros((17,2))\n",
    "\n",
    "\n",
    "# cleaning the new video frames\n",
    "newVideoFrameFolder = SCENE_FOLDER_PATH+\"tempDataVisualizerFrames/\"\n",
    "!rm $newVideoFrameFolder*\n",
    "\n",
    "\n",
    "sceneFile = [f for f in scandir(INPUT_SCENE_FOLDER_PATH) if f.name == file_name][0]\n",
    "# extracting the frames of the passed video \n",
    "extractionFrameFolder = SCENE_FOLDER_PATH+\"temp_extraction_frames/\"\n",
    "print(\"#### NOT EXTRACTING FRAMES!!! #####\")\n",
    "# extractFrames(sceneFile.path,extractionFrameFolder,rotate=ROTATE) \n",
    "    \n",
    "framesInFolder = [f.path for f in scandir(extractionFrameFolder)]\n",
    "framesInFolder.sort()\n",
    "\n",
    "actionFontSize = 40\n",
    "font = ImageFont.truetype('Pillow/Tests/fonts/Arial.ttf', actionFontSize)\n",
    "fontPlt = {'family' : 'Arial',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : actionFontSize}\n",
    "\n",
    "singleFrameToAnalyse = 115\n",
    "# for i,framePath in enumerate(framesInFolder[singleFrameToAnalyse-1:singleFrameToAnalyse]):\n",
    "#     i=singleFrameToAnalyse\n",
    "for i,framePath in enumerate(framesInFolder):\n",
    "    if i % 30 == 0:\n",
    "        print(\"{}/{}\".format(i,len(framesInFolder)))\n",
    "    plt.axis(\"off\")\n",
    "    im = plt.imread(framePath)\n",
    "    implot = plt.imshow(im)\n",
    "    \n",
    "    #read dataset for that picture\n",
    "    if modelToLoad == \"PoseNet-101\":\n",
    "        x = [x[1] for x in readFeatures[idx][i]]\n",
    "        y = [x[0] for x in readFeatures[idx][i]]\n",
    "    else:\n",
    "        x = [x[0] for x in readFeatures[idx][i]]\n",
    "        y = [x[1] for x in readFeatures[idx][i]]\n",
    "        \n",
    "\n",
    "    if np.count_nonzero(x[17:34]) == 0:\n",
    "        x = x[:17]\n",
    "        y = y[:17]\n",
    "        \n",
    "    # to avoid to plot outside the frame\n",
    "    x = [max(p,7) for p in x]\n",
    "    y = [max(p,7) for p in y]\n",
    "    x = [min(p,im.shape[1]-7) for p in x]\n",
    "    y = [min(p,im.shape[0]-7) for p in y]\n",
    "    \n",
    "    categories = np.zeros(17).astype(int)\n",
    "    if len(x) == 34:  \n",
    "        categories = np.concatenate((categories,np.ones(17))).astype(int)\n",
    "\n",
    "    colormap = np.array(['r', 'b'])    \n",
    "    \n",
    "    plt.scatter(x=x, y=y, \n",
    "                c=colormap[categories], \n",
    "                s = 40)\n",
    "    \n",
    "    message = \"         ...         \"\n",
    "    if i >= SLICING_WINDOW_SIZE-1:\n",
    "        message = y_test_pred[i+1-SLICING_WINDOW_SIZE]    \n",
    "    \n",
    "    w, h = font.getsize(message)\n",
    "    (text_x, text_y) = (im.shape[1]*0.5-(w/2), im.shape[0]*0.9)\n",
    "\n",
    "    t =  plt.text(text_x, text_y, message, fontdict = fontPlt)\n",
    "    t.set_bbox(dict(facecolor='white', alpha=0.5, edgecolor='black')) \n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.plot([70, 70], [100, 250], 'k-', lw=2) # draw skeleton lines\n",
    "    \n",
    "    newFramePath = newVideoFrameFolder+basename(framePath)\n",
    "    plt.savefig(newFramePath, bbox_inches='tight')\n",
    "    plt.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_FOLDER = \"/data/students_home/amoscatelli/Desktop/actionAnalysis\"\n",
    "# SCENE_FOLDER_PATH = PROJECT_FOLDER + \"/scenes/\"\n",
    "# SCENE_POSES_FOLDER_PATH = SCENE_FOLDER_PATH + \"poses/\"\n",
    "# SCENE_DATASET_FOLDER_PATH = SCENE_FOLDER_PATH + \"dataset/\"\n",
    "# INPUT_SCENE_FOLDER_PATH = SCENE_FOLDER_PATH + \"input_video/\"\n",
    "# OUTPUT_SCENE_FOLDER_PATH = SCENE_FOLDER_PATH + \"output_video/\"\n",
    "# INFERENCE_MODEL_FOLDER_PATH = SCENE_FOLDER_PATH + \"model/\"\n",
    "# newVideoFrameFolder = SCENE_FOLDER_PATH+\"tempDataVisualizerFrames/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    " \n",
    "    \n",
    "img_array = []\n",
    "frames = glob.glob(newVideoFrameFolder+\"*.jpg\")\n",
    "frames.sort()\n",
    "size = (0,0)\n",
    "for idx_frame,filename in enumerate(frames):\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "#     size = (max(width,size[0]),max(height,size[1]))\n",
    "    newSize = (width,height)\n",
    "    if size != newSize:\n",
    "        print(idx_frame,newSize)\n",
    "        size= newSize\n",
    "    img_array.append(img)\n",
    "    \n",
    "tempFilePath = newVideoFrameFolder+\"tempVideo.mp4\"\n",
    "\n",
    "size = (size[0]+1,size[1]+1)\n",
    "    \n",
    "out = cv2.VideoWriter(tempFilePath,cv2.VideoWriter_fourcc(*'DIVX'), 29, size)\n",
    " \n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()\n",
    "\n",
    "\n",
    "VideoFileClip(tempFilePath).ipython_display(width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
