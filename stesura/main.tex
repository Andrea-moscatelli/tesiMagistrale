\documentclass[a4paper,12pt,dvips]{thesis}

\usepackage[latin1]{inputenc}
\usepackage[italian]{babel}
\usepackage{amsfonts}
\usepackage{amssymb,amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{latexsym}

\newtheorem{theorem}{Teorema}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollario}[section]
\newtheorem{remark}{Osservazione}[section]
\newtheorem{definition}{Definizione}[section]

\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{makecell}
\usepackage{subcaption}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Macro algoritmi in un frame, con caption e label. Si usano cosi':



\newcommand{\algoritmo}{
\begin{figure}[htbp]%
\begin{center}%
\begin{framepage}{\textwidth}%
\singlespace\small%
\begin{tabbing}%
}

\newcommand{\finealgoritmo}[2]{
\end{tabbing}%
\end{framepage}%
\end{center}%
\caption{#2}%
\label{fi:#1}%
\end{figure}%
\noindent%
}

\newenvironment{chapterAbstract}
         {\singlespace \begin{quote}\begin{itshape}\begin{small}  \hugeinitial}
         {\newline \line(1,0){200} \end{small}\end{itshape}\end{quote} \onehalfspace}
                                           

\university{Firenze} \faculty{Scienze Matematiche Fisiche e Naturali} \dept{Informatica} \course{Informatica - Data Science}
\accademicyear{2018 - 2019} 
\supervisor{Marco Bertini}
%\supervisor{Secondo Supervisore}
\advisor{Correlatore 1}
\author{Andrea Moscatelli}
\title{Riconoscimento di azioni umane usando tecniche di apprendimento profondo per la stima della posa}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\def\conclusionname{Conclusioni}
%\def\conclusion{
%  \chapter*{\conclusionname
%        %\@mkboth{\uppercase{\conclusionname}}{\uppercase{\conclusionname}}
%        }%
%  \addcontentsline{toc}{chapter}{\conclusionname}%
%}



\begin{document}
\sffamily
\maketitle

\onehalfspace
\oddsidemargin  1.75cm 
\evensidemargin 1.75cm
\hyphenation{words}

\tableofcontents

\newpage

%\algoritmo
%1. \= iscriviti ad Ingegneria \\
%2. \> finch\a'e non ti stufi o non finisci gli esami \\
%   \> 2.1. \= prova a dare l' esame $i$ \\
%   \> 2.2. \> se superi l'esame $i$ \\
%   \>      \> 2.2.1. \= $i=i+1$ \\
%3. \> prepara la tesi oppure fattela fare (CEPU) \\
%4. \> scrivila in \LaTeX :) \\
%5. \> laureati
%\finealgoritmo{laurea}{Algoritmo per conseguire la Laurea in Ingegneria}
%
%
% ora si puo' fare riferimento all' algoritmo con \ref{fi:laurea}.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\preface{
prefazione
}

\introduction{
Introduzione
}


%% Deep learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{L'apprendimento automatico}
Quando parliamo di apprendimento automatico (in inglese \emph{machine learning}), ovvero \emph{l'apprendimento delle macchine}, viene spontaneo domandarsi ``Come impara una macchina?".

L'apprendimento viene definito come un processo iterativo che permette di mutare le proprie conoscenze a seconda delle informazioni che vengono raccolte.

Nel machine learning l'apprendimento segue esattamente le stesse dinamiche ma a seconda di come gestiamo le informazioni a nostra disposizione vengono definite 4 macro classi.

\begin{itemize}
\item \textbf{Apprendimento supervisionato (Supervised Learning)} - questo primo caso, può essere paragonato al processo di apprendimento che si avrebbe con un'insegnante \emph{onnisciente} che supervisiona il suo alunno e che interromperà l'apprendimento solo quando quest'ultimo raggiungerà un livello accettabile del compito da imparare.

L'apprendimento supervisionato lo si ha cioè quando nel nostro \emph{dataset} di allenamento sono a disposizione  sia i dati di input $X$ che quelli di output $Y$ e vogliamo insegnare al nostro algoritmo quella funzione $f$ tale che 
$$
Y= f(X).
$$

L'obbiettivo è quello di approssimare sufficientemente bene la funzione $f$ di modo da poter prevedere correttamente il valore $Y_i$ per un futuro $X_i$ non compreso nel nostro dataset di allenamento.

I problemi di apprendimento supervisionato si possono dividere in problemi di:
\begin{enumerate}
\item \emph{Classificazione}, ovvero insegnare ad una macchina a categorizzare. In questo scenario, nel nostro dataset di allenamento ad ogni dato di input sarà associata un'etichetta che ne indica la categoria appartenente. Più grande sarà il dataset e maggiore sarà l'informazione a disposizione dell'algoritmo per imparare.
\item \emph{Regressione} - ovvero insegnare ad una macchina a predire il valore di ciò che sta analizzando. A differenza della classificazione, in questo caso il risultato in output sarà un valore continuo e non un valore categorico. Ad esempio, dati in input le ore di studio di uno studente per la preparazione di un esame e le rispettive ore di sonno, prevedere la probabilità di superamento dell'esame in questione, oppure date in input la superficie e la posizione di un appartamento, prevederne il valore di mercato.
\end{enumerate}
\item \textbf{Apprendimento non supervisionato (Unsupervised Learning)} - in questo tipo di apprendimento, al contrario di quello supervisionato, non viene fornita nessuna etichetta di output $Y$ per i nostri dati $X$. L'obbiettivo che quindi ci si pone in questo tipo di situazione è scovare delle relazioni tra i dati analizzati. In questo caso non c'è nessun ``insegnante" a guidare l'apprendimento e non ci sono risposte corrette o sbagliate, l'algoritmo deve cercare di scoprire ``da solo" se ci sono delle strutture decifrabili nei dati. 

I problemi di apprendimento non supervisionato si possono dividere in:
\begin{enumerate}
\item \emph{Raggruppamento} - detto anche \emph{clustering}, si utilizzano quando è necessario raggruppare i dati che presentano caratteristiche simili. In questo caso l'algoritmo non fa uso di dati categorizzati, come visto in precedenza, ma estrae una regola di raggruppamento secondo caratteristiche che ricava dai dati stessi.
\item \emph{Associazione} - strettamente legata al \emph{Data Mining}, questa classe di problemi è utile in tutti i casi per i quali siamo interessati a scoprire regole induttive nei dati analizzati, ad esempio la tendenza nei consumatori di un certo supermercato ad acquistare il prodotto $A$ dopo aver acquistato il prodotto $B$. Ci si pone quindi l'obbiettivo di identificare schemi frequenti, associazioni, correlazioni o strutture casuali fra gli \emph{item} di un database relazionale cercando si scoprire le regole che predicono l'evento di un certo item in base agli eventi degli item ad esso legati.
\end{enumerate}


\item \textbf{Apprendimento parzialmente supervisionato (Semi-Supervised Learning)} - questo tipo di apprendimento è una sorta di via di mezzo dei primi due, ovvero un apprendimento utilizzato quando solo alcuni dei nostri dati in input $X$ sono etichettati, mentre la maggior parte di essi non lo è. 

In questa categoria di apprendimento ricadono la stragrande maggioranza dei problemi di machine learning e questo perchè etichettare dati è un processo lungo e costoso soprattutto in ambito \emph{Big Data}, campo ideale per il machine learning.

\item \textbf{Apprendimento con rinforzo (Reinforcement Learning)} - questa metodologia di apprendimento simula il processo di apprendimento umano per tentativi ed errori. L'algoritmo si adatta gradualmente tramite un sistema di valutazione basato su ricompense e penalità a seconda della decisione presa. L'obbiettivo è quello di evolvere l'algoritmo massimizzando le ricompense ricevute.
\end{itemize}
\section{Deep learning}
Il \emph{deep learning} (o \emph{apprendimento profondo} in italiano) è una delle tecniche di machine learning che ``insegna" ai computer una cosa estremamente naturale al cervello umano, ovvero \emph{imparare per esempi}. Il deep learning è la tecnologia chiave grazie alla quale abbiamo oggi automobili che si guidano da sole, riconoscimento e controllo vocale dei device, riconoscimento di tumori, traduzioni automatiche, colorazione automatica di vecchi filmati in bianco e nero e molte altre cose ritenute impossibili solo fino a pochi anni fa.

Nel deep learning un computer impara, ad esempio, un task di classificazione direttamente da immagini, testi e suoni e può raggiungere prestazioni allo stato dell'arte superando talvolta persino l'accuratezza umana. I modelli vengono allenati usando grandi dataset etichettati seguendo un'apprendimento di tipo supervisionato ed il sistema centrale sul quale è basato il deep learnig è la \emph{rete neurale}.

\section{Le reti neurali}
Nel campo dell'apprendimento automatico, una rete neurale (\emph{neural network}
o \emph{NN}) è un modello di calcolo ispirato al sistema di elaborazione delle informazioni
tipico del cervello degli esseri viventi nel quale tanti piccoli elementi base,
denominati \emph{neuroni} (figura \ref{fig:neurone}), sono interconnessi e collaborano tra loro per poter eseguire un determinato compito. 

\begin{figure}[htbp]
\includegraphics[width=0.95\textwidth]{imgs/comparazioneNeurone}
\caption{Un neurone biologico (sinistra) ed un neurone artificiale (destra) a confronto. La struttura del neurone artificiale si ispira molto a quella del neurone biologico. }
\label{fig:neurone}
\end{figure}

Queste reti costituiscono ad oggi la miglior soluzione per una vasta gamma di problemi grazie alla loro capacità di approssimazioni di pressoché qualsiasi funzione $f : \mathbb{R}^n \rightarrow  \mathbb{R}$, a patto di scegliere la giusta quantità di neuroni in fase di progettazione.

La struttura di una rete neurale si sviluppa per livelli ed ogni livello, eccetto quello di \emph{input} e quello di \emph{output}, viene chiamato \emph{hidden layer}. Ogni hidden layer può essere composto da un qualsiasi numero di neuroni, che prendono il nome di \emph{hidden units} e maggiore sarà il numero di hidden units, maggiore sarà la \emph{capacità espressiva} della rete, ovvero la capacità di approssimare correttamente una qualsiasi funzione. 

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{imgs/hiddenLayers}
\caption{Struttura di una rete neurale. Ogni livello della rete, eccetto il primo e l'ultimo, viene chiamato \emph{hidden layer} ed i neuroni che lo compongono prendono il nome di \emph{hidden units}. Magiore sarà il numero di hidden units e maggiore sarà la capacità espressiva della rete. }
\label{fig:hiddenLayer}
\end{figure}

Sono proprio gli hidden layer a dare \emph{profondità} alla rete ed è per questo che si parla sia di \emph{apprendimento profondo} che di \emph{reti neurali profonde} (o \emph{deep neural network} in inglese). 

Negli ultimi anni le deep neural network hanno avuto una diffusione sorprendente non solo grazie alla loro adattabilità ad una vasta gamma di problemi, ma anche alla rapida evoluzione che hanno avuto le GPU, che hanno reso parallelizzabile l'addestramento di queste reti.

I livelli che possiamo trovare all'interno di una rete neurale possono essere di diverso tipo, come diverse possono essere le connessioni dei neuroni fra un livello e l'altro. Sono proprio queste diversità a determinare il tipo di rete neurale e le più famose tipologie sono $w$:
\begin{itemize}
\item \textbf{fully connected network} - in questo tipo di reti ogni neurone è connesso con tutti quelli del livello precedente (come in figura \ref{fig:hiddenLayer}). Come si può facilmente intuire, il numero dei parametri (ovvero la capacità espressiva) della rete cresce esponenzialmente al crescere della sua struttura ed il rischio che si corre è quello di creare una rete talmente efficiente nell'ottimizzare la funzione generatrice dei dati di allenamento da essere troppo specifica per essa, non riuscendo a predire correttamente valori al di fuori di quel dataset. Questa problematica, ben nota nel mondo del deep learning, prende il nome di \emph{overfitting}.
\item \textbf{convolutional neural network (CNN)} - queste reti sono composte principalmente da \emph{layer convoluzionali}, ovvero layer nei quali ogni neurone è connesso solo ad una piccola regione localizzata del livello precedente (figura \ref{fig:convNetwork}). Questa particolare tipologia di connessione è estremamente efficiente nell'ambito della visione computazionale ed è in grado di \emph{sintetizzare} correttamente un'immagine senza perdita d'informazione. Questa sintetizzazione viene indicata in gergo col termine di \emph{features extraction}, ovvero l'estrazione delle caratteristiche principali.
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=1\textwidth]{imgs/convolutionalNetwork}
\caption{Un esempio di rete convoluzionale per l'analisi di un'immagine. \emph{Conv} = ``Convolution layer", moltiplicazione vettoriale della porzione di layer al quale ogni neurone è collegato; \emph{Pool} = ``Pooling layer", sintetizzazione della porzione di layer connessa; \emph{FC} = ``Fully connected layer"; \emph{Softmax} = operazione conclusiva per la decisione del valore di output.}
\label{fig:convNetwork}
\end{figure}

Le reti neurali sopra citate, seppur molto efficienti nei loro ambiti, presentano però un problema di \emph{memoria} ovvero, non sono state ideate per processare una serie temporale di dati. L'unica soluzione che hanno è quella di trattare tutta la serie temporale come un unico input e processarlo interamente in un unico ciclo. Questa primordiale soluzione non ha ovviamente preso campo vista la mancanza di generalità.

Esistono però delle reti neurali che, a differenza dalle precedenti, riescono ad analizzare una sequenza temporale in maniera più simile al ragionamento umano, ovvero mantenendo una memoria del ``passato" ed in base a questa adattare la valutazione dello stato corrente. Queste particolari reti vengono chiamate \emph{reti neurali ricorrenti} (o \emph{recurrent neural networks}).

\section{Le reti neurali ricorrenti}
Le reti neurali ricorrenti (\emph{recurrent neural network} o \emph{RNN}) sono una classe
di rete neurali artificiali caratterizzate da una struttura ciclica, ovvero una struttura dove i layer possono essere connessi con loro stessi o con layer precedenti (figura \ref{fig:reteRicorrente}). 
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{imgs/esempioRNN}
\caption{Un esempio di rete ricorrente con due hidden layers.}
\label{fig:reteRicorrente}
\end{figure}

Questa struttura ciclica permette di mantenere una sorta di \emph{stato di memoria} della rete utilizzato per processare ogni elemento della sequenza in input che si è rivelato ideale per compiti di analisi predittiva su sequenze temporali, quali possono essere ad esempio il riconoscimento della grafia, l'analisi di video, il riconoscimento vocale o l'analisi di testi. 

Per quanto riguarda l'addestramento di queste reti, esso
avviene con la stessa modalità delle reti neurali standard ovvero, i pesi dei neuroni che compongono la rete vengono regolati tramite un processo iterativo con lo scopo di minimizzare l'errore di classificazione prodotto sul dataset di allenamento. 

Tipicamente, l'algoritmo che regola l'apprendimento delle RNN viene chiamato \emph{Backpropagation Throw Time (BPTT)} e non è altro che la versione ``ricorrente" del classico \emph{backpropagation algorithm} usato per le reti naurali standard.

L'applicazione del BPTT può essere vista come l'applicazione del backpropagation algorithm allo ``srotolamento" della RNN (figura \ref{fig:srotolamentoRNN}).

\begin{figure}[htbp]
\centering
\includegraphics[width=1\textwidth]{imgs/srotolamentoRNN.jpg}
\caption{Esempio di ``srotolamento" di una rete ricorrente, ovvero la trasformazione da una rete ricorrente ad una combinazione equivalente di reti neurali semplici.}
\label{fig:srotolamentoRNN}
\end{figure}

La lunghezza delle sequenze che queste reti sono in grado di analizzare è teoricamente infinita, ma di fatto per sequenze troppo lunghe si presenta un problema legato alla precisione finita dei calcolatori, ovvero la \emph{scomparsa del gradiente} (o in inglese \emph{vanishing gradient problem}).

Il concetto alla base del backpropagation algorithm e infatti quello di aggiornare iterativamente ogni parametro del modello in maniera proporzionale alla derivata parziale della \emph{loss function} (ovvero la funzione d'errore di classificazione) rispetto al parametro stesso. 

Poiché ogni gradiente sarà un valore compreso fra 0 e 1 e l'aggiornamento dei parametri ai vari livelli verrà propagato tramite la \emph{regola della catena}, ovvero 
$$
\frac{\partial a}{\partial b} = \frac{ \partial a}{\partial c} \cdot \frac{\partial c}{\partial b},
$$
il prodotto dei gradienti decrescerà esponenzialmente al crescere della profondità della rete, col rischio di sparire se dovesse diventare più piccolo della precisione minima del calcolatore.

La profondità delle reti ricorrenti è strettamente legata alla lunghezza della sequenza analizzata e raggiunge velocemente un numero di livelli intrattabile per l'algoritmo di backpropagation, rendendole di fatto inutilizzabili per i casi nel mondo reale.

Per ovviare a questo problema, nel 1997 Sepp Hochreiter e Jürgen Schmidhuber proposero un nuovo tipo di rete ricorrente chiamato \emph{Long short-term memory (LSTM)} \cite{LSTM}.
\newpage
\section{Le reti LSTM}
\begin{figure}[htbp]
\centering
\includegraphics[width=1\textwidth]{imgs/lstmFull}
\caption{Esempio di ``srotolamento" di una rete LSTM. In questa figura ogni unità di rete ``A'' prende in input al tempo $t$ un elemento $X_t$ della sequenza temporale $X$ e restituisce uno stato in output $h_t$. \emph{tanh} e $\sigma$ sono le funzioni di tangente iperbolica e sigmoidale rispettivamente, mentre ``$+$" e ``$\times$" sono rispettivamente le operazioni vettoriali di somma e moltiplicazione.}
\label{fig:srotolamentoLSTM}
\end{figure}
Le reti Long Short-Term Memory, normalmente chiamate LSTM, sono delle particolari reti ricorrenti in grado, non solo di imparare le dipendenze \emph{a lunga distanza} presenti in una sequenza in input, ma anche di risolvere il problema della scomparsa del gradiente. 

Come ogni altra rete ricorrente anche per le LSTM è possibile srotolarne la struttura ed ottenere una catena di ``pezzi di rete" chiamati anche \emph{unità} (figura \ref{fig:srotolamentoLSTM}). Il funzionamento di un'unità ricorrente LSTM ruota tutto intorno ai concetti
di \emph{cell state} (figura \ref{fig:cellState}) e di \emph{gate} (figura \ref{fig:gate}), ovvero:
\begin{itemize}
\item cell state: funziona come un nastro trasportatore in grado di memorizzare informazioni, rendendole disponibili durante tutta l'analisi della sequenza;
\item gate: sono delle NN più piccole che stabiliscono, attraverso una funzione sigmoidale, quali informazioni possono continuare ad essere memorizzate nel cell state e quali invece devono essere ``dimenticate". 
\end{itemize}
\begin{figure}
\centering
\begin{subfigure}{.8\textwidth}
  \centering
  \includegraphics[width=.6\textwidth]{imgs/LSTMcellState}
  \caption{cell state}
  \label{fig:cellState}
\end{subfigure}%
\begin{subfigure}{.2\textwidth}
  \centering
  \includegraphics[width=.5\textwidth]{imgs/LSTMgate}
  \caption{gate}
  \label{fig:gate}
\end{subfigure}
\caption{I concetti chiave di una LSTM.}
\label{fig:LSTMcellStateAndGate}
\end{figure}
Esistono principalmente tre diverse tipologie di gate che regolano il flusso informativo all'interno di una unità LSTM: \emph{forget gate, input gate} e \emph{output gate}.\\

\textbf{Forget gate} -  Il forget gate (figura \ref{fig:forgetGate}) decide quali informazioni mantenere all'interno del cell state e quali invece saranno ``dimenticate". 

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth]{imgs/LSTMforgetGate}
\caption{Forget gate di una LSTM}
\label{fig:forgetGate}
\end{figure}

In formula, al $t$-esimo passo, il forget gate sarà
$$
f_t = \sigma(W_f \cdot [h_{t-1},x_t] + b_f)
$$

dove:
\begin{itemize}
\item $h_{t-1}$ è lo stato della LSTM al passo precedente
\item $x_t$ è il $t$-esimo elemento della sequenza in input
\item $W_f$ e $b_f$ sono rispettivamente la matrice dei pesi e il bias del forget gate
\item $\sigma$ è la funzione sigmoidale.
\end{itemize} 
\ \\
\textbf{Input gate} - Questo gate decide quali nuove informazioni vogliamo registrare nel nostro cell state e la sua funzione può essere suddivisa in due parti (figura \ref{fig:inputGate1}):
\begin{enumerate}
\item attraverso un'operazione sigmoidale, seleziona cosa dell'input corrente deve essere mantenuto. In formula,
$$
i_t = \sigma (W_i \cdot [h_{t-1},x_t] + b_i).
$$
\item trasforma il precedente stato della LSTM con un'operazione \emph{tanh}, ovvero
$$
\widetilde{C} = tanh(W_C\cdot [h_{t-1},x_t] + b_C)
$$
\end{enumerate}

\begin{figure}
\centering
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=\textwidth]{imgs/LSTMinputGate1}
  \caption{Selezione dell'input corrente e trasformazione dello stato LSTM precedente.}
  \label{fig:inputGate1}
\end{subfigure}
\hfill
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[width=\textwidth]{imgs/LSTMinputGate2}
  \caption{Combinazione col forget gate per il calcolo del nuovo stato del cell state.}
  \label{fig:inputGate2}
\end{subfigure}
\caption{Input gate.}
\label{fig:LSTMinputForgetGate}
\end{figure}

I due vettori risultanti vengono dapprima moltiplicati fra loro e successivamente sommati al cell state combinandoli col forget gate (figura \ref{fig:inputGate2}). In formula
$$
C_t = f_t * C_{t-1} + i_t * \widetilde{C}_t.
$$

\textbf{Output gate} - Questo ultimo gate si occupa infine di decidere cosa vogliamo restituire in output ad ogni passo $t$ (figura \ref{fig:outputGate}). Il suo compito è quello di combinare una versione filtrata dello stato precedente della LSTM con una trasformazione del cell state. 
L'operazione applicata al filtraggio dello stato precedente è quella sigmoidale, di modo da selezionarne solo i componenti desiderati, ovvero
$$
o_t = \sigma(W_o[h_{t-1},x_t] + b_o)
$$
mentre quella applicata al cell state è $tanh$ di modo da proiettarne i valori nel range (-1,1), ottenendo
$$
h_t = 0_t * tanh(C_t)
$$

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth]{imgs/LSTMoutputGate}
\caption{Output gate di una LSTM}
\label{fig:outputGate}
\end{figure}

Il valore $h_t$ così calcolato sarà l'output al passo $t$ della rete LSTM e l'input del passo successivo, insieme allo stato corrente del cell state $C_t$.

Quella appena descritta è una classica LSTM ma in alcuni articoli scientifici è possibile trovare delle leggere varianti, come le \emph{``peephole" LSTM} \cite{peepholeLSTM} o le \emph{Gated Recurrent Unit (GRU)} \cite{GRU}. Ad ogni modo il concetto alla base di queste varianti resta lo stesso.

 
 
 
 %% Lavori precedenti %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Lavori precedenti}
Il riconoscimento di azioni corporee tramite l'analisi della \emph{posa} umana o \emph{skeleton} (vedremo nel prossimo capitolo più in dettagli di cosa si tratta) sta attraendo recentemente una notevole attenzione nel mondo della visione computerizzata. Il suo successo è senz'altro dovuto non solo agli ottimi risultati ottenuti, ma anche alla sua efficiente semplificazione della struttura umana riducendo di fatto i costi computazionali e le risorse necessarie allo stoccaggio dati. Normalmente la posa viene calcolata utilizzando  sensori 2D o 3D posizionati in prossimità dei giunti corporei (gomiti, ginocchi, spalle,...), oppure con algoritmi appositi in grado di estrarla da normali video RGB.

Normalmente ci sono due approcci per il riconoscimento di azioni umane facendo uso dalla posa: gli approcci ``\emph{a mano}" e quelli \emph{deep learning}.

Negli approcci \emph{a mano} si cerca di categorizzare le azioni umane seguendo pattern fisici intuitivi, come le posizioni degli arti o le correlazioni temporali fra essi, come ad esempio il movimento ondulatorio delle gambe durante una camminata o una corsa. 

Per quanto riguarda invece gli approcci \emph{deep learning} le tipologie di azioni corporee vengono categorizzate in maniera automatica direttamente dai dati stessi. Reti neurali ricorrenti, come le LSTM, o quelle di tipo \emph{convoluzionali-temporali}, come quella in \cite{TempConv}, si sono rivelate essere estremamente efficaci per il riconoscimento di pattern temporali in un video, ottenendo ottimi risultati nella categorizzazione delle azioni.

Esiste infine un terzo approccio che recentemente sta riscuotendo un discreto interesse, ovvero quello basato su \emph{grafi}, che punta ad esprimere contemporaneamente sia le relazioni temporali che quello nello spazio dei giunti corporei. Questa architettura rappresenta la sequenza di pose in un video come un grafo composto da archi temporali e spaziali, considerando cioè rispettivamente le relazioni inter ed intra-frame.

Ad oggi i migliori risultati sulla categorizzazione dei movimenti corporei possono essere riassunti nei seguenti articoli scientifici:
\begin{itemize}
\item \textbf{Recognizing Human Actions as the Evolution of Pose Estimation Maps} - \emph{Mengyuan Liu, Junsong Yuan} - 2018 \cite{PREV1}
\item \textbf{Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition} - \emph{Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng Wang, and Qi Tian} - 2019  \cite{PREV2}
\item \textbf{Vertex feature encoding and hierarchical temporal modeling in a spatial-temporal graph convolutional network for action recognition} - \emph{Konstantinos Papadopoulos, Enjie Ghorbel, Djamila Aouada, Bjorn Ottersten} - 2019  \cite{PREV3}
\end{itemize}
Vediamo adesso più nel dettaglio come questi lavori hanno affrontato il problema del riconoscimenti di azioni umane.

\section{Recognizing Human Actions as the Evolution of Pose Estimation Maps (2018)}
L'idea alla base di questo lavoro è quella di creare delle \emph{mappe di stima della posa} umana dalle quali estrarre delle \emph{heatmaps} globali e delle \emph{predizioni di pose} e una volta ottenute queste due componenti, combinare le loro caratteristiche temporali e spaziali di modo da identificare l'azione svolta.

Più precisamente, indicando con $\mathcal{Y}_k \in \{x,y\}$ le cordinate del $k$-esimo giunto corporeo, possiamo definire un'intera struttura corporea come $\mathcal{Y} = \{\mathcal{Y}_1, ... , \mathcal{Y}_k, ..., \mathcal{Y}_K\}$, dove $K$ è il numero totale dei tipi di giunti corporei considerati.

Addestrando un classificatore multiclasse $g_t^k$ per predire la posizione del $k$-esimo giunto corporeo al passo $t$, otteniamo per ogni punto \textbf{z} dell'immagine una mappa di stima relativa al giunto $k$,
$$
\boldsymbol{B}^k_t(\mathcal{Y}_k = \boldsymbol{z}) = g_t^k \left(\boldsymbol{f_z}; \bigcup_{i=1, ..., K}\psi(\boldsymbol{z}, \boldsymbol{B}_{t-1}^i)\right)
$$
dove $\boldsymbol{f_z}$ è la \emph{color feature} per il punto \textbf{z}, $\boldsymbol{B}_{t-1}^i$ è la mappa stimata da $g^i_{t-1}$ al passo precedente, $\bigcup$ è l'operatore di concatenazione vettoriale e $\psi$ è la funzione di estrazione della \emph{feature} interessata partendo dalla mappa ottenuta al passo precedente. Dopo $T$ passi, le mappe ottenute vengono utilizzate per stimare la posizione dei giunti.

Partendo dalle $K$ mappe così ottenute, per ogni frame $n$ del video, ovvero $\{\boldsymbol{B}_{T}^{1,n}, ..., \boldsymbol{B}_{T}^{K,n}\}$, vengono calcolate una heatmap $\boldsymbol{G}_{n}$ ed una posa $\mathcal{L}_n$ che rappresentano globalmente la figura umana nell'immagine (figura \ref{fig:heatmapAndPose}). La heatmap $\boldsymbol{G}_{n}$ sarà
$$
\boldsymbol{G}_{n} = \frac{1}{K}\sum_{k=1}^K \boldsymbol{B}_{T}^{k,n},
$$
mentre la posa $\mathcal{L}_n$ sarà quell'insieme di punti dell'immagine $\{\boldsymbol{z}^{k,n}\}^K_{k=1}$ per i quali
$$
\boldsymbol{z}^{k,n} = \underset{\boldsymbol{z} \in \mathcal{Z}}{arg max} \{\boldsymbol{B}_T^{k,n} (\mathcal{Y}_k = \boldsymbol{z})\},
$$
dove $ \mathcal{Z} \in \mathbb{R}^2$ è l'insieme di tutti i punti dell'immagine.

\begin{figure}[htbp]
\centering
\includegraphics[width=1.1\textwidth]{imgs/heatmapPoseCobination}
\caption{Processo di creazione della heatmap e della posa a partire da un singolo frame del video. Ogni giunto corporeo viene stimato singolarmente per poi essere successivamente combinato agli altri.}
\label{fig:heatmapAndPose}
\end{figure}

Quello che abbiamo fatto è stato cioè trasformare un video in un sequenza di heatmaps e pose, a questo punto dobbiamo però sintetizzarle in un unico elemento rappresentativo di più facile manipolazione.

Gli autori del lavoro mettono a confronto 3 diversi tipi di sintetizzazione:
\begin{itemize}
\item \textbf{sintetizzazione temporale delle heatmaps} - Data una sequenza di $N$ heatmaps $\boldsymbol{\mathcal{V}}_G = \{ \boldsymbol{G}_1, ... \boldsymbol{G} _n, ..., \boldsymbol{G}_N\}$ dove $\boldsymbol{G}_n \in \mathbb{R}^{P\times Q}$ è quella relativa all'$n$-esimo frame con $P$ righe e $Q$ colonne, allora la sequenza $\boldsymbol{G}_{1:n}$ può essere mappata in un vettore
$$
\boldsymbol{v}_n = V(\frac{1}{n}\sum^n_{i=1}\boldsymbol{G}_i),
$$
dove la funzione $V$ trasforma una matrice in un vettore $\boldsymbol{v}_n \in \mathbb{R}^{(P\cdot Q)\times 1}$ (figura \ref{fig:sintTempHeatmap}).

\begin{figure}[htbp]
\centering
\includegraphics[width=1\textwidth]{imgs/combinazioneHeatmaps}
\caption{Sintetizzazione temporale delle heatmaps.}
\label{fig:sintTempHeatmap}
\end{figure}

\item \textbf{sintetizzazione spazio-temporale delle heatmaps} - Partizionando ogni heatmap $\boldsymbol{G}_n$ in in $P$ righe, ovvero $ \boldsymbol{G}_n= [(\boldsymbol{p}_1^T, ..., \boldsymbol{p}_s^T, ..., \boldsymbol{p}_P^T]^T$ o in $Q$ colonne, ovvero $ \boldsymbol{G}_n= [(\boldsymbol{q}_1, ..., \boldsymbol{q}_s, ..., \boldsymbol{q}_Q]$ ed usando la precedente funzione $V$ per mappare $(\boldsymbol{p}_{1:s})^T$ e $\boldsymbol{q}_{1:s}$ in $\boldsymbol{v}_{s}^P$ e $\boldsymbol{v}_{s}^Q$ rispetivamente, si può ottenere due vetori $\boldsymbol{u^p},\boldsymbol{u^q}$ rappresentativi del frame attraverso la seguente funzione obbiettivo:
\begin{align*}
\underset{\boldsymbol{u}^\eta}{arg\ min}\ & \frac{1}{2}||\boldsymbol{u}^\eta||^2 + W \sum_{\forall_{i,j}\ v^\eta_{s_i} \succ v^\eta_{s_j}}\epsilon_{ij}, \\
s.t. \  & (\boldsymbol{u}^\eta)^T \cdot (\boldsymbol{v}^\eta_{s_i} - \boldsymbol{v}^\eta_{s_j}) \geq 1 - \epsilon_{ij} \\
& \epsilon_{ij} \geq 0 
\end{align*}
dove $\eta \in \{\boldsymbol{p,q}\}$, $v^\eta_{s_i} \succ v^\eta_{s_j}$ indica la successione temporale fra $v^\eta_{s_i}$ e $v^\eta_{s_j}$, $\boldsymbol{u^p}\in \mathbb{R}^{Q}$ e $\boldsymbol{u^q}\in \mathbb{R}^{P}$. Per gli $N$ frame vengono concatenati in ordine temporale i vettori cosi calcolati, ottenendo $\boldsymbol{U^p} \in \mathbb{R}^{Q\times N}$ e $\boldsymbol{U^q} \in \mathbb{R}^{P\times N}$. La matrice finale $\boldsymbol{U}$ sarà l'unione di queste concatenazioni, ovvero $[(\boldsymbol{U^p})^T,\boldsymbol{U^q})^T]^T$ ovvero una matrice che manterrà sia le caratteristiche temporali che spaziali del movimento ripreso nel video. L'intero processo è schematizzato in figura \ref{fig:spatialTemporalRank}.

\begin{figure}[htbp!]
\centering
\includegraphics[width=0.8\textwidth]{imgs/sintetizzazioneSpazioTemporaleHeatmpas}
\caption{Sintetizzazione spazio-temporale delle heatmaps.}
\label{fig:spatialTemporalRank}
\end{figure}

\item \textbf{sintetizzazione spazio-temporale delle pose} - Per comprendere come viene creata questa sintetizzazione, supponiamo di avere una sequenza di pose $\mathcal{V}_ \mathcal{L}= \{ \mathcal{L}_1, ..., \mathcal{L}_n, ..., \mathcal{L}_N \}$ dove $\mathcal{L}_n = \{ \boldsymbol{z}^{k,n} \}^K_{k=1}$ e $\boldsymbol{z}^{k,n} = (x^{k,n},y^{k,n})$, ovvero le coordinate orizzontali e verticali del $k$-esimo giunto corporeo. Per codificare le caratteristiche di tale sequenza, ogni posa viene trasformata in una successione di distanze fra giunti consecutivi, seguendo l'ordine rappresentato in figura \ref{fig:ordineGiunti}. Ripetendo questo processo per ogni frame e concatenando i risultati ottenuti, si ottiene un'immagine rappresentativa delle caratteristiche spazio-temporali dell'evoluzione della posa nel video. L'intero processo è rappresentato in figura \ref{fig:ordineGiunti}.

\begin{figure}[htbp!]
\centering
\includegraphics[width=1\textwidth]{imgs/sintetizzazioneDellePose}
\caption{Sintetizzazione spazio-temporale delle heatmaps.}
\label{fig:ordineGiunti}
\end{figure}
\end{itemize}

Gli esperimenti effettuati dagli autori sono molteplici e una delle caratteristiche principali sulla quale è necessario porre attenzione è la telecamera usata per la creazione delle pose nei video: in alcuni casi hanno utilizzato la telecamera di profondità ed in altri no.

Uno dei dataset utilizzato nel lavoro è NTU-RGB60 \cite{NTURGB}, che verrà trattato in dettaglio nel capitolo 5 e che come vedremo propone due metodi di valutazione distinti: \emph{cross-subject} e \emph{cross-view}. I migliori risultati ottenuti in questo lavoro sono riassunti in tabella \ref{tab:risultati4}.

\begin{table}[ht]
\centering
\begin{tabular}{| c | c | c | c | c | c |}
\hline
Telecamera & \thead{Accuratezza  \\ cross-view} & \thead{Accuratezza  \\ cross-subject} \\
\hline
normale   &    84.21\%     &   78.80           \\ \hline
di profondità   &    95.26\%    &   91.71\%            \\ \hline
\end{tabular}
\caption{Risultati ottenuti da  ``Recognizing Human Actions as the Evolution of Pose Estimation Maps - (2018)"}
\label{tab:risultati4}
\end{table}


\section{Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition (2019)}
Seconda spiegazione.
\section{Vertex feature encoding and hierarchical temporal modelling in a spatial-temporal graph convolutional network for action recognition (2019)}
Terza spiegazione.


%% Stima della Posa %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Stima della posa}
\begin{figure}[htbp]
\includegraphics[width=\textwidth]{imgs/stimaPosaEsempi}
\caption{Esempi di stima della posa. In alto tre esempi di stima della posa utilizzando modelli di tipo volumetrico. In basso due esempi di stima della posa ottenuti utilizando modelli di tipo scheletrici. }
\end{figure}
\newpage
Cos è la stima della posa?\\
Quando parliamo di \textit{stima della posa} ci riferiamo ad una tecnica di \textit{computer vision} dedita al riconoscimento di figure umane all'interno di video ed immagini, così da poter localizzare ad esempio dove, all'interno dell'immagine, si trova la testa, il braccio, la gamba destra, etc.. della persona inquadrata. \\
Questa tecnica non va assolutamente confusa con tecniche di riconoscimento di persone, infatti la stima della posa è in grado solo di riconoscere dove le parti del corpo di un individuo sono situate all'interno dell'immagine, non \textit{chi} è inquadrato. \\

I campi di applicazione della stima della posa sono i più svariati: software interattivi che reagiscono al movimento della persona, robotica, realtà aumentata, animazione, fotoritocco intelligente, fitness, riabilitazione, etc. Stiamo parlando di un problema tutt'altro che semplice, infatti la condizione di luce dell'immagine, la variabilità dell'ambiente circostante, l'inclinazione del soggetto inquadrato, rendono il riconoscimento della posa un problema non affatto banale. \\

Spinti dal crescente interesse, negli ultimi anni sono stati sviluppati diversi algoritmi per la stima della posa, raggiungendo in molti casi risultati davvero sorprendenti con un'accuratezza prossima alla perfezione. \\ 
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{imgs/stimaPosaEsempioFisio}
\caption{Un esempio di utilizzo in campo medico della stima della posa}
\end{figure}

La maggior parte dei software in circolazione in grado di stimare in maniera sufficientemente corretta la posa di un individuo non sono liberamente accessibili. Due fra i migliori algoritmi (ad oggi) di \emph{pose detection} sono sicuramente \textit{Posenet} \cite{PosenetArticle} e \textit{Detectron 2} \cite{Detectron2Link}, dei quali ci occuperemo in maniera più approfondita nei capitoli seguenti.


%% PoseNet %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{PoseNet}
I recenti progressi nel campo della visione artificiale hanno permesso alla comunità scientifica di spostarsi verso problemi ancora più articolati rispetto a quelli classici, con l'obiettivo di riconoscere figure umane in contesti non vincolati e molto variabili. 

L'algoritmo \emph{PoseNet} è stato ideato proprio con lo scopo di identificare una o più figure umane in qualsiasi contesto, anche in quelli ``affollati", ed essere in grado di identificare ogni persona stimandone i suoi \emph{punti chiave} (o \emph{key-points}).

Generalmente esistono due approcci principali per affrontare il problema del rilevamento di più persone in un'immagine, la stima della loro posa e la loro \emph{segmentazione} (ovvero l'identificazione dei pixel che rappresentano ogni persona):
\begin{itemize}
\item l'approccio \emph{top-down} inizia identificando e localizzando approssimativamente le singole istanze di persona delimitando il riquadro dell'immagine dentro il quale sono contenute, seguito da una fase di stima della posa o di separazione ``primo piano-sfondo" nell'area identificata. 
\item Al contrario, l'approccio \emph{bottom-up} inizia localizzando \emph{entità semantiche individuali}, come ad esempio gambe, braccia, mani, etc, seguito dal loro raggruppamento in istanze di persone complete. PoseNet adotta questo secondo approccio.
\end{itemize}
Inoltre PoseNet utilizza una rete neurale convoluzionale per la quale il costo computazionale del riconoscimento delle pose è essenzialmente indipendente dal numero di persone raffigurate nella scena ma dipende esclusivamente dalla scelta delle features della rete.

L'approccio adottato in PoseNet è quello di identificare dapprima tutti i punti chiave e successivamente raggrupparli in istanze di persona utilizzando un processo ``greedy", ovvero partendo dal rilevamento ``più sicuro" (e non come spesso accade, da un punto fisso di riferimento, ad esempio il naso). Anche se potrebbe sembrare una tecnica più "disordinata" i risultati empirici hanno dimostrato funzionare estremamente bene.

Oltre a stimare punti chiave sparsi, PoseNet stima anche delle maschere di segmentazione per ogni persona. Per fare ciò, viene allenata una seconda rete neurale con la quale viene associato ad ogni pixel $x_i$ la probabilità di appartenenza di quel pixel ad ogni candidato $j$ identificato. Se la probabilità è sufficientemente alta allora viene associato il pixel $x_i$ al candidato $j$.

Questo algoritmo è stato allenato utilizzando il famoso dataset COCO \cite{COCO-2016} (versione 2016) che, fra molte altre cose, contiene anche l'annotazione dei 17 punti chiave (12 del corpo e 5 del volto) di migliaia di persone ed è riuscito a migliorarne l'\emph{AP} (average-precision) da 0,655 a 0,687 diventandone il miglior risultato. 

Essendo molto semplice questo metodo è anche quindi molto rapido, poiché non richiede alcuna fase supplementare di raffinamento dei risultati con tecniche di tipo \emph{box-based} o \emph{clustering}, rendendo di fatto PoseNet uno degli algoritmi più facilmente installabili su piccoli dispositivi, come ad esempio i cellulari.

Ma vediamo adesso più nel dettaglio come PoseNet stima e raggruppa i punti chiavi di una o più persone raffigurate in un'immagine.

\subsection{Stima dei key-points}
L'obiettivo di questa fase è quello di rilevare, in modo indipendente dall'istanza, tutti i key-points visibili appartenenti a qualsiasi persona dell'immagine. A tale scopo vengono prodotte delle \emph{heat-maps}, ovvero dei canali della rete neurale dediti al riconoscimento di particolari caratteristiche dell'immagine (una canale per ogni key-point) e degli \emph{offset} (due canali per ogni key-point per gli spostamenti in orizzontale e verticale). Sia $x_i$ la posizione 2-D nell'immagine, dove $i = 1, ... ,N$ e $N$ è il numero di pixels; $D_R (y) = \{ x:  \|x - y \| \leq R\}$ un disco di raggio $R$ centrato in $y$ e $y_{j,k}$ la posizione 2-D del $k$-esimo key-point della j-esima istanza di persona, con $j = 1, ..., M$, dove $M$ è il numero di istanze nell'immagine. 
\begin{figure}
\centering
\includegraphics[width=1\textwidth]{imgs/heatmaps-posenet}
\caption{Generazione con PoseNet delle heat-maps per ogni tipologia di key-point}
\end{figure}

Per ogni tipo di key-point $k = 1, ..., K$, viene impostato un task di classificazione binaria come segue. Viene generata una heat-map $p_k(x)$ tale che $p_k(x) = 1$ se $x \in D_R (y_{j,k})$ per qualsiasi istanza $j$, altrimenti $p_k(x) = 0$. Abbiamo quindi $K$ tasks di classificazione binaria indipendenti (uno per ogni tipo di key-point) e ciascuno equivale a prevedere un disco di raggio $R$ attorno a un tipo di key-point specifico appartenente a qualsiasi persona nell'immagine.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{imgs/short-range-Posenet}
\caption{Esempio di stima degli offset a corto raggio con PoseNet}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{imgs/mid-range-Posenet}
\caption{Esempio di stima degli offset a medio raggio con PoseNet. L'intento è quello di raggruppare i keypoints appartenenti alla stessa persona.}
\label{fig:midRangeOffset}
\end{figure}

Oltre alle heat-maps, vengono anche usati vettori di \emph{offset a corto raggio} $S_k(x)$ il cui scopo è quello di migliorare l'accuratezza della localizzazione dei key-points. Per ogni punto $x$ all'interno dei dischi ricavati al passo precedente, il vettore di offset 2-D a corto raggio $S_k(x) = y_{j,k} - x$ rappresenta la distanza fra il punto $x$ e il $k$-esimo key-point della $j$-esima persona. Vengono cosi generati $K$ vettori per ogni punto $x$ all'interno del disco definito che, combinati insieme in una \emph{trasformata di Hough} $h_k(x)$, miglioreranno l'accuratezza della posizione predetta di ogni key-point. 
Solo i punti che superano una certa soglia di Hough (0.01, come indicato da \cite{PosenetArticle}) vengono considerati dei key-point, gli altri invece vengono scartati.

\subsection{Raggruppamento dei key-points in istanze di persona}
\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{imgs/legamiKeypointsPoseNet}
\caption{Struttura ad albero utilizzata da PoseNet per raggruppare i key-points appartenenti alla stessa persona}
\label{fig:strutturakeypoints}
\end{figure}
A questo punto è però necessario capire come associare ogni key-point alle persone raffigurate nell'immagine (nel caso ce ne sia più di una). 
Seguendo lo schema delle connessioni fra tipi di key-points (rappresentati in figura \ref{fig:strutturakeypoints}) la rete viene allenata per restituire in output anche i cosiddetti \emph{offset a medio raggio}, ovvero probabilità di connessioni fra key-points, col lo scopo di raggruppare quelli appartenenti alla stessa persona. 

Un esempio di questa stima è raffigurato in figura \ref{fig:midRangeOffset} mentre una raffigurazione completa del sistema adottato da PoseNet per il riconoscimento delle pose di persone raffigurate in un'immagine è rappresentato in figura \ref{fig:overview-posenet}.


\begin{figure}
\centering
\includegraphics[width=1.1\textwidth]{imgs/overview-posenet}
\caption{Combinazione delle fasi adottate da PoseNet per il riconoscimento della posa in un'immagine.}
\label{fig:overview-posenet}
\end{figure}


%% Detecron 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Detectron2}
Detectron2 è un progetto open-source lanciato dalla \emph{Facebook AI Research (FAIR)} ampiamente usato dalla comunità di ricerca in ambito \emph{computer vision} e rappresenta, ad oggi, una piattaforma per il riconoscimento di oggetti allo stato dell'arte.\\

Il suo predecessore \emph{Detectron} \cite{DetectronLink} fu un progetto iniziato nel 2016 con l'obiettivo di creare un sistema rapido e flessibile per il riconoscimento di oggetti in immagini ed originariamente basato su \emph{Caffe2} \cite{caffe2} (un framework ideato per facilitare la sperimentazione e la divulgazione di nuovi modelli e algoritmi in ambito \emph{deep learning}) e scritto in \emph{Python}. 

Negli anni Detectron è stato perfezionato e supportato da una grande quantità di progetti, compreso ``\emph{Mask-R-CNN}" \cite{maskRCNN} e ``\emph{Focal Loss for Dense Object Detection}" \cite{focalLossDetection}, vincitori rispettivamente del \emph{Premio Marr} e di \emph{Miglior articolo scientifico studentesco} all'\emph{Internation Conference on Compuer Vision} (ICCV) del 2017. L'intuitività e l'efficacia di questi algoritmi hanno permesso un notevole sviluppo nella risoluzione di problemi complessi nell'ambito della computer vision, come ad esempio l'\emph{instance segmentation}, e hanno sicuramente giocato un ruolo rilevante nell'avanzamento tecnologico dei sistemi di riconoscimento visivo.\\

\begin{figure}
\centering
\includegraphics[width=1.1\textwidth]{imgs/typesOfDetection}
\caption{Tipologie di analisi visive.}
\label{fig:typeOfDetection}
\end{figure}

Detectron2 è adesso basato su \emph{Pytorch}, una libreria open-source dedita al machine learning ed ampiamente usata nel campo della computer-vision che ha inglobato in se anche il precedente framework Caffe2.  Più nello specifico Detectron2 include ad oggi le implementazioni dei seguenti algoritmi di object-detection:
\begin{itemize}
\item Cascade R-CNN \cite{cascadeRCNN}
\item Panoptic FPN \cite{panopticFeatures}
\item TensorMask \cite{tensorMask}
\item Mask R-CNN \cite{maskRCNN}
\item RetinaNet \cite{focalLossDetection}
\item Faster R-CNN \cite{fasterRCNN}
\item RPN \cite{fasterRCNN}
\item Fast R-CNN \cite{fastRCNN}
\item R-FCN \cite{RFCN}
\end{itemize}

utilizzando le seguenti reti \emph{backbone} (ovvero reti precedentemente allenate con lo scopo di estrarre in maniera efficiente le \emph{features} di un'immagine):
\begin{itemize}
\item ResNeXt\{50,101,152\} \cite{resNetXt}
\item ResNet\{50,101,152\} \cite{resNet}
\item Feature Pyramid Networks (con ResNet/ResNeXt) \cite{featurePyramid}
\item VGG16 \cite{vgg16}
\end{itemize}

Inoltre, nel caso fosse necessario implementare nuove reti backbone, è possibile farlo facilmente grazie alla struttura modulare di Pytorch, che permette di separare il nuovo modello dai precedenti algoritmi di Detectron2.\\
\begin{figure}
\centering
\includegraphics[width=1.1\textwidth]{imgs/instanceSeg}
\caption{Instance segmentation con Detectron2}
\end{figure}
Essendo stato interamente riscritto in Pytorch, Detectron2 è più rapido del suo predecessore nei compiti di \emph{object-detection}, \emph{instance segmentation} e \emph{human-pose prediction}, ed in più è in grado di fornire supporto per i nuovi task di \emph{semantic segmentation} e \emph{panoptic segmentation}, ovvero la combinazione fra instance-segmentation e semantic-segmentation (figura \ref{fig:typeOfDetection}).

Oltre che nella ricerca, questa piattaforma viene usata anche da numerosi team di Facebook (e non solo) per l'addestramento di nuovi modelli in svariati campi della \emph{computer vision}, come ad esempio la \emph{realtà aumentata}, e in materia di sicurezza informatica, come ad esempio la \emph{community integrity}, ovvero la difesa e la protezione di account su piattaforme social da contenuti maligni.

Per quanto riguarda la stima della posa umana in un'immagine, Detectron2 utilizza una Mask R-CNN \cite{maskRCNN} riadattata all'estrazione dei keypoints.

\subsection{Mask R-CNN}
\begin{figure}
\centering
\includegraphics[width=1.1\textwidth]{imgs/instanceSegmenttionMaskRCNN2}
\caption{Alcuni esempi di instance segmentation con Mask R-CNN}
\end{figure}
Mask R-CNN nasce con l'intento di creare un framework semplice e flessibile per affrontare il problema dell'\emph{instance segmentation}. Questo metodo è in grado di identificare gli oggetti in un'immagine e simultaneamente generare una maschera di segmentazione ben definita per ogni istanza. Mask R-CNN è sostanzialmente un'estensione di \emph{Faster R-CNN} \cite{fasterRCNN} in quanto aggiunge parallelamente ai due output già presenti per il \emph{bounding box} e la classificazione anche quello per la predizione della maschera d'istanza. Nonostante sia un metodo nato per l'instance segmentation è però facilmente adattabile ad altri tipi di predizioni, come ad esempio quella della posa umana.

Mask R-CNN è suddiviso in due fasi di procedura. La prima, identica a quella adottata per Faster R-CNN, chiamata ``\emph{Region Proposal Network}" (RPN) ha il compito di proporre porzioni di immagine nelle quali potrebbero essere raffigurate delle istanze. Nella seconda, viene predetto \emph{in parallelo} alla classe e al delineamento preciso dell'istanza (\emph{bounding box}) anche una maschera binaria per ogni \emph{regione d'interesse} (RoI) dell'immagine. 

Più precisamente, durante l'allenamento della rete, la \emph{loss} definita per ogni RoI è la seguente: $L= L_{cls} + L_{box} + L_{mask}$. La loss di classificazione e quella del boundig box sono identiche a quelle definite per Fast R-CNN \cite{fastRCNN}. 

Per quanto riguarda invece $L_{mask}$, dato che decodifica $K$ maschere binarie di dimensione $m \times m$ (dove $K$ è il numero di classi), la sua dimensione è $Km^2$ per ogni RoI.

Questa definizione di $L_{mask}$ permette alla rete, non solo di generare maschere completamente indipendenti l'una dall'altra, ma anche di disaccoppiare la predizione delle maschere dalla classificazione delle istanze. Questa caratteristica, innovativa rispetto alle pratiche comuni in materia di semantic segmentation, si è rivelata essere basilare per il raggiungimento di una buona segmentazione.

\subsection{Predizione della posa con Mask R-CNN}
Grazie alla sua grande flessibilità, questo framework può essere facilmente esteso alla stima della posa umana in un'immagine. 

\begin{figure}
\centering
\includegraphics[width=1.1\textwidth]{imgs/posePredictionMaskRCNN}
\caption{Alcuni esempi di pose prediction con Mask R-CNN}
\end{figure}

Le coordinate dei keypoints vengono trasformate in maschere di tipo \emph{one-hot} e Mask R-CNN viene utilizzato per predire $K$ maschere, una per ogni tipo di keypoint (gomito sinistro, spalla destra, etc..).

Più nello specifico, per ognuno dei $K$ keypoint di un'istanza, il training target è una maschera binaria $m \times m$ di tipo \emph{one-hot} dove cioè solo un pixel viene etichettato come positivo. Durante l'allenamento della rete, per ogni keypoint visibile nell'immagine, viene minimizzata una \emph{cross-entropy} loss con regolarizzatore L2 (per incoraggiare l'identificazione di un singolo punto). Anche per questa procedura, le $K$ maschere dei $K$ tipi di keypoint sono completamente indipendenti l'una dall'altra. 

%%%%%%  Metodo proposto %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Metodo proposto}
In questo capitolo oltre ad approfondire le diverse tecniche adottate nel lavoro di tesi, verrà illustrato anche l'iter procedurale che ha portato ai risultati ottenuti. 
Come vedremo nel capitolo seguente, il dataset utilizzato è NTU-RGB60 \cite{NTURGB} e nonostante fornisca anche i dati relativi alla posa dei soggetti inquadrati (quello che viene spesso indicato come \emph{skeleton}) ho preferito non farne assolutamente uso ed utilizzare invece i soli video RGB, dai quali successivamente stimare la posa e servirmi di quest'ultima per il riconoscimento delle azioni svolte. Questa scelta è stata presa con lo scopo di favorire una maggiore generalizzazione e applicazione dell'algoritmo, adoperabile in contesti più diffusi dove la stima della posa non è data, ma abbiamo a disposizione solo un comunissimo video RGB.
Passiamo adesso ad analizzare una ad una le varie fasi affrontate nel lavoro di tesi.
\section{Estrazione delle pose}
Per prima cosa è necessario convertire tutti i video del dataset in sequenze di pose, in modo da sintetizzarne efficacemente il movimento umano senza un'eccessiva perdita d'informazione.  Gli algoritmi scelti per questa fase sono stati Detectron2 \cite{Detectron2Link} per la sua rimarcabile accuratezza e PoseNet \cite{PosenetArticle} per la sua velocità d'inferenza e portabilità.
I modelli proposti da queste due tecniche sono molteplici. Per Detectron2 abbiamo:
\begin{itemize}
\item  \emph{R50-FPN-1x}
\item \emph{R50-FPN-3x}
\item \emph{R101-FPN-3x} 
\item  \emph{X101-FPN-3x} 
\end{itemize}
mentre per PoseNet abbiamo:
\begin{itemize}
\item  \emph{PoseNet-50}
\item \emph{PoseNet-75}
\item \emph{PoseNet-100}
\item \emph{PoseNet-101}
\end{itemize}

Ognuno di questi modelli ha valori diversi in accuratezza e velocità di inferenza. Nel caso di Detectron2 questa comparazione viene fornita dagli autori stessi e schematizzata in tabella \ref{proposteDetectronPosenet}, mentre per quanto riguarda PoseNet non viene purtroppo fornito un confronto simile. 

\begin{table}[ht]
\centering
\begin{tabular}{| c | c | c | c |}
\hline
Nome & \thead{Tempo di  \\ inferenza \\ (s/imm)} &  \thead{Box  \\ AP} &  \thead{KP  \\ AP}  \\
\hline
 R50-FPN-1x	 		& 0.072  	& 53.6 	& 64.0 \\ \hline
 R50-FPN-3x 			& 0.066  	& 55.4 	& 65.5 \\ \hline
 R101-FPN-3x 			& 0.076  	& 56.4 	& 66.1 \\ \hline
 \textbf{X101-FPN-3x} 	& 0.121  	& 57.3 	& 66.0 \\ \hline
\end{tabular}
\caption{Diversi modelli in Detectron 2 per l'estrazione della posa da un'immagine. Il tempo di inferenza viene misurato in \emph{secondi/immagine}; Box AP = ``Bounding box average precision" ; KP AP = ``Keypoint average precision"}
\label{proposteDetectronPosenet}
\end{table}

È stato quindi scelto il modello \emph{X101-FPN} per Detectron2, in quanto quello con il miglior valore di accuratezza globale e PoseNet-101, per per una più facile comparazione dei risultati.

\section{Assegnazione delle pose}
Quello che otteniamo dalla fase precedente è, per ogni video, una sequenza di pose umane estrapolate da ogni frame. Sia Detectron2 che PoseNet sono in grado di riconoscere più persone all'interno di un'immagine, producendo una lista di pose $P_1, P_2, ..., P_n$ ordinata secondo le regole dell'algoritmo, ad esempio Detectron2 le ordina per valore di \emph{score} decrescente, ovvero quel valore che indica quanto l'algoritmo sia sicuro di aver correttamente stimato la posa di quella persona.

\begin{figure}
\centering
\includegraphics[width=1.1\textwidth]{imgs/swapPose}
\caption{Un esempio di assegnazione inconsistente delle pose lungo il video. Nell'esempio, l'ordine delle pose restituite dall'algoritmo è blu, rosso, verde e viola. Quest'ordine è indipendente dai soggetti inquadrati, che possono talvolta essere scambiati fra loro, non essere riconosciuti o addirittura scambiati per dei riflessi.}
\label{fig:swapPose}
\end{figure}

Quello che però questi algoritmi non hanno è una conoscenza relativa al concetto di video, ovvero alla successione logica dei frame e trattano ognuno di essi come un'immagine indipendente. Quello che ne segue è una totale indipendenza fra l'ordine delle pose riconosciute e la loro assegnazione consistente ai soggetti rappresentati nel video. Ad esempio, ammettiamo di avere due soggetti $A$ e $B$ rappresentati in un video e che per l'$i$-esimo frame vengano identificate due pose $\mathcal{P}_{1,i}$ e $\mathcal{P}_{2,i}$ appartenenti ad A e B rispettivamente. Quest'ordine non è però garantito nel frame successivo dove $\mathcal{P}_{1,i+1}$ potrebbe essere la posa di B e $\mathcal{P}_{2,i+1}$ quella di A. 

Per ovviare a questo problema la tecnica adottata è stata quella di calcolare iterativamente ad ogni frame la distanza fra l'ultima posa assegnata ad ogni soggetto e tutte quelle riconosciute al frame successivo. La distanza fra due pose non è altro che la somma delle distanze euclidee fra i punti in comune delle due pose, ovvero se $\mathcal{P}_i = \{p_{i,k}\}_{k=1}^K$ è l'insieme dei punti che definiscono la posa $i$, dove ogni $p_{i,k} = (x_{i,k},y_{i,k})$ sono le ascisse e le ordinate del $k$-esimo giunto e $K$ il numero di tipi di giunto, allora la distanza fra $P_i$ e $P_j$ sarà:
$$
\mathcal{D}_{ij} = \sum_{k=1}^K I(p_{i,k}) \cdot I(p_{j,k}) \cdot eucl(p_{i,k},p_{j,k})
$$
dove $I$ è la funzione identità, ovvero $I(p) = 1$ se $p$ è definito, 0 altrimenti e $eucl(p_1,p_2)$ è  la funzione di distanza euclidea fra i punti $p_1$ e $p_2$. Se le due pose non hanno punti in comune, ovvero  $\sum_{k=1}^K I(p_{i,k}) \cdot I(p_{j,k}) = 0 $ allora $\mathcal{D}_{ij} = \infty$.

Una volta ottenuta ed ordinata in maniera decrescente la lista delle distanze fra pose, si può procede alla loro assegnazione ai soggetti inquadrati e ripetere il procedimento per il frame successivo.

Abbiamo però un ultimo problema da fronteggiare in questa fase, ovvero la variabilità del numero di pose stimate in ogni frame del video. Questo accade ad esempio, quando per uno o più frame uno dei soggetti non viene riconosciuto dall'algoritmo o quando l'improvviso riflesso su una finestra o uno specchio viene interpretato come una nuova persona, come raffigurato in figura \ref{fig:swapPose}. 

Questo problema è stato risolto ordinando le sequenze di pose secondo il loro score medio lungo tutto il video. Si presuppone infatti che un riflesso sia mediamente meno convincente di una persona vera e propria. Una volta ordinate le sequenze, sono state mantenute solo le migliori due e scartate le altre, visto che siamo interessati ad analizzare solo azioni individuali o di coppia.

\section{Rimozione degli zeri}
Come abbiamo appena visto, PoseNet e Detectron2 talvolta in qualche frame possono non riconoscere un'intera persona o non riuscire a stimare la posizione di qualche giunto, se ad esempio coperti o sfocati. Entrambi gli algoritmi trattano questi 2 casi nella stessa maniera, ovvero assegnano il punto $(0,0)$ ad ogni giunto non riconosciuto. 

\begin{figure}
\centering
\includegraphics[width=1.1\textwidth]{imgs/rimozioneZeri.jpg}
\caption{Esempio di rimozione degli zeri. Nei frame 2-3-4-5 i giunti del braccio sinistro non sono stati riconosciuti. Nei frame 2-3 verranno quindi rimpiazzati con quelli del frame 1, mentre nei frame 4-5 con quelli del frame 6.}
\label{fig:rimozioneZeri}
\end{figure}

Questo valore però è fortemente fuorviante considerando che le informazioni a nostra disposizione sono solo pochi punti corporei. Con Detectron2 e PoseNet la posa ottenuta è composta di soli 17 punti, quindi per ogni giunto non riconosciuto non solo perdiamo 1/17 d'informazione ma confondiamo anche il nostro algoritmo facendogli analizzare un movimento inesistente, per quel giunto, verso il punto (0,0).

È necessario quindi adottare una buona tecnica di pulizia degli zeri che limiti la perdita d'informazione mantenendo una consistenza con i risultati ottenuti.

La cosa più semplice da fare è assegnare ad ogni giunto non riconosciuto la sua ultima posizione identificata, ma cosi facendo utilizzeremmo solo il passato del video in questione e per lunghe sequenze di valori mancanti potremmo perdere molta informazione utile. È stato quindi scelto di attribuire ad ogni giunto non identificato il primo punto identificato più vicino nel tempo, passato o futuro, per quel giunto.

L'intento di questa tecnica è quello di ridurre la distanza fra ogni giunto non identificato e la loro effettiva posizione nell'immagine. Un caso esempio è rappresentato in figura \ref{fig:rimozioneZeri}

% PARLA DI RIMOZIONE DEGLI ZERI IN MANIERA INTERPOLANTE SE RIESCI AD INCUDERLO NEL LAVORO


\begin{itemize}
\item tecniche:
\begin{itemize}
\item centro video
\item centro frame
\item 5 Baricentri video
\item 3 Baricentri frame
\item 3 Baricentri video
\item 3 Baricentri video ABS
\item 17 baricentri video (uno per ogni keypoint)
\item 17 baricentri video ABS 
\item next frame
\item next frame 5
\item next frame 11
\item next frame 17
\item distanze cumulative
\item distanze fra punti
\end{itemize}
\item tipi di normalizzazione - Classica o Indipendente
\item dropout e dropout ricorrente
\item smussamento
\item layers
\item LSTM e CudNNLSTM?
\item regolarizzatori?
\end{itemize}


%%%%%%  NTU RGB+D %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Il dataset NTU RGB+D}
Il recente sviluppo dei sensori di profondità ha permesso un notevole miglioramento degli studi in ambito 3D, come ad esempio il riconoscimento di oggetti o azioni in scenari tridimensionali. 

Essendo però un campo scientifico piuttosto ``giovane'', la ricerca di un adeguato dataset per il \emph{riconoscimento di azioni umane} che sia solido, voluminoso e vario può non essere un compito facile. 

Spesso i dataset pubblicamente disponibili sono composti da uno stretto gruppo di soggetti, il che riduce notevolmente la variabilità intra-classi e un'\emph{attività umana} dipende fortemente dall'età, il sesso, la cultura e le condizioni fisiche di chi la svolge quindi avere una sufficiente variabilità di soggetti nel dataset che stiamo analizzando è di vitale importanza.

Un'altra cosa estremamente importante è il numero di azioni che stiamo analizzando. Se il nostro dataset contiene poche tipologie di attività umane, sarà facile distinguerle e classificarle fra loro, magari identificando un semplice pattern di movimento o addirittura la struttura di un oggetto coinvolto nell'azione da classificare. Se invece il numero di azioni trattate è sufficientemente alto, allora i pattern di movimento e gli oggetti con i quali si interagisce vengono condivisi fra più classi di azioni, rendendo la classificazione estremamente più difficile.

Il terzo punto da considerare nella scelta di un buon dataset è il punto di vista dal quale vengono riprese le azioni. La maggior parte dei dataset contengono video che riprendono l'azione da un unico punto di vista (spesso frontale), in altri casi invece i punti di vista sono strettamente due (magari frontale e di lato), ottenuti solo perchè sono state utlizzate più telecamere contemporaneamente.

Come ultima cosa e forse la più importante, l'insufficienza di video nei dataset ci impedisce spesso di applicare tecniche di machine-learning al nostro problema, portandoci inevitabilmente ad una situazione di overfitting.

Consapevoli di quanto detto finora Amir Shahroudy, Jun Liu, Tian-Tsong Ng e Gang Wang hanno creato nel 2016 \emph{NTU RGB+D} \cite{NTURGB}, un vasto e variegato dataset dedito all'analisi di attività umane in ambito 3D.

\begin{figure} [htb!]
\centering
\includegraphics[width=0.9\textwidth]{imgs/varietaNTURGB}
\caption{Alcuni esempi di frames del dataset NTU RGB+D. Nelle prime 4 righe è possibile notare la varietà degli attori partecipanti al progetto e dei diversi punti di ripresa utilizzati. La quinta riga mostra la varietà intra-classe per una stessa azione. L'ultima riga mostra per uno stesso frame i valori RGB, i valori RGB + giunti, mappa di profondità,  mappa di profondità + giunti e i valori a infrarossi. }
\label{fig:varietaNTURGB}
\end{figure}

NTU RGB+D si compone di 56880 RGB+D video, ottenuti riprendendo 40 differenti attori ed utilizzando Microsoft Kinect v2. Il dataset contiene i video RGB, le sequenze di profondità, lo skeleton dei soggetti (ovvero le posizioni 3D dei principali giunti corporei) e i frames a infrarossi. I soggetti sono stati ripresi da 80 punti di vista differenti e la loro età varia dai 10 ai 35 anni, il che rende più realistica la variazione di qualità delle azioni svolte. Anche se tutte le riprese sono di tipo \emph{indoor}, gli scenari ricreati intorno agli attori sono estremamente variabili.

\section{NTU RGB+D in dettaglio}
\textbf{Tipologie di dati} - I dati raccolti sono stati ottenuti con sensori Microsoft Kinect v2 che consentono di estrapolare dalle riprese 4 tipologie di dato: mappe di profondità, informazioni 3D dei giunti corporei, frame RGB e sequenze a infrarossi.

\begin{figure} [htb]
\centering
\includegraphics[width=0.5\textwidth]{imgs/giuntiNTURGB}
\caption{La configurazione dei 25 giunti utilizzati in NTU RGB+D. Le etichette utilizzate per i giunti sono: 1 - base della colonna vetrebrale, 2 - punto medio della colonna vertebrale, 3 - collo, 4 - testa, 5 - spalla sinistra, 6 - gomito sinistro, 7 - polso sinistro, 8 - mano sinistra, 9 - spalla destra, 10 - gomito destro, 11 - polso destro, 12 - mano destra, 13 - anca sinistra, 14 - ginocchio sinistro, 15 - caviglia sinistra, 16 - piede sinistro, 17 - anca destra, 18 - ginocchio destro, 19 - caviglia destra, 20 - piede destro, 21 - colonna vertebrale, 22 - punta della mano sinistra, 23 - pollice sinistro, 24 - punta della mano destra, 25 - pollice destro.}
\label{fig:giuntiNTURGB}
\end{figure}

Le \emph{mappe di profondità} sono sequenze di valori in millimetri che rappresentano la distanza di ogni punto dalla telecamera. Ogni mappa di profondità è stata  poi ridotta ad una risoluzione di $512 \times 424$ senza perdita di informazione.

Le \emph{informazioni 3D dei giunti corporei} sono coordinate 3D dei 25 giunti corporei più importanti. Per ogni giunto e per ogni frame viene inoltre saltavo il corrispondente pixel nel video RGB e la realtiva mappa di profondità. I 25 giunti corporei utilizzati sono raffigurati in figura \ref{fig:giuntiNTURGB}.

I \emph{video RGB} sono normali video registrati con una risoluzione $1920 \times 1080$.

Le \emph{sequenze a infrarossi} sono, come per le mappe di profondità, salvate frame per frame con una dimensione di $512 \times 424$.

\textbf{Le classi delle azioni} - Il dataset contiene 60 tipi di azioni differenti divise in 3 gruppi principali: 40 azioni \emph{quotidiane} (bere, mangiare, leggere, etc.), 9 azioni \emph{legale alla salute} (starnutire, barcollare, svenire, etc...) e 11 azioni \emph{di coppia} (dare un cazzotto a qualcuno, abbracciarsi, etc...)

\textbf{I soggetti} - Per la realizzazione di questo dataset sono stati utilizzati 40 attori di età compresa dai 10 ai 35 anni. In figura \ref{fig:varietaNTURGB} è possibile vedere come l'insieme delle persone scelte sia estremamente variegato in età, altezza e sesso. Ogni attore è rappresentato con un ID unico in tutto il dataset.

\textbf{I punti di ripresa} - Ogni azione è stata registrata da 3 telecamere contemporaneamente di modo da generare 3 prospettive diverse della stessa azione. Le 3 telecamere sono state posizionate tutte alla stessa altezza e sempre con la stessa angolazione di ripresa, ovvero $\ang{-45}, \ang{0}$ e $\ang{45}$. 

\begin{figure}[htb!]
\centering
\includegraphics[width=1\textwidth]{imgs/disposizioneCamereNTURGB-3}
\caption{Disposizione delle telecamere per ogni setup. La telecamera 1 ha ripreso ogni azione da una posizione centrale mentre le telecamere 2 e 3 hanno ripreso le azioni con un'angolazione di $\ang{45}$. Ogni attore ha ripetuto l'azione 2 volte: una rivolto verso la telecamere di destra e una verso quella di sinistra. }
\label{fig:schemaTelecamere}
\end{figure}

Ad ogni attore è stato inoltre richiesto di ripetere l'azione due volte: una rivolto verso la telecamere di sinistra ed una verso quella di destra. Cosi facendo le 6 riprese ottenute forniscono 5 prospettive diverse della stessa azione, ovvero  quella frontale (ripresa 2 volte), quella dal profilo sinistro, quella dal profilo destro, quella ad una angolazione di $\ang{45}$ da sinistra e quella ad una angolazione di $\ang{45}$ da destra. Come per gli attori, l'ID assegnato ad ogni telecamera è fisso per tutto il dataset: la telecamera 1 è sempre quella che riprende da $\ang{0}$, la telecamera 2 da $\ang{45}$ e la 3 da $\ang{-45}$. Uno schema della disposizione delle telecamere e della procedura di ripresa è raffigurato in figura \ref{fig:schemaTelecamere}

Per aumentare ulteriormente la variabilità delle riprese, ad ogni \emph{setup} (ovvero ad ogni set di ripresa) le 3 telecamere sono state posizionate ad un'altezza e ad una distanza diversa dall'attore inquadrato, ovviamente mantenendo sempre la stessa altezza fra le 3 telecamere. In tabella \ref{tab:puntiDivista} sono elencate le posizioni delle 3 telecamere per ogni setup.

\begin{table}[ht]
\centering
\begin{tabular}{| c | c | c | c | c | c |}
\hline
Setup & \thead{Altezza  \\ (m)} &  \thead{Distanza  \\ (m)} & Setup &\thead{Altezza  \\ (m)}   &   \thead{Distanza  \\ (m)} \\
\hline
1   &    1.7     &   3.5  &   2   &    1.7    &   2.5          \\ \hline
3   &    1.4     &   2.5  &   4  &    1.2    &   3.0          \\ \hline
5   &    1.2     &   3.0  &   6  &    0.8    &   3.5          \\ \hline
7   &    0.5     &   4.5  &   8  &    1.4    &   3.5          \\ \hline
9   &    0.8     &   2.0  &   10  &    1.8    &   3.0          \\ \hline
11   &    1.9     &   3.0  &   12  &    2.0    &   3.0          \\ \hline
13   &    2.1     &   3.0  &   14  &    2.2    &   3.0          \\ \hline
15   &    2.3     &   3.5  &   16  &    2.7    &   3.5          \\ \hline
17   &    2.5     &   3.0  &    &      &            \\
\hline
\end{tabular}
\caption{Altezza e distanza delle 3 telecamere per ogni setup di ripresa. Le altezze e le distanze sono espresse in metri. }
\label{tab:puntiDivista}
\end{table}

\section{Criteri di valutazione}
Per avere una valutazione standard dei risultati ottenuti su questo dataset, gli autori definiscono nel loro lavoro \cite{NTURGB} due particolari criteri di valutazione, di modo da poter allineare anche i risultati futuri e poterli comparare fra loro.

\subsection{Valutazione Cross-subject}
Per questa valutazione, i 40 attori sono stati divisi in due gruppi: \emph{training} e \emph{test} ed ogni gruppo è costituito da 20 attori. Così facendo è stato ottenuto un insieme di training di 40320 video e un insieme di test di 16560 video. 

Gli ID degli attori dell'insieme di training sono: 1, 2, 4, 5, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 27, 28, 31, 34, 35, 38. I rimanenti  fanno parte dell'insieme di test.
\subsection{Valutazione Cross-view}
Per quanto riguarda invece la valutazione \emph{cross-view}, tutti i video ripresi dalle telecamere 2 e 3 costituiscono l'insieme di training, mentre quelli ripresi dalla telecamera 1 l'insieme di test. In altre parole l'insieme di training include tutte le prospettive frontali del soggetto inquadrato e tutte quelle di profilo mentre l'insieme di test include tutte le prospettive a $\ang{45}$. Dividendo il dataset con questo criterio si ottiene un insieme di training composto da 37920 video e quello di test da 18960.

%% Esperimenti svolti e risultati %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Esperimenti e risultati}


%% Esperimenti svolti e risultati %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusioni e sviluppi futuri}

%\thebibliography{}

\begin{thebibliography}{100}  % 100 is a random guess of the total number of references

\bibitem{PosenetArticle} PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model - \emph{George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros Gidaris, Jonathan Tompson, Kevin Murphy} - 2018
\bibitem{COCO-2016} Coco 2016 keypoint challenge - Lin, T.Y., Cui, Y., Patterson, G., Ronchi, M.R., Bourdev, L., Girshick, R., Dollr,P. - 2016
\bibitem{PosenetLink} PoseNet with TensorFlow.js - \emph{https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5}

\bibitem{Detectron2Link} Detectron2 - \emph{Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick - https://github.com/facebookresearch/detectron2}, 2019

\bibitem{DetectronLink} Detectron - \emph{Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollàr, Kaiming He - https://github.com/facebookresearch/detectron}, 2018

\bibitem{caffe2} Caffe2 - \emph{https://caffe2.ai/docs}

\bibitem{maskRCNN} Mask R-CNN - \emph{Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick}, 2017

\bibitem{focalLossDetection} Focal Loss for Dense Object Detection - \emph{Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár}, 2017

\bibitem{fasterRCNN} Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - \emph{Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun}, 2015

\bibitem{fastRCNN} Fast R-CNN - \emph{Ross Girshick}, 2015

\bibitem{RFCN} R-FCN: Object Detection via Region-based Fully Convolutional Networks - \emph{Jifeng Dai, Yi Li, Kaiming He, Jian Sun}, 2016

\bibitem{resNetXt} Aggregated Residual Transformations for Deep Neural Networks - \emph{Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He}, 2016

\bibitem{resNet} Deep Residual Learning for Image Recognition - \emph{Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun}, 2015

\bibitem{featurePyramid} Feature Pyramid Networks for Object Detection - \emph{Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie}, 2016

\bibitem{vgg16} Very Deep Convolutional Networks for Large-Scale Image Recognition - \emph{Karen Simonyan, Andrew Zisserman}, 2014

\bibitem{cascadeRCNN} Cascade R-CNN: High Quality Object Detection and Instance Segmentation - \emph{Zhaowei Cai, Nuno Vasconcelos} - 2019

\bibitem{panopticFeatures} Panoptic Feature Pyramid Networks - \emph{Alexander Kirillov, Ross Girshick, Kaiming He, Piotr Dollár} - 2019

\bibitem{tensorMask} TensorMask: A Foundation for Dense Object Segmentation - \emph{Xinlei Chen, Ross Girshick, Kaiming He, Piotr Dollár} - 2019

\bibitem{NTURGB} NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis - \emph{Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang} - 2016

\bibitem{LSTM} Long short-term memory - \emph{Sepp Hochreiter; Jürgen Schmidhuber} - 1997

\bibitem{peepholeLSTM} Recurrent Nets that Time and Count - \emph{Felix A. Gers; Jürgen Schmidhuber} - 2000


\bibitem{GRU} Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation - \emph{Kyunghyun Cho; Bart van Merriënboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio } - 2014

\bibitem{PREV1}{Recognizing Human Actions as the Evolution of Pose Estimation Maps} -
\emph{Mengyuan Liu, Junsong Yuan} - 2018

\bibitem{PREV2}{Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition} - \emph{Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng Wang, and Qi Tian} - 2019

\bibitem{PREV3}{Vertex feature encoding and hierarchical temporal modeling in a spatial-temporal graph convolutional network for action recognition} - \emph{Konstantinos Papadopoulos, Enjie Ghorbel, Djamila Aouada, Bj?orn Ottersten} - 2019
 
\bibitem{TempConv}{Interpretable 3D human action analysis with temporal convolutional networks} - \emph{T. S. Kim, A. Reiter} - 2017


\end{thebibliography}


\end{document}