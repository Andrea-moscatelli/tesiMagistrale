%--------------------------------------------------------------
% thesis.tex 
%--------------------------------------------------------------
% Corso di Laurea in Informatica 
% http://if.dsi.unifi.it/
% @Facolt\`a di Scienze Matematiche, Fisiche e Naturali
% @Universit\`a degli Studi di Firenze
%--------------------------------------------------------------
% - template for the main file of Informatica@Unifi Thesis 
% - based on Classic Thesis Style Copyright (C) 2008 
%   Andr\'e Miede http://www.miede.de   
%--------------------------------------------------------------
\documentclass[twoside,openright,titlepage,fleqn,
	headinclude,12pt,a4paper,BCOR5mm,footinclude]{scrbook}
%--------------------------------------------------------------
\newcommand{\myItalianTitle}{Riconoscimento di azioni umane usando tecniche di apprendimento profondo per la stima della posa\xspace}
\newcommand{\myEnglishTitle}{Human action recognition using deep learning techniques for pose estimation\xspace}
% use the right myDegree option
\newcommand{\myDegree}{Corso di Laurea Magistrale in Informatica\xspace}
%\newcommand{\myDegree}{
	%Corso di Laurea Specialistica in Scienze e Tecnologie 
	%dell'Informazione\xspace}
\newcommand{\myName}{Andrea Moscatelli\xspace}
\newcommand{\myProf}{Marco Bertini\xspace}
\newcommand{\myOtherProf}{Correlatore2\xspace}
\newcommand{\mySupervisor}{Nome Cognome\xspace}
\newcommand{\myFaculty}{
	Scuola di Scienze Matematiche, Fisiche e Naturali\xspace}
\newcommand{\myUni}{\protect{
	Universit\`a degli Studi di Firenze}\xspace}
\newcommand{\myLocation}{Firenze\xspace}
\newcommand{\myTime}{Anno Accademico 2018-2019\xspace}
\newcommand{\myVersion}{Version 0.1\xspace}
%--------------------------------------------------------------
\usepackage[italian]{babel}
\usepackage[latin1]{inputenc} 
\usepackage[T1]{fontenc} 
\usepackage[square,numbers]{natbib} 
\usepackage[fleqn]{amsmath}  
\usepackage{ellipsis}
\usepackage{listings}
%\usepackage{subfig}
\usepackage{caption}
\usepackage{appendix}
\usepackage{siunitx}
%--------------------------------------------------------------
\usepackage{dia-classicthesis-ldpkg}
%--------------------------------------------------------------
% Options for classicthesis.sty:
% tocaligned eulerchapternumbers drafting linedheaders 
% listsseparated subfig nochapters beramono eulermath parts 
% minionpro pdfspacing
\usepackage[eulerchapternumbers,linedheaders,beramono,eulermath,
parts]{classicthesis}
%--------------------------------------------------------------

%------------------------- MY PACKAGES -------------------------------------
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{makecell}
\usepackage{caption}
%\usepackage{subcaption}
\usepackage{longtable}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{color, colortbl}
\definecolor{highlightColor}{rgb}{0.88,1,1}
\usepackage{adjustbox}
%--------------------------------------------------------------

\newlength{\abcd} % for ab..z string length calculation
% how all the floats will be aligned
\newcommand{\myfloatalign}{\centering} 
\setlength{\extrarowheight}{3pt} % increase table row height
\captionsetup{format=hang,font=small}
%--------------------------------------------------------------
% Layout setting
%--------------------------------------------------------------
\usepackage{geometry}
\geometry{
	a4paper,
	ignoremp,
	bindingoffset = 1cm, 
	textwidth     = 13.5cm,
	textheight    = 21.5cm,
	lmargin       = 3.5cm, % left margin
	tmargin       = 4cm    % top margin 
}

\lstset{
  	frame=tb,
	language=Matlab,
  	aboveskip=3mm,
  	belowskip=3mm,
  	showstringspaces=false,
  	columns=flexible,
  	basicstyle={\small\ttfamily},
  	numbers=none,
  	breaklines=true,
  	breakatwhitespace=true,
  	tabsize=3
}

%--------------------------------------------------------------
\begin{document}
\frenchspacing
\raggedbottom
\pagenumbering{roman}
\pagestyle{plain}
%--------------------------------------------------------------
% Frontmatter
%--------------------------------------------------------------
\include{titlePage}
\pagestyle{scrheadings}
%--------------------------------------------------------------
% Mainmatter
%--------------------------------------------------------------
\pagenumbering{arabic}
% use \cleardoublepage here to avoid problems with pdfbookmark
%\include{intro} % use \myChapter command instead of \chapter
\tableofcontents
\listoffigures
\cleardoublepage
\thispagestyle{empty}
\begin{flushright}
\null\vspace{\stretch {1}}

%Ricerca scientifica: l?unica forma di poesia che sia retribuita dallo Stato.
%(Jean Rostand)

%Il mondo non ha bisogno di dogmi, ma di libera ricerca.
%(Bertrand Russell)

%Se cerchi la luna nel pozzo, se non altro vi troverai l?acqua.
%(Confucio)

%Non v?è causa d?errore più frequente che la ricerca della verità assoluta.
%(Samuel Butler)

%Nessuno si impegna in una ricerca in fisica con l?intenzione di vincere un premio. È la gioia di scoprire qualcosa che nessuno conosceva prima.
%(Stephen Hawking)

%La ricerca è l?atto di percorrere i vicoli per vedere se sono ciechi.
%(Marston Bates)

%Se sapessimo quello che stiamo facendo non si chiamerebbe ricerca, no?
%(Albert Einstein)

%I veri viaggiatori, non sono persone ricche ma curiose. Non sono alla ricerca di comodità, ma di novità, sorprese.
%(Paolo Crepet)

%     ?I computer sono incredibilmente veloci, accurati e stupidi. Gli uomini sono incredibilmente lenti, inaccurati e intelligenti. L'insieme dei due costituisce una forza incalcolabile.?
%    Albert Einstein

%     ?Continua a piantare i tuoi semi, perché non saprai mai quali cresceranno - forse lo faranno tutti.?
%    Albert Einstein

%     ?L'insegnamento deve essere tale da far percepire ciò che viene offerto come un dono prezioso, e non come un dovere imposto.?
%    Albert Einstein


%Tutti sanno che una cosa è impossibile da realizzare, finché arriva uno sprovveduto che non lo sa e la inventa.
%    Albert Einstein


%La crisi è la più grande benedizione per le persone e le nazioni, perché la crisi porta progressi. La creatività nasce dall'angoscia come il giorno nasce dalla notte oscura. E' nella crisi che sorge l'inventiva, le scoperte e le grandi strategie. Chi supera la crisi supera sé stesso senza essere superato.
%
%Chi attribuisce alla crisi i suoi fallimenti e difficoltà, violenta il suo stesso talento e dà più valore ai problemi che alle soluzioni. La vera crisi, è la crisi dell'incompetenza. L'inconveniente delle persone e delle nazioni è la pigrizia nel cercare soluzioni e vie di uscita. Senza crisi non ci sono sfide, senza sfide la vita è una routine, una lenta agonia. Senza crisi non c'è merito. E' nella crisi che emerge il meglio di ognuno, perché senza crisi tutti i venti sono solo lievi brezze. Parlare di crisi significa incrementarla, e tacere nella crisi è esaltare il conformismo. Invece, lavoriamo duro. Finiamola una volta per tutte con l'unica crisi pericolosa, che è la tragedia di non voler lottare per superarla.
%    Albert Einstein

%La follia sta nel fare sempre la stessa cosa aspettandosi risultati diversi.
%    Albert Einstein

%Imparare è un'esperienza, tutto il resto è solo informazione.
%    Albert Einstein

 %    Albert Einstein


%     ?I computer sono incredibilmente veloci, accurati e stupidi. Gli uomini sono incredibilmente lenti, inaccurati e intelligenti. L'insieme dei due costituisce una forza incalcolabile.?
%    Albert Einstein

\emph{"I computer sono incredibilmente veloci, accurati e stupidi. \\Gli uomini sono incredibilmente lenti, inaccurati e intelligenti. \\L'insieme dei due costituisce una forza incalcolabile." \break --- Albert Einstein} \vspace{\stretch{2}}\null
\end{flushright}
\cleardoublepage


% -----------------------------------------------------------------------------------------------------------------------------

%% Deep learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{L'apprendimento automatico}
Quando parliamo di apprendimento automatico (in inglese \emph{machine learning}), ovvero \emph{l'apprendimento delle macchine}, viene spontaneo domandarsi ``Come impara una macchina?".

L'apprendimento viene definito come un processo iterativo che permette di mutare le proprie conoscenze a seconda delle informazioni che vengono raccolte.

Nel machine learning l'apprendimento segue esattamente le stesse dinamiche ma a seconda di come gestiamo le informazioni a nostra disposizione vengono definite 4 macro classi.

\begin{itemize}
\item \textbf{Apprendimento supervisionato (Supervised Learning)} - questo primo caso, può essere paragonato al processo di apprendimento che si avrebbe con un'insegnante \emph{onnisciente} che supervisiona il suo alunno e che interromperà l'apprendimento solo quando quest'ultimo raggiungerà un livello accettabile del compito da imparare.

L'apprendimento supervisionato lo si ha cioè quando nel nostro \emph{dataset} di allenamento sono a disposizione  sia i dati di input $X$ che quelli di output $Y$ e vogliamo insegnare al nostro algoritmo quella funzione $f$ tale che 
$$
Y= f(X).
$$

L'obbiettivo è quello di approssimare sufficientemente bene la funzione $f$ di modo da poter prevedere correttamente il valore $Y_i$ per un futuro $X_i$ non compreso nel nostro dataset di allenamento.

I problemi di apprendimento supervisionato si possono dividere in problemi di:
\begin{enumerate}
\item \emph{Classificazione}, ovvero insegnare ad una macchina a categorizzare. In questo scenario, nel nostro dataset di allenamento ad ogni dato di input sarà associata un'etichetta che ne indica la categoria appartenente. Più grande sarà il dataset e maggiore sarà l'informazione a disposizione dell'algoritmo per imparare.
\item \emph{Regressione} - ovvero insegnare ad una macchina a predire il valore di ciò che sta analizzando. A differenza della classificazione, in questo caso il risultato in output sarà un valore continuo e non un valore categorico. Ad esempio, dati in input le ore di studio di uno studente per la preparazione di un esame e le rispettive ore di sonno, prevedere la probabilità di superamento dell'esame in questione, oppure date in input la superficie e la posizione di un appartamento, prevederne il valore di mercato.
\end{enumerate}
\item \textbf{Apprendimento non supervisionato (Unsupervised Learning)} - in questo tipo di apprendimento, al contrario di quello supervisionato, non viene fornita nessuna etichetta di output $Y$ per i nostri dati $X$. L'obbiettivo che quindi ci si pone in questo tipo di situazione è scovare delle relazioni tra i dati analizzati. In questo caso non c'è nessun ``insegnante" a guidare l'apprendimento e non ci sono risposte corrette o sbagliate, l'algoritmo deve cercare di scoprire ``da solo" se ci sono delle strutture decifrabili nei dati. 

I problemi di apprendimento non supervisionato si possono dividere in:
\begin{enumerate}
\item \emph{Raggruppamento} - detto anche \emph{clustering}, si utilizzano quando è necessario raggruppare i dati che presentano caratteristiche simili. In questo caso l'algoritmo non fa uso di dati categorizzati, come visto in precedenza, ma estrae una regola di raggruppamento secondo caratteristiche che ricava dai dati stessi.
\item \emph{Associazione} - strettamente legata al \emph{Data Mining}, questa classe di problemi è utile in tutti i casi per i quali siamo interessati a scoprire regole induttive nei dati analizzati, ad esempio la tendenza nei consumatori di un certo supermercato ad acquistare il prodotto $A$ dopo aver acquistato il prodotto $B$. Ci si pone quindi l'obbiettivo di identificare schemi frequenti, associazioni, correlazioni o strutture casuali fra gli \emph{item} di un database relazionale, cercando si scoprire le regole che predicono l'evento di un certo item in base agli eventi degli item ad esso legati.
\end{enumerate}


\item \textbf{Apprendimento parzialmente supervisionato (Semi-Supervised Learning)} - questo tipo di apprendimento è una sorta di via di mezzo dei primi due, ovvero un apprendimento utilizzato quando solo alcuni dei nostri dati in input $X$ sono etichettati, mentre la maggior parte di essi non lo è. 

In questa categoria di apprendimento ricadono la stragrande maggioranza dei problemi di machine learning e questo perchè etichettare dati è un processo lungo e costoso soprattutto in ambito \emph{Big Data}, campo ideale per il machine learning.

\item \textbf{Apprendimento con rinforzo (Reinforcement Learning)} - questa metodologia di apprendimento simula il processo di apprendimento umano per tentativi ed errori. L'algoritmo si adatta gradualmente tramite un sistema di valutazione basato su ricompense e penalità a seconda della decisione presa. L'obbiettivo è quello di evolvere l'algoritmo massimizzando le ricompense ricevute.
\end{itemize}

\section{Deep learning}
Il \emph{deep learning} (o \emph{apprendimento profondo} in italiano) è una delle tecniche di machine learning che ``insegna" ai computer una cosa estremamente naturale al cervello umano, ovvero \emph{imparare per esempi}. 

Il deep learning si basa su diversi livelli di rappresentazione, corrispondenti a gerarchie di caratteristiche di fattori o concetti, dove i concetti di alto livello sono definiti sulla base di quelli di basso. In altre parole per apprendimento profondo si intende un insieme di tecniche basate su reti neurali artificiali organizzate in diversi strati, dove ogni strato calcola i valori per quello successivo affinché l'informazione venga elaborata in maniera sempre più completa.

Il deep learning è la tecnologia chiave grazie alla quale abbiamo oggi automobili che si guidano da sole, riconoscimento e controllo vocale dei device, riconoscimento di tumori, traduzioni automatiche, colorazione automatica di vecchi filmati in bianco e nero e molte altre cose ritenute impossibili solo fino a pochi anni fa.

Nel deep learning un computer impara, ad esempio, un task di classificazione direttamente da immagini, testi e suoni e può raggiungere prestazioni allo stato dell'arte superando talvolta persino l'accuratezza umana. I modelli vengono allenati usando grandi dataset etichettati seguendo un'apprendimento di tipo supervisionato ed il sistema centrale sul quale è basato il deep learnig è la \emph{rete neurale}.

\section{Le reti neurali}
Nel campo dell'apprendimento automatico, una rete neurale (\emph{neural network}
o \emph{NN}) è un modello di calcolo ispirato al sistema di elaborazione delle informazioni
tipico del cervello degli esseri viventi, nel quale tanti piccoli elementi base,
denominati \emph{neuroni} (figura \ref{fig:neurone}), sono interconnessi e collaborano tra loro per poter eseguire un determinato compito. 

\begin{figure}[htbp]
\centering
\includegraphics[width=1\textwidth]{imgs/comparazioneNeurone}
\caption{Un neurone biologico (sinistra) ed un neurone artificiale (destra) a confronto. La struttura del neurone artificiale si ispira molto a quella del neurone biologico. }
\label{fig:neurone}
\end{figure}

Queste reti costituiscono ad oggi la miglior soluzione per una vasta gamma di problemi grazie alla loro capacità di saper aprossimare pressoché qualsiasi funzione $f : \mathbb{R}^n \rightarrow  \mathbb{R}$, a patto di scegliere la giusta quantità di neuroni in fase di progettazione.

La struttura di una rete neurale si sviluppa per livelli ed ogni livello, eccetto quello di \emph{input} e quello di \emph{output}, viene chiamato \emph{hidden layer}. Ogni hidden layer può essere composto da un qualsiasi numero di neuroni, che prendono il nome di \emph{hidden units} e maggiore sarà il numero di hidden units, maggiore sarà la \emph{capacità espressiva} della rete, ovvero la capacità di approssimare correttamente una qualsiasi funzione. 

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{imgs/hiddenLayers}
\caption{Esempio di una \emph{fully connected network}. In questo esempio di rete abbiamo 2 \emph{hidden layer} (in viola chiaro) composti da 5 \emph{hidden units} ciascuno (in viola scuro). Maggiore è il numero di hidden units e maggiore è la capacità espressiva della rete. }
\label{fig:hiddenLayer}
\end{figure}

Sono proprio gli hidden layer a dare \emph{profondità} alla rete ed è per questo che si parla sia di \emph{apprendimento profondo} che di \emph{reti neurali profonde} (o \emph{deep neural network} in inglese). 

Negli ultimi anni le deep neural network hanno avuto una diffusione sorprendente grazie non solo alla loro adattabilità ad una vasta gamma di problemi, ma anche alla rapida evoluzione vista nel mondo delle GPU, che ha reso parallelizzabile e velocizzato notevolmente l'addestramento di queste reti.

I livelli che possiamo trovare all'interno di una rete neurale sono di diverso tipo, come diverse possono essere le connessioni dei neuroni fra un livello e l'altro. Sono proprio queste diversità a determinare il tipo di rete neurale e le più famose tipologie sono:
\begin{itemize}
\item \textbf{Fully connected network} - in questo tipo di reti ogni neurone è connesso con tutti quelli del livello precedente (come in figura \ref{fig:hiddenLayer}). Come si può facilmente intuire, il numero dei parametri (ovvero la capacità espressiva) della rete cresce esponenzialmente al crescere della sua struttura ed il rischio che si corre è quello di creare una rete talmente efficiente nell'ottimizzare la funzione generatrice dei dati di allenamento da essere troppo specifica per essa, non riuscendo a predire correttamente i valori al di fuori di quel dataset. Questa problematica, ben nota nel mondo del machine learning, prende il nome di \emph{overfitting}.
\item \textbf{Convolutional neural network (CNN)} - queste reti sono composte principalmente da \emph{layer convoluzionali}, ovvero layer nei quali ogni neurone è connesso solo ad una piccola regione localizzata del livello precedente (figura \ref{fig:convNetwork}). Questa particolare tipologia di connessione è estremamente efficiente nell'ambito della visione computerizzata ed è in grado di \emph{sintetizzare} correttamente un'immagine senza perdita d'informazione. Questa sintetizzazione viene indicata in gergo col termine di \emph{features extraction}, ovvero l'estrazione delle caratteristiche principali di un certo dato in input.
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=1\textwidth]{imgs/convolutionalNetwork}
\caption{Un esempio di rete convoluzionale per l'analisi di un'immagine. \emph{Conv} = ``Convolution layer", moltiplicazione vettoriale della porzione di layer al quale ogni neurone è collegato; \emph{Pool} = ``Pooling layer", sintetizzazione della porzione di layer connessa; \emph{FC} = ``Fully connected layer"; \emph{Softmax} = operazione conclusiva per la decisione del valore di output.}
\label{fig:convNetwork}
\end{figure}

Le reti neurali sopra citate, seppur molto efficienti nei loro ambiti, presentano però un problema di \emph{memoria} ovvero, non sono state ideate per processare una serie temporale di dati. L'unica soluzione che hanno è quella di trattare tutta la serie temporale come un unico input e processarlo interamente in un unico ciclo. Questa primordiale soluzione non ha ovviamente preso campo vista la mancanza di generalità.

Esistono però delle reti neurali che, a differenza dalle precedenti, riescono ad analizzare una sequenza temporale in maniera simile al ragionamento umano, ovvero mantenendo una memoria del ``passato" ed in base a quest'ultima adattare le scelte e la valutazione dello stato corrente. Queste particolari reti vengono chiamate \emph{reti neurali ricorrenti} (o \emph{recurrent neural networks}).

\section{Le reti neurali ricorrenti}
Le reti neurali ricorrenti (\emph{recurrent neural network} o \emph{RNN}) sono una classe
di rete neurali artificiali caratterizzate da una struttura ciclica, ovvero una struttura dove i layer possono essere connessi con loro stessi o con layer precedenti (figura \ref{fig:reteRicorrente}). 
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{imgs/esempioRNN}
\caption{Un esempio di rete ricorrente con due hidden layers.}
\label{fig:reteRicorrente}
\end{figure}

Questa struttura ciclica permette di mantenere una sorta di \emph{stato di memoria} della rete, utilizzato per processare ogni elemento della sequenza in input. Questa struttura si è rivelata ideale per compiti di analisi predittiva su sequenze temporali, quali possono essere ad esempio il riconoscimento della grafia, l'analisi di video, il riconoscimento vocale o l'analisi di testi. 

Per quanto riguarda l'addestramento di queste reti, esso
avviene con la stessa modalità delle reti neurali standard, ovvero i pesi dei neuroni che compongono la rete vengono regolati tramite un processo iterativo con lo scopo di minimizzare l'errore di classificazione prodotto sul dataset di allenamento. 

Tipicamente, l'algoritmo che regola l'apprendimento delle RNN viene chiamato \emph{Backpropagation Throw Time (BPTT)} e non è altro che la versione ``ricorrente" del classico \emph{backpropagation algorithm} usato per le reti naurali standard.

L'applicazione del BPTT può essere vista come l'applicazione del backpropagation algorithm allo ``srotolamento" della RNN (figura \ref{fig:srotolamentoRNN}).

\begin{figure}[htbp]
\centering
\includegraphics[width=1\textwidth]{imgs/srotolamentoRNN.jpg}
\caption{Esempio di ``srotolamento" di una rete ricorrente, ovvero la trasformazione da una rete ricorrente ad una combinazione equivalente di reti neurali semplici.}
\label{fig:srotolamentoRNN}
\end{figure}

La lunghezza delle sequenze che queste reti sono in grado di analizzare è teoricamente infinita ma di fatto, per sequenze troppo lunghe, si presenta un problema legato alla precisione finita dei calcolatori, ovvero la \emph{scomparsa del gradiente} (in inglese \emph{vanishing gradient problem}).

Il concetto alla base del backpropagation algorithm e infatti quello di aggiornare iterativamente ogni parametro del modello in maniera proporzionale alla derivata parziale della \emph{loss function} (ovvero la funzione d'errore di classificazione) rispetto al parametro stesso. 

Poiché ogni gradiente sarà un valore compreso fra 0 e 1 e l'aggiornamento dei parametri ai vari livelli verrà propagato tramite la \emph{regola della catena}, ovvero 
$$
\frac{\partial a}{\partial b} = \frac{ \partial a}{\partial c} \cdot \frac{\partial c}{\partial b},
$$
il prodotto dei gradienti decrescerà esponenzialmente al crescere della profondità della rete, col rischio di sparire se dovesse diventare più piccolo della precisione minima del calcolatore.

La profondità delle reti ricorrenti è strettamente legata alla lunghezza della sequenza analizzata e raggiunge velocemente un numero di livelli intrattabile per l'algoritmo di backpropagation, rendendole di fatto inutilizzabili per i casi nel mondo reale.

Per ovviare a questo problema, nel 1997 Sepp Hochreiter e Jürgen Schmidhuber proposero un nuovo tipo di rete ricorrente chiamato \emph{Long short-term memory (LSTM)} \cite{LSTM}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{imgs/lstmFull}
\caption{Esempio di ``srotolamento" di una rete LSTM. In questa figura ogni unità di rete ``A'' prende in input al tempo $t$ un elemento $X_t$ della sequenza temporale $X$ e restituisce uno stato in output $h_t$. \emph{tanh} e $\sigma$ sono le funzioni di tangente iperbolica e sigmoidale rispettivamente, mentre ``$+$" e ``$\times$" sono rispettivamente le operazioni vettoriali di somma e moltiplicazione.}
\label{fig:srotolamentoLSTM}
\end{figure}
\section{Le reti LSTM}
Le reti Long Short-Term Memory, normalmente chiamate LSTM, sono delle particolari reti ricorrenti in grado di imparare non solo le dipendenze \emph{a lunga distanza} presenti in una sequenza in input ma anche di risolvere il problema della scomparsa del gradiente. 

Come ogni altra rete ricorrente anche per le LSTM è possibile srotolarne la struttura ed ottenere una catena di ``pezzi di rete" chiamati anche \emph{unità} (figura \ref{fig:srotolamentoLSTM}). 

\begin{figure}[!ht] 
  \subfloat[Cell State\label{fig:cellState}]{% 
    \includegraphics[width=.5\textwidth]{imgs/LSTMcellState}
  } 
  \hfill 
  \subfloat[Gate\label{fig:gate}]{% 
    \includegraphics[width=.15\textwidth]{imgs/LSTMgate}
  } 
  \caption{I concetti chiave di una LSTM.} 
  \label{fig:LSTMcellStateAndGate}
\end{figure}



Il funzionamento di un'unità ricorrente LSTM ruota tutto intorno ai concetti
di \emph{cell state} (figura \ref{fig:cellState}) e di \emph{gate} (figura \ref{fig:gate}), ovvero:
\begin{itemize}
\item cell state: funziona come un nastro trasportatore in grado di memorizzare informazioni, rendendole disponibili durante tutta l'analisi della sequenza;
\item gate: sono delle NN più piccole che stabiliscono, attraverso una funzione sigmoidale, quali informazioni possono continuare ad essere memorizzate nel cell state e quali invece devono essere ``dimenticate". 
\end{itemize}


%\begin{figure}
%\centering
%\begin{subfigure}{.8\textwidth}
%  \centering
%  \includegraphics[width=.6\textwidth]{imgs/LSTMcellState}
%  \caption{cell state}
%  \label{fig:cellState}
%\end{subfigure}%
%\begin{subfigure}{.2\textwidth}
%  \centering
%  \includegraphics[width=.5\textwidth]{imgs/LSTMgate}
%  \caption{gate}
%  \label{fig:gate}
%\end{subfigure}
%\caption{I concetti chiave di una LSTM.}
%\label{fig:LSTMcellStateAndGate}
%\end{figure}





Esistono principalmente tre diverse tipologie di gate che regolano il flusso informativo all'interno di una unità LSTM: \emph{forget gate, input gate} e \emph{output gate}.\\

\textbf{Forget gate} -  Il forget gate (figura \ref{fig:forgetGate}) decide quali informazioni mantenere all'interno del cell state e quali invece saranno ``dimenticate". 

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth]{imgs/LSTMforgetGate}
\caption{Forget gate di una LSTM}
\label{fig:forgetGate}
\end{figure}

In formula, al $t$-esimo passo, il forget gate sarà
$$
f_t = \sigma(W_f \cdot [h_{t-1},x_t] + b_f)
$$

dove:
\begin{itemize}
\item $h_{t-1}$ è lo stato, nonché l'output, della LSTM al passo precedente
\item $x_t$ è il $t$-esimo elemento della sequenza in input
\item $W_f$ e $b_f$ sono rispettivamente la matrice dei pesi e il bias del forget gate
\item $\sigma$ è la funzione sigmoidale.
\end{itemize} 
\ \\
\textbf{Input gate} - Questo gate decide quali nuove informazioni vogliamo registrare nel nostro cell state e la sua funzione può essere suddivisa in due parti (figura \ref{fig:inputGate1}):
\begin{enumerate}
\item attraverso un'operazione sigmoidale, seleziona cosa dell'input corrente deve essere mantenuto. In formula,
$$
i_t = \sigma (W_i \cdot [h_{t-1},x_t] + b_i).
$$
\item trasforma il precedente stato della LSTM con un'operazione \emph{tanh}, ovvero
$$
\widetilde{C} = tanh(W_C\cdot [h_{t-1},x_t] + b_C)
$$
\end{enumerate}

%\begin{figure}
%\centering
%\begin{subfigure}{.45\textwidth}
%  \centering
%  \includegraphics[width=\textwidth]{imgs/LSTMinputGate1}
%  \caption{Selezione dell'input corrente e trasformazione dello stato LSTM precedente.}
%  \label{fig:inputGate1}
%\end{subfigure}
%\hfill
%\begin{subfigure}{.45\textwidth}
%  \centering
%  \includegraphics[width=\textwidth]{imgs/LSTMinputGate2}
%  \caption{Combinazione col forget gate per il calcolo del nuovo stato del cell state.}
%  \label{fig:inputGate2}
%\end{subfigure}
%\caption{Input gate.}
%\label{fig:LSTMinputForgetGate}
%\end{figure}

\begin{figure}[!ht] 
  \subfloat[Selezione dell'input corrente e trasformazione dello stato LSTM precedente.\label{fig:inputGate1}]{% 
    \includegraphics[width=.45\textwidth]{imgs/LSTMinputGate1}
  } 
  \hfill 
  \subfloat[Combinazione col forget gate per il calcolo del nuovo stato del cell state.\label{fig:inputGate2}]{% 
    \includegraphics[width=.45\textwidth]{imgs/LSTMinputGate2}
  } 
  \caption{Input gate.} 
  \label{fig:LSTMinputForgetGate}
\end{figure}



%\begin{figure}
%  \centering
%  \begin{tabular}{@{}c@{}}
%    \includegraphics[width=\textwidth]{imgs/LSTMinputGate1}
%    \small (a) Selezione dell'input corrente e trasformazione dello stato LSTM precedente.
%  \end{tabular}
%
%  \vspace{\floatsep}
%
%  \begin{tabular}{@{}c@{}}
%    \includegraphics[width=\textwidth]{imgs/LSTMinputGate2}
%    \small (b) Combinazione col forget gate per il calcolo del nuovo stato del cell state.
%  \end{tabular}
%
%  \caption{Input gate.}
%  \label{fig:LSTMcellStateAndGate}
%\end{figure}

I due vettori risultanti vengono dapprima moltiplicati fra loro e successivamente sommati al cell state combinandoli col forget gate (figura \ref{fig:inputGate2}). In formula
$$
C_t = f_t * C_{t-1} + i_t * \widetilde{C}_t.
$$

\textbf{Output gate} - Questo ultimo gate si occupa infine di decidere cosa vogliamo restituire in output ad ogni passo $t$ (figura \ref{fig:outputGate}). Il suo compito è quello di combinare una versione filtrata dello stato precedente con una trasformazione del cell state. 

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth]{imgs/LSTMoutputGate}
\caption{Output gate di una LSTM}
\label{fig:outputGate}
\end{figure}

L'operazione applicata al filtraggio dello stato precedente è quella sigmoidale, di modo da selezionarne solo i componenti desiderati, ovvero
$$
o_t = \sigma(W_o[h_{t-1},x_t] + b_o)
$$
mentre quella applicata al cell state è $tanh$ di modo da proiettarne i valori nel range (-1,1), ottenendo
$$
h_t = 0_t * tanh(C_t)
$$


Il valore $h_t$ così calcolato sarà l'output al passo $t$ della rete LSTM e l'input del passo successivo, insieme allo stato corrente del cell state $C_t$.

Quella appena descritta è una classica LSTM ma in alcuni articoli scientifici è possibile trovare delle leggere varianti, come le \emph{``peephole" LSTM} \cite{peepholeLSTM} o le \emph{Gated Recurrent Unit (GRU)} \cite{GRU}. Ad ogni modo il concetto alla base di queste varianti resta lo stesso.

 
 
 
 %% Lavori precedenti %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Lavori precedenti}
Il riconoscimento di azioni corporee tramite l'analisi della \emph{posa} umana o \emph{skeleton} (vedremo nel prossimo capitolo più in dettagli di cosa si tratta) sta attraendo recentemente una notevole attenzione nel mondo della visione computerizzata. Il suo successo è senz'altro dovuto non solo agli ottimi risultati ottenuti, ma anche alla sua efficiente semplificazione della struttura umana riducendo di fatto i costi computazionali e le risorse necessarie allo stoccaggio dati. Normalmente la posa viene calcolata utilizzando  sensori 2D o 3D posizionati in prossimità dei giunti corporei (gomiti, ginocchi, spalle,...), oppure con algoritmi appositi in grado di estrarla da normali video RGB.

Normalmente ci sono due approcci per il riconoscimento di azioni umane facendo uso dalla posa: gli approcci ``\emph{manuali}" e quelli \emph{deep learning}.

Negli approcci \emph{manuali} si cerca di categorizzare le azioni umane seguendo pattern fisici intuitivi, come le posizioni degli arti o le correlazioni temporali fra essi, ad esempio il movimento ondulatorio delle gambe durante una camminata o una corsa. 

Per quanto riguarda invece gli approcci \emph{deep learning} le tipologie di azioni corporee vengono categorizzate in maniera automatica direttamente dai dati stessi. Le reti neurali ricorrenti, come le LSTM, o quelle di tipo \emph{convoluzionali-temporali}, come quella in \cite{TempConv}, si sono rivelate estremamente efficaci per il riconoscimento di pattern temporali in un video, ottenendo ottimi risultati nella categorizzazione delle azioni.

Esiste infine un terzo approccio che recentemente sta riscuotendo un discreto interesse, ovvero quello basato su \emph{grafi} che punta ad esprimere contemporaneamente sia le relazioni temporali che quelle spaziali dei giunti corporei. Questa tecnica punta a rappresentare la sequenza di pose in un video come un grafo composto da archi temporali e spaziali, considerando cioè rispettivamente le relazioni inter ed intra-frame.

%Molti dei lavori scientifici effettuati fino ad oggi allenano le proprie reti col dataset NTU-RGB+D \cite{NTURGB} ma fra di loro è necessario fare una distinzione a seconda delle informazioni utilizzate. fra chi utilizza l'informazione delle pose messa a disposizione da questo dataset (estratta con sensori 2D o 3D), chi solo i semplici video RGB e chi entrambi.

Ad oggi i migliori risultati sulla categorizzazione dei movimenti corporei possono essere riassunti nei seguenti articoli scientifici:
\begin{itemize}
\item \textbf{Recognizing Human Actions as the Evolution of Pose Estimation Maps} - \emph{Mengyuan Liu, Junsong Yuan} - CVPR 2018 \cite{PREV1}
\item \textbf{Action Machine: Rethinking Action Recognition in Trimmed Videos} - \emph{Jiagang Zhu, Wei Zou, Liang Xu, Yiming Hu, Zheng Zhu, Manyu Chang,
Junjie Huang, Guan Huang, Dalong Du} - arXiv:1812.05770v2  [cs.CV], 2018  \cite{PREV2}
%\item \textbf{Infrared and 3D skeleton feature fusion for RGB-D action recognition} - \emph{Alban Main de Boissiere e Rita Noumeir} - 2020  \cite{PREV3}
\end{itemize}

Prima di entrare nel dettaglio dei due articoli scientifici occorre precisare che in entrambi i lavori l'accuratezza dei risultati viene espressa attraverso le metriche \emph{cross subject} e \emph{cross view} del dataset \emph{NTU-RGB+D}. Nel capitolo 5 spiegheremo dettagliatamente la struttura di questo dataset e le sue metriche, ma giusto per una migliore comprensione delle sezioni seguenti occorre precisare che:
\begin{itemize}
\item Il dataset NTU-RGB+D rappresenta un insieme di video ottenuti riprendendo 40 diversi attori, ognuno da 3 angolazioni diverse.
\item il criterio \textbf{cross subject} rappresenta lo splitting training/test ottenuto separando i 40 attori in due gruppi distinti, uno per il training ed uno per il test, di modo da avere un insieme di test composto da attori esclusivi e non utilzzati nell'insieme di training.
\item il criterio \textbf{cross view} invece definisce l'insieme di training e quello di test separando i video a seconda della loro angolazione di ripresa, ovvero tutti i video ripresi frontalmente fanno parte dell'insieme di test, mentre tutti gli altri dell'insieme di training.
\end{itemize}

Vediamo adesso più nel dettaglio come questi lavori hanno affrontato il problema del riconoscimenti di azioni umane.

\section{Recognizing Human Actions as Evolution of Pose Estimation Maps (2018)}
L'idea alla base di questo lavoro è quella di creare delle \emph{mappe di stima della posa} umana dalle quali estrarre delle \emph{heatmap} globali e delle \emph{predizioni di pose} e una volta ottenute queste due componenti, combinare le loro caratteristiche temporali e spaziali di modo da identificare l'azione svolta.

Più precisamente, indicando con $\mathcal{Y}_k \in \{x,y\}$ le cordinate del $k$-esimo giunto corporeo, possiamo definire un'intera struttura corporea come $\boldsymbol{\mathcal{Y}}= \{\mathcal{Y}_1, ... , \mathcal{Y}_k, ..., \mathcal{Y}_K\}$, dove $K$ è il numero totale dei tipi di giunti corporei considerati.

Addestrando un classificatore multiclasse $g_t^k$ per predire la posizione del $k$-esimo giunto corporeo al passo $t$, otteniamo per ogni punto \textbf{z} dell'immagine una mappa di stima relativa al giunto $k$,
$$
\boldsymbol{B}^k_t(\mathcal{Y}_k = \boldsymbol{z}) = g_t^k \left(\boldsymbol{f_z}; \bigcup_{i=1, ..., K}\psi(\boldsymbol{z}, \boldsymbol{B}_{t-1}^i)\right)
$$
dove $\boldsymbol{f_z}$ è la \emph{color feature} per il punto \textbf{z}, $\boldsymbol{B}_{t-1}^i$ è la mappa stimata da $g^i_{t-1}$ al passo precedente, $\bigcup$ è l'operatore di concatenazione vettoriale e $\psi$ è la funzione di estrazione della \emph{feature} interessata partendo dalla mappa ottenuta al passo precedente. Dopo $T$ passi, le mappe ottenute vengono utilizzate per stimare la posizione dei giunti.

Partendo dalle $K$ mappe così ottenute per ogni frame $n$ del video, ovvero $\{\boldsymbol{B}_{T}^{1,n}, ..., \boldsymbol{B}_{T}^{K,n}\}$, vengono calcolate una heatmap $\boldsymbol{G}_{n}$ ed una posa $\mathcal{L}_n$ che rappresentano globalmente la figura umana nell'immagine (figura \ref{fig:heatmapAndPose}). La heatmap $\boldsymbol{G}_{n}$ sarà
$$
\boldsymbol{G}_{n} = \frac{1}{K}\sum_{k=1}^K \boldsymbol{B}_{T}^{k,n},
$$
mentre la posa $\mathcal{L}_n$ sarà quell'insieme di punti dell'immagine $\{\boldsymbol{z}^{k,n}\}^K_{k=1}$ per i quali
$$
\boldsymbol{z}^{k,n} = \underset{\boldsymbol{z} \in \mathcal{Z}}{arg max} \{\boldsymbol{B}_T^{k,n} (\mathcal{Y}_k = \boldsymbol{z})\},
$$
dove $ \mathcal{Z} \in \mathbb{R}^2$ è l'insieme di tutti i punti dell'immagine.

\begin{figure}[t]
\centering
\includegraphics[width=1.1\textwidth]{imgs/heatmapPoseCobination}
\caption{Processo di creazione della heatmap e della posa a partire da un singolo frame del video. Ogni giunto corporeo viene stimato singolarmente per poi essere successivamente combinato agli altri.}
\label{fig:heatmapAndPose}
\end{figure}

Quello che abbiamo fatto è stato cioè trasformare un video in un sequenza di heatmap e pose, a questo punto dobbiamo però sintetizzarle in un unico elemento rappresentativo di più facile manipolazione.

\begin{figure}[b]
\centering
\includegraphics[width=1\textwidth]{imgs/combinazioneHeatmaps}
\caption{Sintetizzazione temporale delle heatmap.}
\label{fig:sintTempHeatmap}
\end{figure}

Gli autori del lavoro mettono a confronto 3 diversi tipi di sintetizzazione:
\begin{itemize}
\item \textbf{sintetizzazione temporale delle heatmap} - Data una sequenza di $N$ heatmap $\boldsymbol{\mathcal{V}}_G = \{ \boldsymbol{G}_1, ... \boldsymbol{G} _n, ..., \boldsymbol{G}_N\}$ dove $\boldsymbol{G}_n \in \mathbb{R}^{P\times Q}$ è quella relativa all'$n$-esimo frame con $P$ righe e $Q$ colonne, allora la sequenza $\boldsymbol{G}_{1:n}$ può essere mappata in un vettore
$$
\boldsymbol{v}_n = V(\frac{1}{n}\sum^n_{i=1}\boldsymbol{G}_i),
$$
dove la funzione $V$ trasforma una matrice in un vettore $\boldsymbol{v}_n \in \mathbb{R}^{(P\cdot Q)\times 1}$ (figura \ref{fig:sintTempHeatmap}).

\item \textbf{sintetizzazione spazio-temporale delle heatmap} - Partizionando ogni heatmap $\boldsymbol{G}_n$ in in $P$ righe, ovvero $ \boldsymbol{G}_n= [(\boldsymbol{p}_1^T, ..., \boldsymbol{p}_s^T, ..., \boldsymbol{p}_P^T]^T$ o in $Q$ colonne, ovvero $ \boldsymbol{G}_n= [(\boldsymbol{q}_1, ..., \boldsymbol{q}_s, ..., \boldsymbol{q}_Q]$ ed usando la precedente funzione $V$ per mappare $(\boldsymbol{p}_{1:s})^T$ e $\boldsymbol{q}_{1:s}$ in $\boldsymbol{v}_{s}^P$ e $\boldsymbol{v}_{s}^Q$ rispetivamente, si può ottenere due vetori $\boldsymbol{u^p},\boldsymbol{u^q}$ rappresentativi del frame attraverso la seguente funzione obbiettivo:
\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{imgs/sintetizzazioneSpazioTemporaleHeatmpas}
\caption{Sintetizzazione spazio-temporale delle heatmap.}
\label{fig:spatialTemporalRank}
\end{figure}
\begin{align*}
\underset{\boldsymbol{u}^\eta}{arg\ min}\ & \frac{1}{2}||\boldsymbol{u}^\eta||^2 + W \sum_{\forall_{i,j}\ v^\eta_{s_i} \succ v^\eta_{s_j}}\epsilon_{ij}, \\
s.t. \  & (\boldsymbol{u}^\eta)^T \cdot (\boldsymbol{v}^\eta_{s_i} - \boldsymbol{v}^\eta_{s_j}) \geq 1 - \epsilon_{ij} \\
& \epsilon_{ij} \geq 0 
\end{align*}
dove $\eta \in \{\boldsymbol{p,q}\}$, $v^\eta_{s_i} \succ v^\eta_{s_j}$ indica la successione temporale fra $v^\eta_{s_i}$ e $v^\eta_{s_j}$, $\boldsymbol{u^p}\in \mathbb{R}^{Q}$ e $\boldsymbol{u^q}\in \mathbb{R}^{P}$. Per gli $N$ frame vengono concatenati in ordine temporale i vettori così calcolati, ottenendo $\boldsymbol{U^p} \in \mathbb{R}^{Q\times N}$ e $\boldsymbol{U^q} \in \mathbb{R}^{P\times N}$. La matrice finale $\boldsymbol{U}$ sarà l'unione di queste concatenazioni, ovvero $[(\boldsymbol{U^p})^T,\boldsymbol{U^q})^T]^T$, cioè una matrice che manterrà sia le caratteristiche temporali che quelle spaziali del movimento ripreso nel video. L'intero processo è schematizzato in figura \ref{fig:spatialTemporalRank}.

\item \textbf{sintetizzazione spazio-temporale delle pose} - Per comprendere come viene creata questa sintetizzazione, supponiamo di avere una sequenza di pose $\boldsymbol{\mathcal{V}}_ \mathcal{L}= \{ \mathcal{L}_1, ..., \mathcal{L}_n, ..., \mathcal{L}_N \}$ dove $\mathcal{L}_n = \{ \boldsymbol{z}^{k,n} \}^K_{k=1}$ e $\boldsymbol{z}^{k,n} = (x^{k,n},y^{k,n})$, ovvero le coordinate orizzontali e verticali del $k$-esimo giunto corporeo. Per codificare le caratteristiche di tale sequenza, ogni posa viene trasformata in una successione di distanze fra giunti consecutivi, seguendo l'ordine rappresentato in figura \ref{fig:ordineGiunti}. Ripetendo questo processo per ogni frame e concatenando i risultati ottenuti, si ottiene un'immagine rappresentativa delle caratteristiche spazio-temporali dell'evoluzione della posa nel video. L'intero processo è rappresentato in figura \ref{fig:ordineGiunti}.

\begin{figure}[htb]
\centering
\includegraphics[width=1\textwidth]{imgs/sintetizzazioneDellePose}
\caption{Sintetizzazione spazio-temporale delle heatmap.}
\label{fig:ordineGiunti}
\end{figure}
\end{itemize}

Gli esperimenti effettuati dagli autori sono molteplici e una delle caratteristiche principali sulla quale è necessario porre attenzione è la tipologie delle pose utilizzate: in alcuni casi hanno utilizzato delle pose 3D ottenute con delle telecamere di profondità, in altri casi hanno utilizzato pose 2D.


Il dataset NTU-RGB+D è uno dei dataset utilizzati in questo lavoro ed i risultati ottenuti per esso sono riassunti in tabella \ref{tab:risultati4}.

\begin{table}[ht]
\centering
\begin{tabular}{| c | c | c | c | c | c |}
\hline
Pose & \thead{Accuratezza  \\ cross-view} & \thead{Accuratezza  \\ cross-subject} \\
\hline
2D   &    84.21\%     &   78.80           \\ \hline
3D   &    95.26\%    &   91.71\%            \\ \hline
\end{tabular}
\caption{Risultati ottenuti da  ``Recognizing Human Actions as the Evolution of Pose Estimation Maps - (2018)"}
\label{tab:risultati4}
\end{table}

\section{Action Machine: Rethinking Action Recognition in Trimmed Videos (2018)}
In questo lavoro gli autori categorizzano le azioni riprese nei video combinando un'analisi sul video e una sulla posa estratta da quest'ultimo.

\begin{figure}[htb]
\centering
\includegraphics[width=1\textwidth]{imgs/actionMachineStructureIta}
\caption{Schematizzazione di \emph{Action Machine} }
\label{fig:actionMachineStructure}
\end{figure}

Come mostrato in figura \ref{fig:actionMachineStructure}, \emph{Action machine} (questo è il nome che gli autori hanno dato a questo algoritmo) si suddivide in fasi:
\begin{enumerate}
\item \textbf{Input} - Tutti i video del dataset vengono analizzati con \emph{Deformable CNN} \cite{Deformable}, un algoritmo per il riconoscimento di persone in immagini in grado di delinearne la relativa \emph{bounding box}. La threshold in questa fase è stata settata a $0.99$ per scartare immediatamente la maggior parte dei falsi positivi riconosciuti. Successivamente viene selezionata la più piccola bounding box che contiene tutte le bounding box identificate nel video ed utilizzata come maschera di ritaglio per ogni frame. Utilizzare questa tecnica ha un doppio vantaggio: riesce a risolvere molti dei falsi negativi lungo il video e allinea le features sulla dimensione temporale.
\item \textbf{Backbone} - I ritagli dei frame vengono passati all'\emph{Inflated 3D ConvNet} (I3D, proposto nel lavoro di J. Carreira e A. Zisserman nel 2017 \cite{QuoVadis}) implementato con ResNet-50 \cite{resNet} per l'estrazione delle features. Per aiutare la stima della posa in ogni frame viene rimosso il \emph{temporal max pooling} dopo la prima fase di I3D. La mappa delle features ottenute ha dimensione $2048\times 8 \times 7 \times 7$ è verra utilizzata sia per la categorizzazione dell'azione basata solo sui valori RGB che per la stima della posa.
\item \textbf{Categorizzazione con valori RGB} - Dopo l'ultimo layer convoluzionale di I3D viene effettuato un \emph{global average pooling} per ottenere il vettore delle features composto da 2048 valori, che indicheremo con $P_{rgb}$. Se definiamo un dataset di $N$ video con $n$ categorie $\{(X_i,y_i)\}_{i=1}^N$, dove $y_i \in \{1, ..., n\}$ è l'etichetta corrispondente alle features $X_i$, allora la predizione dell'azione può essere ottenuta direttamente come segue:
$$
Y_{rgb} = \varphi(W_cP{rgb}+b_c),
$$
dove $\varphi$ è l'operazione \emph{softmax}, $Y_{rgb} \in \mathbb{R}^n$ e $W_c$ e $b_c$ sono i parametri del livello completamente connesso. In fase di training la loss finale viene combinata alla \emph{cross-entropy loss} ottenendo
$$
L_r = -\sum_{i=1}^N log(Y_{rgb}(y_i)),
$$
dove $Y_{rgb}(y_i)$ è il valore dell'$y_i$-esima posizione di $Y_{rgb}$.
\item \textbf{Stima della posa} - Una volta ottenuto l'output dalla I3D, la stima della posa viene effettuata sulla dimensione temporale. Ispirandosi a Mask R-CNN \cite{maskRCNN}, viene aggiunta all'ultimo livello della I3D una \emph{head} di deconvoluzione 2D, ovvero 2 livelli di deconvoluzione con la normalizzazione dei batch e un'attivazione di tipo RELU. Ogni layer ha $256$ filtri con un kernel $4\times4$ ed uno stride di $2$. Viene infine aggiunto un livello convoluzionale $1\times1$ per generare la heatmap corrispondente ai $K$ keypoint (un canale per ogni keypoint) e gli offsets (2 canali per ogni keypoint, per le direzioni sull'asse \emph{x} e \emph{y}) per un totale di $3K$ canali in output, dove $K=17$.

Data un'immagine, definiamo $f_k(x_i) = 1$ se il $k$-esimo keypoint è situato in posizione $x_i$, altrimenti $f_k(x_i) = 0$, dove $i \in 1, ..., Q$ è l'indice dei pixel nell'immagine. Per ogni punto $x_i$ e per ogni keypoint $k$ viene calcolata la probabilità $h_k(x_i) = 1$ per $||x_i-l_k|| \leq M$ ovvero la probabilità che il punto $x_i$ si trovi entro un raggio $M$ dall'effettivo punto $l_k$ del keypoint $k$. Queste heatmap vengono allenate risolvendo un problema di classificazione binaria per ogni posizione e per ogni keypoint in maniera indipendente. Per ogni posizione $x_i$ e per ogni keypoint $k$ viene inoltre stimato il vettore-distanza $F_k(x_i) = l_k - x_i$ fra $x_i$ e l'effettivo punto del keypoint corrispondente. 

Questa parte dell'algoritmo fornisce quindi le heatmap delle probabilità $h_k(x_i)$ per ogni keypoint $k$ e per ogni punto $x_i$. Il \emph{training target} per ogni heatmap è $\overline{h}_k(x_i)$ ovvero una mappa di zeri e uno tale che $\overline{h}_k(x_i) = 1$ se $||x_i-l_k|| \leq M$, $0$ altrimenti. La corrispondente loss function $L_h(\theta)$ è la somma delle loss $L1$ per ogni punto e per ogni keypoint in maniera indipendente, ovvero
$$
L_h(\theta) = \frac{1}{K}\sum_{k=1}^K\sum_{i=1}R(h_k(x_i),\overline{h}_k(x_i)),
$$
dove $R$ è la loss $L1$.

Per quanto riguarda invece la stima degli offset, quest'ultimi vengono calcolati solo per tutti quei punti $x_i$ entro un raggio $M$ da ogni keypoint e le differenze fra i valori stimati e quelli reali vengono penalizzati con una loss $L1$, ovvero
$$
L_o(\theta) = \frac{1}{K}\sum_{k=1}^K\sum_{i: ||l_k - x_i||\leq M}R(F_k(x_i), (l_k - X_i)).
$$
La loss finale per la stima della posa sarà quindi
$$
L_p = \lambda_h L_h(\theta) + \lambda_oL_o(\theta),
$$
dove $\lambda_h = 0.5$ e  $\lambda_o = 0.5$.

In fase di test, viene effettuata un'operazione \emph{argmax} su ognuna della $k$ heatmap per ottenere la stima del punto corrispondente, ovvero
$$
x_k =  \underset{x_i}{arg\  max \ } (h_k(x_i), i \in 1, ..., Q).
$$ 
La posizione accurata del $k$-esimo keypoint viene infine ottenuta sommando il relativo offset $F_k(x_k)$ a $x_k$.
\item \textbf{Stima dell'azione utilizzando la posa} - Le coordinate 2D delle pose vengono dapprima trasformate in un tensore di dimensione $2\times T\times K$, dove $T$ è il numero di frame in input e successivamente passate come input a ResNet-18 \cite{resNet} per la classificazione dell'azione. A causa dell'insufficiente dimensione spaziale di questo input, tutte le operazioni di pooling in ResNet-18 sono state rimosse e tutti gli stride di $2$ nei livelli convoluzionali sono stati rimpiazzati con stride di $1$. Infine un \emph{global average pooling} al termine di ResNet-18 restituisce in output un vettore di features lungo 512. Per ogni sequenza di pose viene quindi stimata l'azione $Y{paction}$ attraverso una cross-enttopy loss
$$
L_{paction} = -\sum_{i=1}^N log(Y_{paction}(y_i)).
$$
\item \textbf{Allenamento multi-task} - Action machine ha 3 obbiettivi: classificazione dell'azione utilizzando i valori RGB, stima della posa e classificazione dell'azione utilizzando le sequenze di pose. Questi 3 task sono ottimizzati parallelamente utilizzando la seguente funzione loss 
$$
L = \lambda_1L_r + \lambda_2L_p + \lambda_3L_{paction},
$$
dove $\lambda_1, \lambda_2$ e $\lambda_3$ sono dei bilanciatori del peso che vogliamo dare ad ogni task. Gli autori di action machine settano questi bilanciatori tutti a $1$.
\item \textbf{Fusione dei risultati RGB con quelli della posa} - Nella fase di test, le probabilità provenienti dalla stima con i valori RGB vengono sommate a quelle provenienti dalla stima con le pose, di modo da combinare i punti di forza di entrambe le tecniche.
\end{enumerate}

I risultati ottenuti con questo lavoro sono, ad oggi, i migliori su diversi dataset, tra i quali NTU-RGB+D dove Action machine è attualmente il miglior algoritmo con la seguente accuratezza:
\begin{itemize}
\item \textbf{97.2}\% per il criterio \emph{cross-view}
\item \textbf{94.3}\% per il criterio \emph{cross-subject}.
\end{itemize}


%\newgeometry{left=1cm, right=1.5cm, bottom=0.1cm, top=1.5cm}
\section{Overview}
I migliori risultati fino ad oggi ottenuti utilizzando il dataset NTU-RGB sono schematizzati in tabella \ref{tab:risultatiPrecedenti} ordinati per risultato e suddivisi a seconda delle informazioni utilizzate dato che, come vedremo nel capitolo 5, il dataset NTU-RGB mette a disposizione diverse tipologie di dati, tra i quali i video RGB delle azioni svolte, le pose dei soggetti inquadrati e le riprese ad infrarossi. 

\begin{table}[h]
\centering
\begin{adjustbox}{max width=0.9\textwidth}
\begin{tabular} {| c | c | c | c | c | c |}
\hline
Articolo & Posa & RGB & IR & \thead{Accuratezza  \\ cross-view \\ (\%)} & \thead{Accuratezza  \\ cross-subject \\ (\%)} \\
\hline
Lie Group \cite{LieGroup}   				& \checkmark  &  & & 52.8     &   50.1 \\ \hline
H-RNN \cite{HRNN} 						& \checkmark  &  & & 64.0     &   59.1 \\ \hline
Deep LSTM \cite{DeppLSTM} 				& \checkmark  &  & & 67.3     &   60.7 \\ \hline
PA-LSTM \cite{DeppLSTM}   				& \checkmark  &  & & 70.3     &   62.9 \\ \hline
ST-LSTM+TS \cite{ST-LSTM+TS} 			& \checkmark  &  & & 77.7     &   69.2 \\ \hline
Temporal Conv \cite{TemporalConv}   		& \checkmark  &  &  &83.1     &   74.3 \\ \hline
DSSCA-SSLM \cite{DSSCA-SSLM} 			&  & \checkmark  & & -     	&  74.9 \\ \hline 
Skelemotion \cite{Skelemotion} 			&  \checkmark &   & & 84.7 	&  76.5 \\ \hline 
C-CNN+MTLN \cite{C-CNN+MTLN}  		& \checkmark  &   & & 84.8     &   79.6 \\ \hline
VA-LSTM \cite{VA-LSTM}					& \checkmark  &   & & 87.6     &   79.4 \\ \hline
Chained  \cite{Chained} 					&   & \checkmark  & & -     	&   80.8 \\ \hline
ST-GCN \cite{ST-GCN}   					& \checkmark  &   & & 88.3     &   81.5 \\ \hline
SR-TSL \cite{SR-TSL} 					& \checkmark  &   & & 92.4     &   84.8 \\ \hline
2D-3D-Softargmax \cite{2D-3D-Softargmax} 	&   & \checkmark  & & -     	&   85.5 \\ \hline
Glimpse Clouds \cite{GlimpseClouds}		&   & \checkmark & & 93.2     &   86.6 \\ \hline 
PB-GCN  \cite{PB-GCN}					& \checkmark  &  & & 93.2     &   87.5 \\ \hline 
MFAS \cite{MFAS}						&  \checkmark & \checkmark & & - &   90.04 \\ \hline
FUSION \cite{FUSION}					&  \checkmark &  & \checkmark& 94.5 &   91.6 \\ \hline
PoseMap \cite{PREV1}					& \checkmark & \checkmark & & 95.2     &   91.7 \\ \hline
MMTM \cite{MMTM}						&  \checkmark & \checkmark & & -  &   91.99 \\ \hline
Action Machine \cite{PREV2}				&   & \checkmark & & 97.2     &   94.3 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Risultati dei lavori precedenti sul dataset NTU-RGB. Le risorse utilizzate sono: \emph{Posa} = skeleton dei soggetti fornita da NTU-RGB; \emph{RGB} = video RGB; \emph{IR} = video ad infrarossi.}
\label{tab:risultatiPrecedenti}
\end{table}
%\restoregeometry

%\begin{table}[ht]
%\centering
%\begin{tabular}{| c | c | c | c | c | c |}
%\hline
%Articolo & Posa & RGB & IR & \thead{Accuratezza  \\ cross-view \\ (\%)} & \thead{Accuratezza  \\ cross-subject \\ (\%)} \\
%\hline
%Lie Group \cite{LieGroup}   				& \checkmark  &  & & 52.8     &   50.1 \\ \hline
%H-RNN \cite{HRNN} 						& \checkmark  &  & & 64.0     &   59.1 \\ \hline
%Deep LSTM \cite{DeppLSTM} 				& \checkmark  &  & & 67.3     &   60.7 \\ \hline
%PA-LSTM \cite{DeppLSTM}   				& \checkmark  &  & & 70.3     &   62.9 \\ \hline
%ST-LSTM+TS \cite{ST-LSTM+TS} 			& \checkmark  &  & & 77.7     &   69.2 \\ \hline
%Temporal Conv \cite{TemporalConv}   		& \checkmark  &  &  &83.1     &   74.3 \\ \hline
%DSSCA-SSLM \cite{DSSCA-SSLM} 			&  & \checkmark  & & -     	&  74.9 \\ \hline 
%Skelemotion \cite{Skelemotion} 			&  \checkmark &   & & 84.7 	&  76.5 \\ \hline 
%C-CNN+MTLN \cite{C-CNN+MTLN}  		& \checkmark  &   & & 84.8     &   79.6 \\ \hline
%VA-LSTM \cite{VA-LSTM}					& \checkmark  &   & & 87.6     &   79.4 \\ \hline
%Chained  \cite{Chained} 					&   & \checkmark  & & -     	&   80.8 \\ \hline
%ST-GCN \cite{ST-GCN}   					& \checkmark  &   & & 88.3     &   81.5 \\ \hline
%SR-TSL \cite{SR-TSL} 					& \checkmark  &   & & 92.4     &   84.8 \\ \hline
%2D-3D-Softargmax \cite{2D-3D-Softargmax} 	&   & \checkmark  & & -     	&   85.5 \\ \hline
%Glimpse Clouds \cite{GlimpseClouds}		&   & \checkmark & & 93.2     &   86.6 \\ \hline 
%PB-GCN  \cite{PB-GCN}					& \checkmark  &  & & 93.2     &   87.5 \\ \hline 
%MFAS \cite{MFAS}						&  \checkmark & \checkmark & & - &   90.04 \\ \hline
%FUSION \cite{FUSION}					&  \checkmark &  & \checkmark& 94.5 &   91.6 \\ \hline
%PoseMap \cite{PREV1}					& \checkmark & \checkmark & & 95.2     &   91.7 \\ \hline
%MMTM \cite{MMTM}						&  \checkmark & \checkmark & & -  &   91.99 \\ \hline
%Action Machine \cite{PREV2}				&   & \checkmark & & 97.2     &   94.3 \\ \hline
%\end{tabular}
%\caption{Risultati dei lavori precedenti sul dataset NTU-RGB. Le risorse utilizzate sono: \emph{Posa} = skeleton dei soggetti fornita da NTU-RGB; \emph{RGB} = video RGB; \emph{IR} = video ad infrarossi.}
%\label{tab:risultatiPrecedenti}
%\end{table}



%% Stima della Posa %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Stima della posa}

Cos è la stima della posa? \\ 

Quando parliamo di \textit{stima della posa} ci riferiamo ad una tecnica di \textit{computer vision} dedita al riconoscimento di figure umane all'interno di video ed immagini, così da poter localizzare ad esempio dove si trova la testa, il braccio, la gamba destra, etc.. della persona inquadrata. 

\begin{figure}[htbp]
\includegraphics[width=\textwidth]{imgs/stimaPosaEsempi}
\caption{Esempi di stima della posa. In alto tre esempi di stima della posa utilizzando modelli di tipo volumetrico. In basso due esempi di stima della posa ottenuti utilizando modelli di tipo scheletrici. }
\end{figure}

Questa tecnica non va assolutamente confusa con tecniche di riconoscimento di persone, infatti la stima della posa è in grado solo di riconoscere dove sono situate le parti del corpo di un individuo all'interno dell'immagine, non \textit{chi} è inquadrato. 

I campi di applicazione della stima della posa sono i più svariati: software interattivi che reagiscono al movimento della persona, robotica, realtà aumentata, animazione, fotoritocco intelligente, fitness, riabilitazione, etc. 

Stiamo parlando di un problema tutt'altro che semplice, infatti la condizione di luce dell'immagine, la variabilità dell'ambiente circostante, l'inclinazione del soggetto inquadrato, rendono il riconoscimento della posa un problema estremamente difficile. 

Spinti dal crescente interesse degli ultimi anni, sono stati sviluppati diversi algoritmi per la stima della posa, raggiungendo in molti casi risultati davvero sorprendenti con un'accuratezza prossima alla perfezione. 

\begin{figure} [h]
\centering
\includegraphics[width=0.7\textwidth]{imgs/stimaPosaEsempioFisio}
\caption{Un esempio di utilizzo in campo medico della stima della posa}
\end{figure}

La maggior parte dei software in circolazione in grado di stimare in maniera sufficientemente corretta la posa di un individuo non sono liberamente accessibili. Due fra i migliori algoritmi (ad oggi) di \emph{pose detection} sono sicuramente \textit{Posenet} \cite{PosenetArticle} e \textit{Detectron-2} \cite{Detectron2Link}, dei quali ci occuperemo in maniera più approfondita nei capitoli seguenti.


%% PoseNet %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{PoseNet}
I recenti progressi nel campo della visione artificiale hanno permesso alla comunità scientifica di spostarsi verso problemi ancora più articolati rispetto a quelli classici, con l'obiettivo di riconoscere figure umane in contesti non vincolati e molto variabili. 

L'algoritmo \emph{PoseNet} è stato ideato proprio con lo scopo di identificare una o più figure umane in qualsiasi contesto, compreso quelli ``affollati", ed essere in grado di identificare ogni persona stimandone i suoi \emph{punti chiave} (o \emph{keypoint}).

Generalmente esistono due approcci principali per affrontare i problemi di rilevamento di più persone in un'immagine e la relativa stima della loro posa o \emph{segmentazione} (ovvero l'identificazione dei pixel che rappresentano ogni persona):
\begin{itemize}
\item \textbf{Approccio top-down} - l'approccio \emph{top-down} inizia con una fase di identificazione e localizzazione approssimativa della posizione delle persone, delimitando il riquadro dell'immagine entro il quale sono contenute e continua con una fase di stima della posa o di separazione ``primo piano-sfondo" nell'area identificata. 
\item \textbf{Approccio bottom-up} -  l'approccio \emph{bottom-up} inizia localizzando \emph{entità semantiche individuali}, come ad esempio gambe, braccia, mani, etc, e procede raggruppandole logicamente in istanze di persone complete. PoseNet adotta questo secondo approccio.
\end{itemize}

La rete neurale utilizzata in PoseNet è di tipo convoluzionale ed il costo computazionale per il riconoscimento delle pose è essenzialmente indipendente dal numero di persone raffigurate nella scena ma dipende esclusivamente dalla scelta dei filtri della rete.

L'approccio adottato in PoseNet è quello di identificare dapprima tutti i punti chiave e successivamente raggrupparli in istanze di persona utilizzando un processo ``greedy", ovvero partendo dal rilevamento ``più sicuro" e non come spesso accade da un punto fisso di riferimento, ad esempio il naso. Anche se questo approccio potrebbe sembrare più "disordinato", i risultati empirici ne hanno dimostrato l'efficacia.

Oltre a stimare punti chiave sparsi, PoseNet stima anche delle maschere di segmentazione per ogni persona. Per fare ciò, viene allenata una seconda rete neurale con la quale viene associato ad ogni pixel $x_i$ la probabilità di appartenenza di quel pixel ad ogni candidato $j$ identificato. Se la probabilità è sufficientemente alta allora viene associato il pixel $x_i$ al candidato $j$.

Questo algoritmo dopo essere stato allenato col famoso dataset COCO-2016 \cite{COCO-2016} (che contiene anche l'annotazione dei 17 keypoint di migliaia di persone), è riuscito a migliorarne l'\emph{AP} (average-precision) da 0,655 a 0,687, diventando il miglior risultato. 

Questo metodo essendo molto semplice è anche quindi molto rapido, poiché non richiede alcuna fase supplementare di raffinamento dei risultati con tecniche di tipo \emph{box-based} o \emph{clustering}, rendendo di fatto PoseNet uno degli algoritmi più facilmente installabili su piccoli dispositivi, come ad esempio i cellulari.

Ma vediamo adesso più nel dettaglio come PoseNet stima e raggruppa i punti chiavi di una o più persone raffigurate in un'immagine.

\subsection{Stima dei keypoint}
L'obiettivo di questa fase è quello di rilevare, in modo indipendente dall'istanza, tutti i keypoint visibili appartenenti a qualsiasi persona dell'immagine. A tale scopo, per ogni tipologia di keypoint, viengono prodotte delle \emph{heatmap} e degli \emph{offset}. 

Sia $x_i$ la coordinata 2D del pixel $i$ nell'immagine, dove $i = 1, ... ,N$ e $N$ è il numero totale di pixel, allora definiamo con $D_R (y) = \{ x:  \|x - y \| \leq R\}$ un disco di raggio $R$ centrato nel punto $y$ e $y_{j,k}$ la coordinata 2D del $k$-esimo keypoint della j-esima persona, con $j = 1, ..., M$, dove $M$ è il numero di persone nell'immagine. 

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{imgs/heatmaps-posenet}
\caption{Generazione con PoseNet delle heatmap per ogni tipologia di keypoint}
\end{figure}

Come abbiamo visto nel capitolo precedente, per ogni tipo di keypoint $k = 1, ..., K$, viene impostato un task di classificazione binaria come segue: la heatmap $p_k(x) = 1$ se $x \in D_R (y_{j,k})$ per qualsiasi istanza $j$, altrimenti $p_k(x) = 0$. 

Abbiamo quindi $K$ tasks di classificazione binaria indipendenti (uno per ogni tipo di keypoint) e ciascuno di loro equivale a stimare un disco di raggio $R$ attorno a un tipo di keypoint specifico appartenente a qualsiasi persona nell'immagine.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{imgs/short-range-Posenet}
\caption{Esempio di stima degli offset a corto raggio con PoseNet}
\end{figure}



Oltre alle heatmap, vengono anche stimati vettori di \emph{offset a corto raggio} (\emph{short-range offset}) $S_k(x)$ il cui scopo è quello di migliorare l'accuratezza della localizzazione dei keypoint. Per ogni punto $x$ all'interno dei dischi ricavati al passo precedente, il vettore di offset 2D a corto raggio $S_k(x) = y_{j,k} - x$ rappresenta la distanza fra il punto $x$ e il $k$-esimo keypoint della $j$-esima persona più vicina. Vengono quindi generati $K$ vettori per ogni punto $x$ che verranno successivamente combinati fra loro in una \emph{trasformata di Hough} per migliorare l'accuratezza della posizione stimata di ogni keypoint. Rimandiamo all'articolo \cite{PosenetArticle} per i dettagli d'implementazione.
%$h_k(x)$, per migliorare l'accuratezza della posizione predetta di ogni keypoint. 

%Solo i punti che superano una certa soglia di Hough (0.01, come indicato da \cite{PosenetArticle}) vengono considerati dei keypoint, gli altri invece vengono scartati.

\subsection{Raggruppamento dei keypoint in istanze di persona}
\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{imgs/legamiKeypointsPoseNet}
\caption{Struttura ad albero utilizzata da PoseNet per raggruppare i keypoint appartenenti alla stessa persona}
\label{fig:strutturakeypoints}
\end{figure}
A questo punto è però necessario capire come associare ogni keypoint alle persone raffigurate nell'immagine (nel caso ce ne sia più di una). 

Seguendo lo schema delle connessioni fra tipi di keypoint rappresentati in figura \ref{fig:strutturakeypoints} la rete viene allenata per restituire in output anche i cosiddetti \emph{offset a medio raggio} (\emph{mid-range offsets}), ovvero le probabilità di connessioni fra keypoint, col lo scopo di raggruppare quelli appartenenti alla stessa persona. 

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{imgs/mid-range-Posenet}
\caption{Esempio di stima degli offset a medio raggio con PoseNet. L'intento è quello di raggruppare i keypoint appartenenti alla stessa persona.}
\label{fig:midRangeOffset}
\end{figure}

Un esempio di questa stima è raffigurato in figura \ref{fig:midRangeOffset} mentre una raffigurazione completa del sistema adottato da PoseNet è rappresentato in figura \ref{fig:overview-posenet}.

\begin{figure} [h]
\centering
\includegraphics[width=1\textwidth]{imgs/overview-posenet}
\caption{Combinazione delle fasi adottate da PoseNet per il riconoscimento della posa in un'immagine.}
\label{fig:overview-posenet}
\end{figure}

%% Detecron 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Detectron-2}
Detectron-2 è un progetto open-source lanciato da \emph{Facebook AI Research (FAIR)} ampiamente usato dalla comunità di ricerca in ambito \emph{computer vision} e rappresenta, ad oggi, una piattaforma per il riconoscimento di oggetti allo stato dell'arte.\\

Il suo predecessore \emph{Detectron} \cite{DetectronLink} fu un progetto iniziato nel 2016 con l'obiettivo di creare un sistema rapido e flessibile per il riconoscimento di oggetti in immagini originariamente basato su \emph{Caffe2} \cite{caffe2} (un framework ideato per facilitare la sperimentazione e la divulgazione di nuovi modelli e algoritmi in ambito \emph{deep learning}) e scritto in \emph{Python}. 

Negli anni Detectron è stato perfezionato e supportato da una grande quantità di progetti, compreso ``\emph{Mask-R-CNN}" \cite{maskRCNN} e ``\emph{Focal Loss for Dense Object Detection}" \cite{focalLossDetection}, vincitori rispettivamente del \emph{Premio Marr} e di \emph{Miglior articolo scientifico studentesco} all'\emph{Internation Conference on Compuer Vision} (ICCV) del 2017. L'intuitività e l'efficacia di questi algoritmi hanno permesso un notevole sviluppo nella risoluzione di problemi complessi nell'ambito della computer vision, come ad esempio l'\emph{instance segmentation} e hanno sicuramente giocato un ruolo rilevante nell'avanzamento tecnologico dei sistemi di riconoscimento visivo.\\

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{imgs/typesOfDetection}
\caption{Tipologie di analisi visive.}
\label{fig:typeOfDetection}
\end{figure}

Detectron-2 è adesso basato su \emph{Pytorch}, una libreria open-source dedita al machine learning ed ampiamente usata nel campo della computer-vision, che ha inglobato in se anche il precedente framework Caffe2.  Più nello specifico Detectron-2 include ad oggi le implementazioni dei seguenti algoritmi di object-detection:
\begin{itemize}
\item Cascade R-CNN \cite{cascadeRCNN}
\item Panoptic FPN \cite{panopticFeatures}
\item TensorMask \cite{tensorMask}
\item Mask R-CNN \cite{maskRCNN}
\item RetinaNet \cite{focalLossDetection}
\item Faster R-CNN \cite{fasterRCNN}
\item RPN \cite{fasterRCNN}
\item Fast R-CNN \cite{fastRCNN}
\item R-FCN \cite{RFCN}
\end{itemize}

utilizzando le seguenti reti \emph{backbone} (ovvero reti precedentemente allenate con lo scopo di estrarre in maniera efficiente le \emph{features} di un'immagine):
\begin{itemize}
\item ResNeXt-50-101-152 \cite{resNetXt}
\item ResNet-50-101-152 \cite{resNet}
\item Feature Pyramid Networks (con ResNet/ResNeXt) \cite{featurePyramid}
\item VGG16 \cite{vgg16}
\end{itemize}

Inoltre, nel caso fosse necessario implementare nuove reti backbone, è possibile farlo grazie alla struttura modulare di Pytorch, che permette di separare facilmente il nuovo modello dai precedenti algoritmi di Detectron-2.\\
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{imgs/instanceSeg}
\caption{Instance segmentation con Detectron-2}
\end{figure}
Essendo stato interamente riscritto in Pytorch, Detectron-2 è più rapido del suo predecessore nei compiti di \emph{object-detection}, \emph{instance segmentation} e \emph{human-pose prediction} ed in più è in grado di gestire i nuovi task di \emph{semantic segmentation} e \emph{panoptic segmentation}, ovvero la combinazione fra instance-segmentation e semantic-segmentation (figura \ref{fig:typeOfDetection}).

Oltre che nel mondo della ricerca, questa piattaforma viene usata anche per l'addestramento di nuovi modelli in svariati campi della \emph{computer vision}, come ad esempio la \emph{realtà aumentata}, e in materia di sicurezza informatica, come ad esempio la \emph{community integrity}, ovvero la difesa e la protezione di account su piattaforme social da contenuti maligni.

Per quanto riguarda invece la stima della posa umana in un'immagine, Detectron-2 utilizza una Mask R-CNN \cite{maskRCNN} riadattata all'estrazione dei keypoint. Vediamo adesso più nel dettaglio come è stato strutturato questo algoritmo.

\subsection{Mask R-CNN}
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{imgs/instanceSegmenttionMaskRCNN2}
\caption{Alcuni esempi di instance segmentation con Mask R-CNN}
\end{figure}
Mask R-CNN nasce con l'intento di creare un framework semplice e flessibile per affrontare il problema dell'\emph{instance segmentation}. Questo metodo è in grado di identificare gli oggetti in un'immagine e simultaneamente generare una maschera di segmentazione ben definita per ogni istanza. Mask R-CNN è sostanzialmente un'estensione di \emph{Faster R-CNN} \cite{fasterRCNN} in quanto aggiunge parallelamente ai due output già presenti per il \emph{bounding box} e la \emph{classificazione} anche quello per la predizione della \emph{maschera d'istanza}. Nonostante sia un metodo nato per l'instance segmentation è però facilmente adattabile ad altri tipi di predizioni, come ad esempio quella della posa umana.

La porcedura adottata da Mask R-CNN è suddivisa in due fasi. La prima, identica a quella adottata per Faster R-CNN, chiamata ``\emph{Region Proposal Network}" (RPN) ha il compito di proporre porzioni di immagine nelle quali potrebbero essere raffigurate delle istanze di persona. Nella seconda fase vengono stimate \emph{in parallelo}: la classe, il delineamento preciso (\emph{bounding box}) e la maschera binaria di ogni \emph{regione d'interesse} (RoI) dell'immagine. 

Più precisamente, durante l'allenamento della rete, la \emph{loss} definita per ogni RoI è la seguente: $L= L_{cls} + L_{box} + L_{mask}$. La loss di classificazione e quella del boundig box sono identiche a quelle definite per Fast R-CNN \cite{fastRCNN}. 

Per quanto riguarda invece $L_{mask}$, decodifica $K$ maschere binarie di dimensione $m \times m$, dove $K$ è il numero di classi, quindi la sua dimensione sarà $Km^2$ per ogni RoI.

Questa definizione di $L_{mask}$ permette alla rete non solo di generare maschere completamente indipendenti l'una dall'altra, ma anche di disaccoppiare la predizione delle maschere dalla classificazione delle istanze. Questa caratteristica, innovativa rispetto alle pratiche comuni in materia di semantic segmentation, si è rivelata essere basilare per il raggiungimento di una buona segmentazione.

\subsection{Predizione della posa con Mask R-CNN}
Grazie alla sua grande flessibilità, questo framework può essere facilmente esteso alla stima della posa umana in un'immagine. 

\begin{figure}[h]
\centering
\includegraphics[width=1.1\textwidth]{imgs/posePredictionMaskRCNN}
\caption{Alcuni esempi di pose prediction con Mask R-CNN}
\end{figure}

Le coordinate dei keypoint vengono trasformate in maschere di tipo \emph{one-hot} e Mask R-CNN viene utilizzato per predire $K$ maschere, una per ogni tipo di keypoint (gomito sinistro, spalla destra, etc..).

Più nello specifico, per ognuno dei $K$ keypoint di un'istanza il training target è una maschera binaria $m \times m$ di tipo \emph{one-hot} dove cioè solo un pixel viene etichettato come positivo. Durante l'allenamento della rete, per ogni keypoint visibile nell'immagine, viene minimizzata una \emph{cross-entropy} loss con regolarizzatore L2 (per incoraggiare l'identificazione di un singolo punto). Anche per questa procedura le $K$ maschere dei $K$ tipi di keypoint sono completamente indipendenti l'una dall'altra. 

%%%%%%  Metodo proposto %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Metodo proposto}
In questo capitolo vedremo sia quali sono state le tecniche adottate in questo lavoro di tesi che le decisioni prese durante l'iter operativo.

Come vedremo, il dataset utilizzato è NTU-RGB+D \cite{NTURGB} e nonostante esso fornisca anche i dati relativi alla posa dei soggetti inquadrati (quello che viene spesso indicato come \emph{skeleton}) abbiamo preferito non farne assolutamente uso ed utilizzare invece i soli video RGB, dai quali successivamente stimare la posa e servirci di quest'ultima per il riconoscimento delle azioni svolte. 

Questa scelta è stata presa con lo scopo di favorire una maggiore generalizzazione e applicazione dell'algoritmo, adoperabile in contesti più diffusi dove la stima della posa non è data, ma abbiamo a disposizione solo un comunissimo video RGB.

Inoltre siamo interessati a ideare un algoritmo che abbia una bassa latenza di inferenza ed una ridotta dimensione del modello utilizzato, di modo da poter essere installato anche su un qualsiasi dispositivo mobile e riconoscere le azioni motorie riprese dalla fotocamera integrata. Questa eventualità può essere possibile solo se il nostro algoritmo mantiene una struttura semplice senza pesanti fasi di raffinamento dei risultati.

Passiamo adesso ad analizzare una ad una le varie fasi affrontate nel lavoro di tesi.
\section{Estrazione delle pose}
Per prima cosa è necessario convertire tutti i video del dataset in sequenze di pose, in modo da sintetizzarne efficacemente il movimento umano senza un'eccessiva perdita d'informazione.  Gli algoritmi scelti per questa fase sono stati Detectron-2 \cite{Detectron2Link} per la sua rimarcabile accuratezza e PoseNet \cite{PosenetArticle} per la sua velocità d'inferenza e portabilità.
I modelli proposti da queste due tecniche sono molteplici. Per Detectron-2 abbiamo:
\begin{itemize}
\item  \emph{R50-FPN-1x}
\item \emph{R50-FPN-3x}
\item \emph{R101-FPN-3x} 
\item  \emph{X101-FPN-3x} 
\end{itemize}
mentre per PoseNet abbiamo:
\begin{itemize}
\item  \emph{PoseNet-50}
\item \emph{PoseNet-75}
\item \emph{PoseNet-100}
\item \emph{PoseNet-101}
\end{itemize}

Ognuno di questi modelli ha valori diversi in accuratezza e velocità di inferenza. Nel caso di Detectron-2 questa comparazione viene fornita dagli autori stessi e schematizzata in tabella \ref{proposteDetectronPosenet}, mentre per quanto riguarda PoseNet non viene purtroppo fornito un simile confronto.

\begin{table}[ht]
\centering
\begin{tabular}{| c | c | c | c |}
\hline
Nome & \thead{Tempo di  \\ inferenza \\ (s/imm)} &  \thead{Box  \\ AP} &  \thead{KP  \\ AP}  \\
\hline
 R50-FPN-1x	 		& 0.072  	& 53.6 	& 64.0 \\ \hline
 R50-FPN-3x 			& 0.066  	& 55.4 	& 65.5 \\ \hline
 R101-FPN-3x 			& 0.076  	& 56.4 	& 66.1 \\ \hline
 \textbf{X101-FPN-3x} 	& 0.121  	& 57.3 	& 66.0 \\ \hline
\end{tabular}
\caption{Diversi modelli in Detectron-2 per l'estrazione della posa da un'immagine. Il tempo di inferenza viene misurato in \emph{secondi/immagine} e sono stati ottenuti su un server Big Basin  con 8 NVIDIA V100 GPU e NVLink. I software usati sono PyTorch 1.3, CUDA 9.2, cuDNN 7.4.2 o 7.6.3; Box AP = ``Bounding box average precision" ; KP AP = ``Keypoint average precision"}
\label{proposteDetectronPosenet}
\end{table}

Abbiamo comunque scelto di testare l'inferenza di ogni modello sul nostro server con le seguenti specifiche:
\begin{itemize}
\item scheda GPU NVIDIA Corporation GM200 GeForce GTX TITAN X da 12GB
\item Pythorch 1.4.0+cu100
\item CUDA  10.0.130
\item cuDNN 7.3.1
\end{itemize} 

Per ogni modello sono stati generati i keypoints di circa $1000$ immagini ad una risoluzione $1280\times720$ dove la metà delle immagini raffiguravano una sola persona e l'altra metà due persone. Il tempo medio d'inferenza per ogni modello è schematizzato in tabella \ref{tab:tempiInferenze}.

\begin{table}[ht]
\centering
\begin{tabular}{| c | c | c | c |}
\hline
\multicolumn{2}{|c|}{\textbf{Detectron2}}  &  \multicolumn{2}{c|}{\textbf{PoseNet}}  \\
\hline
Modello & \thead{Tempo di  \\ inferenza \\ (s/imm)}  &  Modello &  \thead{Tempo di  \\ inferenza \\ (s/imm)}  \\
\hline
 R50-FPN-1x	 		& $0.1610$  	& PoseNet-50 			& $0.0780$  \\ \hline
 R50-FPN-3x 			& $0.1642$  	& PoseNet-75 			& $0.0803$  \\ \hline
 R101-FPN-3x 			& $0.2046$  	& PoseNet-100 		& $0.0911$  \\ \hline
 \textbf{X101-FPN-3x}	& $0.3391$  	& \textbf{PoseNet-101} 	& $0.1004$  \\ \hline
\end{tabular}
\caption{Tempi di inferenza per ogni modello testati sul nostro server equipaggiato di scheda GPU GeForce GTX TITAN X da 12GB. I software usati sono PyTorch 1.4.0+cu100, CUDA 10.0.130
, cuDNN 7.3.1.}
\label{tab:tempiInferenze}
\end{table}

Il modello scelto è stato \emph{X101-FPN} per Detectron-2, in quanto quello con il miglior valore di accuratezza globale e PoseNet-101, per per una più facile comparazione dei risultati.

\section{Assegnazione coerente delle pose}
Quello che otteniamo per ogni video dalla fase precedente è una sequenza di pose umane estratte da ogni frame. Sia Detectron-2 che PoseNet sono in grado di riconoscere più persone all'interno di un'immagine, producendo una lista di pose $P_1, P_2, ..., P_n$ ordinata secondo le regole dell'algoritmo, ad esempio Detectron-2 le ordina per valore di \emph{score} decrescente, ovvero quel valore che indica quanto è sicuro l'algoritmo di aver correttamente stimato la posa di quella persona.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{imgs/swapPose}
\caption{Un esempio di assegnazione inconsistente delle pose lungo il video. Nell'esempio, l'ordine delle pose restituite dall'algoritmo è blu, rosso, verde e viola. Quest'ordine è indipendente dai soggetti inquadrati, che possono talvolta essere scambiati fra loro, non essere riconosciuti o addirittura scambiati per dei riflessi.}
\label{fig:swapPose}
\end{figure}

Quello che però questi algoritmi non hanno è l'informazione relativa al concetto di video, ovvero alla successione logica dei frame e trattano ognuno di essi come un'immagine indipendente. Quello che ne segue è una totale indipendenza fra l'ordine delle pose riconosciute e la loro assegnazione coerente coi soggetti del video. Ad esempio, ammettiamo di avere due soggetti $A$ e $B$ e che per l'$i$-esimo frame vengano identificate due pose $\mathcal{P}_{1,i}$ e $\mathcal{P}_{2,i}$ appartenenti ad A e B rispettivamente. Quest'ordine non è però garantito nel frame successivo dove $\mathcal{P}_{1,i+1}$ potrebbe essere la posa di B e $\mathcal{P}_{2,i+1}$ quella di A. 

Per ovviare a questo tipo di problema, la tecnica adottata è stata quella di calcolare iterativamente ad ogni frame la distanza fra l'ultima posa assegnata ad ogni soggetto e tutte quelle riconosciute al frame successivo per assegnare poi la posa più corretta. Nel dettaglio, definiamo la distanza fra due pose come la somma delle distanze euclidee fra i loro punti in comune, ovvero se $\mathcal{P}_i = \{p_{i,k}\}_{k=1}^K$ è l'insieme dei punti che definiscono la posa $i$, dove ogni $p_{i,k} = (x_{i,k},y_{i,k})$ è la coordinata del $k$-esimo giunto e $K$ il numero di tipi di giunto, allora la distanza fra $P_i$ e $P_j$ sarà:
$$
\mathcal{D}_{ij} = \sum_{k=1}^K I(p_{i,k}) \cdot I(p_{j,k}) \cdot eucl(p_{i,k},p_{j,k})
$$
dove $I$ è la funzione identità, ovvero $I(p) = 1$ se $p$ è definito, $0$ altrimenti. $eucl(p_1,p_2)$ è  la funzione di distanza euclidea fra i punti $p_1$ e $p_2$. Se le due pose non hanno punti in comune, ovvero  $\sum_{k=1}^K I(p_{i,k}) \cdot I(p_{j,k}) = 0 $ allora $\mathcal{D}_{ij} = \infty$.

Una volta ottenuta la lista delle distanze fra pose si può procede al loro abbinamento, assegnando ad ogni soggetto la posa più vicina e ripetere così il procedimento per il frame successivo.

Abbiamo però un ultimo problema da fronteggiare in questa fase, ovvero la variabilità del numero di pose stimate in ogni frame del video. Questo accade quando per uno o più frame, uno dei soggetti non viene riconosciuto dall'algoritmo o quando l'improvviso riflesso su una finestra o uno specchio viene interpretato come una nuova persona (figura \ref{fig:swapPose}). 

Questo problema è stato risolto selezionando solo le pose con il miglior \emph{score medio} lungo tutto il video, si presuppone infatti che un riflesso sia mediamente meno convincente di una persona vera e propria. Più precisamente sono state mantenute solo le migliori due pose e scartate le altre, visto che in questo lavoro di tesi siamo interessati ad analizzare solo azioni individuali o di coppia.

\section{Rimozione degli zeri}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{imgs/rimozioneZeri.jpg}
\caption{Esempio di rimozione degli zeri. Nei frame 2-3-4-5 i giunti del braccio sinistro non sono stati identificati. Nei frame 2-3 verranno quindi rimpiazzati con quelli del frame 1, mentre nei frame 4-5 con quelli del frame 6.}
\label{fig:rimozioneZeri}
\end{figure}

Come abbiamo appena visto, PoseNet e Detectron-2 talvolta in qualche frame possono non riconoscere un'intera persona o non riuscire a stimare la posizione di qualche giunto, se ad esempio coperti o sfocati. Entrambi gli algoritmi trattano questi due casi nella stessa maniera, ovvero assegnano il punto $(0,0)$ ad ogni giunto non riconosciuto. 

Questo valore però è fortemente fuorviante considerando che le informazioni a nostra disposizione sono solo pochi punti corporei. Detectron-2 e PoseNet stimano una posa di soli 17 punti, quindi per ogni giunto non riconosciuto, non solo perdiamo 1/17 dell'informazione ma confonderemo anche il nostro algoritmo in fase di classificazione dell'azione facendogli analizzare per quel giunto un movimento inesistente verso il punto (0,0). È necessario quindi adottare una buona tecnica di rimozione degli zeri, che limiti la perdita d'informazione, mantenendo al contempo coerenza con le pose stimate.

La cosa più semplice da fare sarebbe assegnare ad ogni giunto non riconosciuto la sua ultima posizione identificata, ma cosi facendo utilizzeremmo solo il passato del video in questione e per lunghe sequenze di valori mancanti potremmo perdere molta informazione utile. È stato quindi scelto di attribuire ad ogni giunto non identificato il primo punto riconosciuto più vicino nel tempo, passato o futuro, per quel giunto.

L'intento di questa tecnica è quello di ridurre la distanza fra ogni giunto non identificato e la sua effettiva posizione nell'immagine. Un esempio di questa tecnica è rappresentato in figura \ref{fig:rimozioneZeri}

% PARLA DI RIMOZIONE DEGLI ZERI IN MANIERA INTERPOLANTE SE RIESCI AD INCUDERLO NEL LAVORO

\section{Tecniche di rielaborazione}
Adesso che siamo riusciti ad ottenere delle sequenze di pose coerenti e complete possiamo procedere alla loro classificazione. 

Potremmo passare i valori fin qui ottenuti ad una rete neurale, senza alcuna lavorazione aggiuntiva, ma come è facilmente prevedibile questo porterà a risultati non eccellenti.

In questo lavoro di tesi sono state analizzate una serie di tecniche di rielaborazione delle sequenze di pose, al fine di capire quale fra queste aiuta maggiormente la loro classificazione.

Nelle sezioni seguenti indicheremo con $K$ il numero totale di giunti di ogni posa (come abbiamo visto, sia per PoseNet che per Detectron-2, $K=17$), con $\mathcal{P}_f$ la posa estratta dal frame $f$, dove $\mathcal{P}_f = \{\boldsymbol{p}_{kf}\}_{k=1}^K$ è l'insieme dei punti nell'immagine corrispondenti ai giunti $k$, ovvero $\boldsymbol{p}_{kf}=(x_{kf},y_{kf})$. 

Vediamo adesso più nel dettaglio le tecniche testate.

\subsection{Baricentro del frame}
\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{imgs/centroFrame.jpg}
\caption{Esempio di applicazione della \emph{tecnica del baricentro del frame}. In figura sono rappresentati 3 frame del video di una camminata da sinistra a destra. Ogni punto blu (relativo ad un giunto corporeo) viene sostituito col vettore-distanza (in verde) fra quel punto e il baricentro della posa. }
\label{fig:centroFrame}
\end{figure}
Applicando questa tecnica, ogni posa $\mathcal{P}_f$ è stata sostituita con l'insieme $\mathcal{T}_f = \{\boldsymbol{t}_{1f}, ..., \boldsymbol{t}_{Kf} \}$, dove ogni $\boldsymbol{t}_{if}$ è il \emph{vettore-distanza} fra $\boldsymbol{p}_{if}$ e il baricentro di $\mathcal{P}_f$, indicato con $\overline{\mathcal{P}_f} $, ovvero 
$$
\boldsymbol{t}_{if} = \boldsymbol{p}_{if} - \overline{\mathcal{P}_f} \ \ \ \ \text{e} \ \ \ \overline{\mathcal{P}_f} = \frac{\sum_{j=1}^K p_{jf}}{K}.
$$
%$$
%\boldsymbol{t}_{if} = \boldsymbol{p}_{if} - \overline{\mathcal{G}_f}
%$$
%$$
% \overline{\mathcal{G}_f} = \frac{\overline{\mathcal{P}_{1,f}} + \overline{\mathcal{P}_{2,f}}}{2}
%$$


Questa tecnica ha il vantaggio di relativizzare la posa ovunque essa si trovi all'interno del frame e sintetizzarne efficientemente il movimento circoscritto al frame. Allo stesso tempo però, utilizzando un punto di vista sempre e solo focalizzato sul singolo frame, questa tecnica potrebbe non riuscire a mappare correttamente il movimento globale della posa lungo tutto il video. Un esempio di questa tecnica è schematizzato in figura \ref{fig:centroFrame}.

Nel caso di più persone nel video, ovvero quando analiziamo azioni di coppia, questa tecnica stima un unico baricentro per ogni frame, adattando le due pose di conseguenza. A tal proposito è stata quindi testata anche una seconda variante, ovvero quella del \emph{baricentro del frame per persona}, dove cioè viene settato un baricentro diverso per ogni persona rappresentata nel frame.

\subsection{Baricentro del video}
\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{imgs/centroVideo.jpg}
\caption{Esempio di applicazione della \emph{tecnica del baricentro del video}. In figura sono rappresentati 3 frame del video di una camminata da sinistra a destra. Ogni punto blu (relativo ad un giunto corporeo) viene sostituito col vettore-distanza (in arancione) fra quel punto e il baricentro del video.}
\label{fig:baricentroVideo}
\end{figure}

Questa tecnica, molto simile alla precedente, utilizza però come baricentro quello relativo all'intero video anziché quello di ogni frame ovvero, se indichiamo con $\mathcal{V}=[F_1,..., F_f, ..., F_L]$ la sequenza di frame del video, allora il baricentro del video $\overline{\mathcal{V}} $ sarà
$$
\overline{\mathcal{V}} = \frac{\sum_{f=1}^L \overline{\mathcal{P}_f}}{L}.
$$

%$$
%\overline{\mathcal{V}_G} = \frac{\sum_{f=1}^L \overline{\mathcal{G}_f}}{L}.
%$$

Come per la tecnica precedente, anche qui sostituiremo ogni posa con l'insieme dei vettori-distanze dal baricentro, ovvero $\mathcal{T}_f = \{\boldsymbol{t}_{1f}, ..., \boldsymbol{t}_{Kf} \}$ dove $\boldsymbol{t}_{if} = \boldsymbol{p}_{if} - \overline{\mathcal{V}} $. 

%$$
%\boldsymbol{t}_{if} = \boldsymbol{p}_{if} - \overline{\mathcal{V}_G}
%$$

Il vantaggio aggiuntivo di questa tecnica rispetto alla precedente è la capacità di saper immagazzinare anche l'informazione sul movimento globale della posa lungo tutto il video. Un esempio di questa tecnica è rappresentato in figura \ref{fig:baricentroVideo}.

Come per la tecnica precedente, anche per questa è stata testata la variante \emph{baricentro del video per persona}, dove viene definito un baricentro diverso per ogni persona rappresentata nel video.

\subsection{Baricentri multipli}
Come vedremo nel capitolo 6, la tecnica del \emph{baricentro del video} avrà risultati leggermente migliori rispetto a quella del \emph{baricentro del frame}. La domanda che a questo punto sorge spontanea è se aumentando il numero dei baricentri si ottenga un miglioramento dei risultati.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{imgs/multiCentroVideo.jpg}
\caption{Esempio di applicazione della \emph{tecnica dei 3 baricentri}. In figura sono rappresentati 3 frame del video di una camminata da sinistra a destra. Ogni punto blu (relativo ad un giunto corporeo) viene sostituito col vettore-distanza fra quel punto e il baricentro al quale quel giunto appartiene.}
\label{fig:multiCentroVideo}
\end{figure}

Se suddividiamo il corpo umano in $B$ gruppi e definiamo ogni giunto corporeo come appartenente ad uno (e uno solo) di questi gruppi, allora possiamo trasformare ogni posa in un insieme di vettori-distanza fra ogni giunto ed il baricentro del gruppo corrispondente. 

Più precisamente, se suddividiamo l'insieme $K = \{\mathcal{K}_1, ..., \mathcal{K}_B\}$ tale che: 
\begin{itemize}
\item $\bigcup_{b=1}^B   \mathcal{K}_b = K $
\item $  \mathcal{K}_i \bigcap \mathcal{K}_j= \emptyset, \ \ \ \forall \  \mathcal{K}_i ,\mathcal{K}_j \in K$ 
\end{itemize}
allora ogni posa $\mathcal{P}_f$ può essere sostituita con la sua trasformazione $\mathcal{T}_f = \{\boldsymbol{t}_{1f}, ..., \boldsymbol{t}_{Kf} \}$ dove, come per le tecniche precedenti, ogni $\boldsymbol{t}_{if} = \boldsymbol{p}_{if} - \overline{\mathcal{B}_b}$ e 

$$
\boldsymbol{t}_{if} = |\boldsymbol{p}_{if} - \overline{\mathcal{B}_b}|
$$

%$$
%\boldsymbol{t}_{if} = \boldsymbol{p}_{if} - \overline{\mathcal{G}_b}
%$$

$$
%\overline{\mathcal{B}} = \bigcup_{b=1}^B 
 \overline{\mathcal{B}_b} = \frac{\bigcup_{i\in\mathcal{B}_b}\sum_{f=1}^L\boldsymbol{p}_{if}}{L}
$$

%$$
%%\overline{\mathcal{B}} = \bigcup_{b=1}^B 
% \overline{\mathcal{G}_b} = \frac{\bigcup_{i\in\mathcal{B}_b}\sum_{f=1}^Lmean(\boldsymbol{p}_{1if},\boldsymbol{p}_{2if})}{L}
%$$


In questo lavoro di tesi sono state testate 3 varianti di questa tecnica, ovvero:
\begin{itemize}
\item \textbf{B=3} - testa, busto e gambe
\item \textbf{B=5} - tronco, braccio sinistro, braccio destro, gamba sinistra e gamba destra (questa suddivisione viene suggerita dagli autori del dataset NTU-RGB+D col modello \emph{"Part-Aware LSTM"} \cite{NTURGB} )
\item \textbf{B=17} - un baricentro per ogni giunto
\end{itemize}

Queste 3 diverse tecniche hanno il vantaggio di frammentare la posa, attribuendo valori più alti alle componenti più dinamiche del video, ovvero quelle componenti che si presuppone caratterizzino maggiormente l'azione ripresa nel video. In figura \ref{fig:multiCentroVideo} è rappresentato un esempio della tecnica dei 3 baricentri.

Considerando che un'azione può essere svolta in maniera speculare (ad esempio una camminata può essere fatta sia da sinistra a destra che viceversa), ognuna di queste tecniche è stata testata anche considerando solo i valori assoluti dei vettori ottenuti cosi da astrarre il modello dalla direzione verso la quale viene svolta l'azione. Nei risultati esposti nel capitolo 6 questa variante verrà indicata col suffisso \emph{``ASS"}.

Come vedremo nel capitolo dedicato ai risultati sperimentali, la tecnica dei 3 baricentri porterà ottimi risultati ed è per questo che nella fase finale degli esperimenti è stata fatta una distinzione fra le due varianti:
\begin{itemize}
\item \emph{3 baricentri personali}, ovvero 3 baricentri per ogni persona raffigurata nel video
\item \emph{3 baricentri globali}, ovvero 3 baricentri comuni a tutte le persone nel video.
\end{itemize}

\subsection{Tecnica ``Next frame"}
Un altro approccio nella classificazione delle sequenze di pose è quello di concentrarci su ciò che accade fra un frame e l'altro, dato che è proprio in questo lasso di tempo che abbiamo il movimento del soggetto o dei soggetti inquadrati. 

Seguendo questa idea trasformeremo tutte le nostre pose in un insieme di vettori-distanza fra la posizione dei giunti al frame $f$ e quelli al frame $f+1$. La nostra posa trasformata $\mathcal{T}_f$ sarà quindi, $\mathcal{T}_f = \mathcal{P}_{f+1} - \mathcal{P}_f$, ovvero

%$$
%\mathcal{T}_f = \mathcal{P}_{f+S} - \mathcal{P}_f
%$$
$$
\mathcal{T}_f = \bigcup_{k \in K} p_{k,f+1} - p_{k,f}
$$

Un esempio visivo di questa tecnica è rappresentato in figura \ref{fig:nextframe}.

\begin{figure}[htb]
\centering
\includegraphics[width=1\textwidth]{imgs/nextFrame.jpg}
\caption{Tecnica del \emph{next frame}. Nell'esempio, il soggetto ripreso alza il braccio sinistro.  Ogni posa viene sostituita con l'insieme dei vettori-distanza (frecce rosa) fra ogni giunto e la posizione dello stesso al frame successivo. }
\label{fig:nextframe}
\end{figure}

Questa tecnica analizza cioè le differenze di posizione dei giunti fra frame consecutivi, questo però porta spesso a dover memorizzare variazioni minime, specialmente se il nostro video ha un FPS (Frame Per Second) elevato. Inoltre potrebbe non essere semplice distinguere le effettive variazioni di movimento dalle approssimazioni derivanti dall'algoritmo di riconoscimento della posa. 

Quello che allora possiamo fare è memorizzare le differenze dei giunti fra frame a distanza $S >1$, ovvero
$$
\mathcal{T}_f = \bigcup_{k \in K} p_{k,f+S} - p_{k,f}
$$

In questo lavoro di tesi sono stati testati i valori di $S = \{1, 3, 7, 15\}$ per capire se e quali miglioramenti porta questa variazione di distanza.

\subsection{Tecnica delle distanze cumulate}

\begin{figure}[htb]
\centering
\includegraphics[width=1\textwidth]{imgs/distanzeCumulate.jpg}
\caption{Tecnica delle \emph{distanze cumulate}. Nell'esempio sono raffigurati i valori che questa tecnica restituirebbe per l'azione ``alzare e abbassare il braccio destro". Ovviamente i valori più alti sono legati al polso e al gomito destro visto che sono le componenti più dinamiche di questo movimento.}
\label{fig:distanzeCumulate}
\end{figure}

Come abbiamo visto nella tecnica precedente, la distinzione fra valori spesso troppo piccoli e il rumore derivante dall'approssimazione della posizione del giunto non è un compito semplice. Per tentare di ovviare a questo problema, un'altra tecnica testata è quella delle \emph{distanze cumulate}, ovvero trasformare ogni posa $\mathcal{P}_f$ nella somma dei vettori distanza calcolati fino a quel punto con la tecnica del \emph{next frame}, ovvero


$$
\mathcal{T}_f = \bigcup_{k \in K} \sum_{j=1}^{f-1} p_{k,j+1} - p_{k,j}
$$

%\begin{align}
%\mathcal{T}_f &= \sum_{j=1}^{f-1} \mathcal{P}_{j+1} - \mathcal{P}_j\\
%&= \bigcup_{k \in K} \sum_{j=1}^{f-1} p_{k,j+1} - p_{k,j}
%\end{align}

Quello che ci aspettiamo di ottenere sono variazioni evidenti per i giunti più dinamici nel video, mentre per quelli statici, che quindi si presuppone non caratterizzino eccessivamente il movimento, ci aspettiamo variazioni minori. In figura \ref{fig:distanzeCumulate} abbiamo una schematizzazione di ciò che otterremmo con la tecnica delle distanze cumulate se analizzassimo una porzione di video dove il soggetto inquadrato alza e abbassa il braccio destro.



\subsection{Tecnica delle distanze relative}
Questa tecnica riprende quella del \emph{centro del frame} con la differenza che ogni posa viene trasformata in una matrice di distanze fra ogni coppia di giunti. Più precisamente per ogni posa $\mathcal{P}_f$, che ricordiamo essere appartenente all'insieme $\mathbb{R}^{K\times 2}$, otterremo la rispettiva trasformazione $\mathcal{T}_f \in \mathbb{R}^{K\times K}$, composta dai valori

%\begin{align*}
%\mathcal{P}_f &\in \mathbb{R}^{K\times 2}\ \ \  \longrightarrow \ \ \  \mathcal{T}_f \in \mathbb{R}^{K\times K}\\
% \mathcal{T}_f &= \bigcup_{i \in K} \bigcup_{j \in K} t_{ij} \\
% &= \bigcup_{i \in K} \bigcup_{j \in K} eucl(p_{if},p_{jf})
%\end{align*}
$$
t_{ij} = eucl(p_{if},p_{jf})
$$

con $i = 0, ..., K$, $j = 0, ..., K$ ed $eucl(a,b)$ la funzione della distanza euclidea fra i punti $a$ e $b$.


\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{imgs/distanzeRelative.jpg}
\caption{Tecnica delle \emph{distanze relative}. In questa figura sono rappresentati solo i vettori-distanza dalla caviglia destra a tutti gli altri giunti.}
\label{fig:distanzeRelative}
\end{figure}


Questa tecnica, seppur focalizzata sul singolo frame, ha lo scopo di estrapolare dalla posa anche quei collegamenti fra giunti corporei distanti fra loro, come ad esempio la correlazione fra il movimento delle braccia e quello delle gambe durante una camminata. In figura \ref{fig:distanzeRelative} vediamo un esempio della tecnica appena descritta.



\section{Normalizzazione}
Ognuna delle tecniche appena viste ha risultati differenti l'uno dall'altro, con intervalli e variabilità dei valori estremamente diversi per ognuna di esse, rendendo quindi necessaria una normalizzazione dei dati prima di passarli alla rete neurale.

La normalizzazione dei risultati svolge in ogni algoritmo di machine learning un ruolo estremamente importante ed è per questo che anche in questo lavoro di tesi le è stata data particolare attenzione. 

Nel nostro caso la maggior parte delle tecniche adottate restituiscono in output un dataset di vettori, ad esempio le coordinate di punti sul piano o distanze vettoriali, mentre qualche tecnica, ad esempio quella delle \emph{distanze relative}, restituisce un dataset di valori semplici. 

\begin{figure}[htb]
\centering
\includegraphics[width=1\textwidth]{imgs/normalizzazione.jpg}
\caption{Differenza fra normalizzazione \emph{globale} e \emph{separata}. La posa blu e la posa rossa sono identiche, con l'unica differenza di essere posizionate rispettivamente nella metà sinistra e destra del frame. In questo caso stiamo trattando punti nel piano e solo normalizzandoli con la tecnica \emph{separata} si ottengono due risultati identici.}
\label{fig:normalizzazione}
\end{figure}

Per questo motivo occorre fare una distinzione fra due tipi di normalizzazione dei dati:
\begin{itemize}
\item \textbf{Normalizzazione globale} - Questa normalizzazione tratta tutti i valori in input globalmente e nel caso di un insieme di vettori, tratta i componenti di quest'ultimi come fossero tutti appartenenti ad un unico grande vettore.
\item \textbf{Normalizzazione separata} -  Questa tecnica, se applicata ad un dataset di valori semplici, funziona esattamente come una normalizzazione globale, se invece viene applicata ad un dataset di vettori allora ne normalizza i valori dimensione per dimensione, trattando cioè quest'ultime in maniera indipendente l'una dall'altra. Questa tecnica è concettualmente più corretta nel caso in cui si analizzi un dataset di punti nel piano, dove vogliamo separare la normalizzazione dei valori dell'asse $x$ da quella dell'asse $y$ (figura \ref{fig:normalizzazione}).
\end{itemize}

Specifichiamo comunque che nel caso della normalizzazione separata quest'ultima avviene video per video, di modo da renderli totalmente indipendenti l'uno dall'altro mentre per quella globale l'intero dataset viene normalizzato interamente come fosse un unico grande vettore. Ovviamente in questo secondo caso, il dataset di test non può essere usato per il calcolo della media e della varianza.

\section{Struttura della rete}
Per quanto riguarda la rete neurale adottata per la classificazione delle azioni è stato scelto di usare le LSTM. Queste reti, nel mondo del machine learning, hanno già dimostrato la loro grande capacità espressiva quando si tratta di analizzare sequenze temporali in input. 

La struttura della rete è stata fatta variare a seconda della fase degli esperimenti svolti. Nella prima fase è stato scelto di testare ogni tecnica utilizzando una rete neurale più piccola e semplice. Successivamente, solo per le tecniche più promettenti, è stato deciso di testare anche strutture di rete più complesse.

Vediamo più nel dettaglio ogni componente scelto per la rete.
\begin{itemize}
\item \textbf{Livelli} - In prima battuta è stato scelto di utilizzare una rete neurale composta da un solo livello LSTM ed un livello completamente connesso per l'inferenza finale. Per le tecniche più promettenti  è stato incrementato progressivamente il numero dei soli livelli LSTM fino al raggiungimento ottimale dei risultati. 

%È importante però sottolineare che aumentando il numero di livelli della rete non deve variare il numero di parametri totali di quest'ultima altrimenti un'eventuale variazione dei risultati non potrebbe essere attribuita alla sola mutazione di profondità della rete, quindi ad ogni incremento di livello sarà anche necessario regolare di conseguenza il numero di hidden units per ognuno di essi.
\item \textbf{Hidden units} - Per quanto riguarda il numero di unità nascoste di ogni livello LSTM della rete, è stata utilizzata la seguente regola generale
$$
N_h = \frac{N_{samples}}{\alpha \cdot (N_{in} + N_{out})}
$$
dove $N_{samples}$ è il numero di esempi del \emph{training set}, $N_{in}$ e $N_{out}$ sono il numero di unità nascoste in entrata e in uscita rispettivamente e $\alpha$ è un moltiplicatore arbitrario, generalmente fra $2$ e $10$, che indica quanto generale vogliamo sia il nostro modello o, in altre parole, quanto vogliamo evitare l'\emph{overfitting}. In questo lavoro di tesi è stato scelto $\alpha \approx 5$ e di conseguenza ogni livello LSTM è stato strutturato con 64 hidden units.
\item \textbf{Loss function} - Visto che siamo in un contesto di classificazione multi-classe, dove però ogni azione appartiene ad una e una sola categoria, la \emph{loss function} scelta è la \emph{``categorical crossentropy"} ovvero l'unione fra un'attivazione \emph{soft-max} e una \emph{cross-entropy loss},

$$
f(s)_i = \frac{e^{s_i}}{\sum_j^C e^{s_j}} \ \ \ \ CCE = -\sum_i^C t_i \cdot log \Big(f(s)_i\Big)
%L(y,\hat{y}) = - \sum_{j=0}^M \sum_{i=0}^N (y_{ij} * log(\hat{y}_{ij}))
$$
dove $C$ è il numero totale di classi, $s_i$ è lo score uscente dalla rete neurale per la classe $i$ e $t_i$ è il valore target per la classe $i$. 

Considerando che il vettore target $\boldsymbol{t}$ è un vettore one-hot, composto cioè da tutti zero eccetto un solo $t_p = 1$, allora solo la classe corretta $C_p$ definirà il valore della loss, che sarà quindi
$$
CCE = - log\left( \frac{e^{s_p}}{\sum_j^C e^{s_j}}\right)
$$
dove $s_p$ è il valore che la rete neurale restituisce per la classe $p$.

\item \textbf{Batch size} - La batch size, vista la dimensione del dataset ed il numero di categorie, è stata scelta pari a $10\times N$,  dove $N$ è il numero di categorie del dataset, di modo da consentire ad ogni iterazione una sufficiente variabilità del batch ed una corretta generalizzazione nell'aggiornamento dei pesi. Per l'allenamento della rete sul dataset completo la batch size è stata quindi fissata a $600$.
\item \textbf{Ottimizzatore} - Per la fase iniziale degli esperimenti, ogni tecnica è stata testata utilizzando  \emph{RMSProp} \cite{RMSProp} come ottimizzatore, con un valore $\rho = 0.9$ ed un learning rate iniziale di $0.01$ decrementato gradualmente fino a $0,001$ in $100$ epoche (figura \ref{fig:schedulingLR}), di modo da testare velocemente ognuna di esse. Per le tecniche più promettenti è stato rimosso il vincolo delle $100$ epoche ed utilizzato un learning rate costante di $0,001$ dopo la $100$-esima epoca, interrompendo l'addestramento secondo il criterio scelto (spiegato nella prossima sezione).

\begin{figure}[htb]
\centering
\includegraphics[width=0.9\textwidth]{imgs/LR_scheduling.jpeg}
\caption{Scheduling del learning rate utilizzato per l'esplorazione di tutte le tecniche ideate. In questa fase l'addestramento è stato interrotto alla 100-esima epoca.}
\label{fig:schedulingLR}
\end{figure}

\item \textbf{Dropout} e \textbf{Dropout ricorrente} - Per una maggiore semplicità di strutturazione degli esperimenti, il \emph{dropout} ed il \emph{dropout ricorrente} sono sempre stati impostati entrambi allo stesso valore. Inizialmente, durante la fase di esplorazione delle tecniche, è stato assegnato a entrambi un valore pari a $0$, successivamente, per le tecniche più promettenti, sono stati fatti variare a $0.5 , 0.4, 0.3, 0.2, 0.15, 0.1$ e $0.05$.
\item \textbf{Regolarizzatore} e \textbf{Regolarizzatore ricorrente}  - Come per il dropout, anche il \emph{regolarizzatore} ed il \emph{regolarizzatore ricorrente} sono sempre stati settati entrambi allo stesso valore, inizialmente entrambi a $0$ e successivamente, per le tecniche più promettenti, sono stati fatti variare a $10^{-1}$, $10^{-2}$, $10^{-3}$, $10^{-4}$, $10^{-5}$, $10^{-6}$, $10^{-7}$ e $10^{-8}$. Ogni regolarizzatore utilizzato in questo lavoro di tesi è di tipo L2.
\item \textbf{Data augmentation} - il dataset di pose estratte è stato aumentato applicano \emph{la tecnica dello specchio} e del \emph{jittering} di modo da quadruplicarne la dimensione. Nello specifico, per quanto riguarda la tecnica del \emph{jittering}, ad ogni coordinata è stato aggiunto un valore casuale scelto da una distribuzione di Gauss di media $0$ e varianza $2$.

\end{itemize}

\section{Criterio di arresto dell'addestramento}
Sapere quando interrompere l'addestramento della rete per testarla sull'insieme di \emph{test} non è un compito facile. 

Le scuole di pensiero più diffuse sono sostanzialmente due e hanno come idea quella di arrestare l'addestramento:
\begin{enumerate}
\item al valor minimo della \textbf{loss di validazione}
\item al valore massimo dell'\textbf{accuratezza di validazione}
\end{enumerate} 
In questo lavoro di tesi sono state testate, per ogni tecnica, entrambe le ipotesi di modo da capire quale delle due è la più adatta al nostro caso di studio. Per fare ciò il dataset di allenamento è stato suddiviso in due parti:
\begin{itemize}
\item l'$\boldsymbol{80\%}$ come effettivo dataset di allenamento
\item il $\boldsymbol{20\%}$ come dataset di validazione.
\end{itemize}

Nel capitolo seguente verrà illustrato più in dettaglio NTU-RGB+D e sulla base delle ripetizioni e delle angolazioni di ripresa di questo dataset è stato scelto di creare il dataset di validazione come segue:
\begin{itemize}
\item per il criterio \textbf{cross view} sono stati scelti in maniera casuale il 60\% delle riprese frontali registrate dalla telecamera 2 ed il 60\% delle riprese frontali registrate dalla telecamera 3. Cosi facendo il dataset di training contiene tutte le angolazioni di ripresa a 90\% ed il 40\% delle riprese frontali per un totale di $30336$ video mentre il dataset di training contiene il 40\% di tutte le riprese frontali per un totale di $7584$ video.
\item per il criterio \textbf{cross subject} i video sono stati raggruppati per triplette di ripetizioni e sono state selezionate in maniera casuale il 20\% di esse. Cosi facendo sono stati ottenuti un dataset di training e di validazione di $32256$ e $8064$ video rispettivamente. 
\end{itemize}

Ogni suddivisione è stata ovviamente fatta bilanciando equamente il numero di azioni su entrambi i dataset.


%%%%%%  NTU RGB+D %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Il dataset NTU RGB+D}
Il recente sviluppo dei sensori di profondità ha permesso un notevole miglioramento degli studi in ambito 3D, come ad esempio il riconoscimento di oggetti o azioni in scenari tridimensionali. 

Essendo però un campo scientifico piuttosto ``giovane'', la ricerca di un adeguato dataset per il \emph{riconoscimento di azioni umane} che sia solido, voluminoso e vario può non essere un compito facile. 

Spesso i dataset pubblicamente disponibili sono composti da uno stretto gruppo di soggetti, il che riduce notevolmente la variabilità intra-classi e considerando che un'\emph{azione} dipende fortemente dall'età, il sesso, la cultura e le condizioni fisiche di chi la svolge, avere una sufficiente variabilità di soggetti nel dataset che stiamo analizzando è di vitale importanza.

Un'altra cosa estremamente importante è il numero di azioni che stiamo analizzando. Se il nostro dataset contiene poche tipologie di azioni, sarà facile distinguerle e classificarle fra loro, magari identificando un semplice pattern di movimento o addirittura la struttura di un oggetto coinvolto. Se invece il numero di azioni trattate è sufficientemente alto, allora i pattern di movimento e gli oggetti con i quali si interagisce vengono condivisi fra più classi, rendendo la classificazione estremamente più difficile.

Il terzo punto da considerare per scelta di un buon dataset è il punto di vista dal quale vengono riprese le azioni. La maggior parte dei dataset contengono video che riprendono l'azione da un unico punto di vista (spesso frontale), in altri casi invece i punti di vista sono strettamente due (magari frontale e di lato), ottenuti solo perchè sono state utlizzate più telecamere contemporaneamente.

Come ultima cosa, probabilmente la più importante, l'insufficienza di video nei dataset ci impedisce spesso di applicare tecniche di machine-learning al nostro problema, portandoci inevitabilmente ad una situazione di overfitting.


\newgeometry{left=1cm, right=1.5cm, bottom=0.1cm, top=1.5cm}
\begin{figure} [htb]
\centering
%\begin{minipage}[r]{1.2\textwidth}
\includegraphics[width=1\textwidth]{imgs/varietaNTURGB}
\caption{Alcuni esempi di frames del dataset NTU RGB+D. Nelle prime 5 righe è possibile notare la varietà degli attori partecipanti al progetto e dei diversi punti di ripresa utilizzati. La sesta riga mostra la varietà intra-classe per una stessa azione (scrivere alla tastiera). L'ultima riga mostra per uno stesso frame i valori RGB, i valori RGB + giunti, mappa di profondità ed i valori a infrarossi. }
\label{fig:varietaNTURGB}

%\end{minipage} 
\end{figure}

\restoregeometry

Consapevoli di quanto detto finora Amir Shahroudy, Jun Liu, Tian-Tsong Ng e Gang Wang hanno creato nel 2016 \emph{NTU RGB+D} \cite{NTURGB}, un vasto e variegato dataset dedito all'analisi di attività umane in ambito 3D.

NTU RGB+D si compone di 56880 RGB+D video, ottenuti riprendendo 40 differenti attori ed utilizzando Microsoft Kinect v2. Il dataset contiene i video RGB, le sequenze di profondità, lo skeleton dei soggetti (ovvero le posizioni 3D dei principali giunti corporei) ed i frames a infrarossi. I soggetti sono stati ripresi da 80 punti di vista differenti e la loro età varia dai 10 ai 35 anni, il che rende più realistica la variazione di qualità delle azioni svolte. Anche se tutte le riprese sono di tipo \emph{indoor}, gli scenari ricreati intorno agli attori sono estremamente vari.



\section{NTU RGB+D in dettaglio}
Ma vediamo adesso più nel dettaglio quali sono le informazioni a disposizione in questo dataset.

\textbf{Tipologie di dati} - I dati raccolti sono stati ottenuti con sensori Microsoft Kinect v2 che consentono di estrapolare dalle riprese 4 tipologie di dato: mappe di profondità, informazioni 3D dei giunti corporei, frame RGB e sequenze a infrarossi.

Le \emph{mappe di profondità} sono sequenze di valori in millimetri che rappresentano la distanza di ogni punto dalla telecamera. Ogni mappa di profondità è stata  poi ridotta ad una risoluzione di $512 \times 424$ senza perdita di informazione.

Le \emph{informazioni 3D dei giunti corporei} sono coordinate 3D dei 25 giunti corporei più importanti. Per ogni giunto e per ogni frame viene saltato il corrispondente pixel nel video RGB e la realtiva mappa di profondità. I 25 giunti corporei utilizzati sono raffigurati in figura \ref{fig:giuntiNTURGB}.

\begin{figure} [htb!]
\centering
\includegraphics[width=0.45\textwidth]{imgs/giuntiNTURGB}
\caption{La configurazione dei 25 giunti utilizzati in NTU RGB+D. Le etichette utilizzate per i giunti sono: 1 - base della colonna vetrebrale, 2 - punto medio della colonna vertebrale, 3 - collo, 4 - testa, 5 - spalla sinistra, 6 - gomito sinistro, 7 - polso sinistro, 8 - mano sinistra, 9 - spalla destra, 10 - gomito destro, 11 - polso destro, 12 - mano destra, 13 - anca sinistra, 14 - ginocchio sinistro, 15 - caviglia sinistra, 16 - piede sinistro, 17 - anca destra, 18 - ginocchio destro, 19 - caviglia destra, 20 - piede destro, 21 - colonna vertebrale, 22 - punta della mano sinistra, 23 - pollice sinistro, 24 - punta della mano destra, 25 - pollice destro.}
\label{fig:giuntiNTURGB}
\end{figure}

I \emph{video RGB} sono normali video registrati con una risoluzione $1920 \times 1080$.

Le \emph{sequenze a infrarossi} sono, come per le mappe di profondità, salvate frame per frame con una dimensione di $512 \times 424$.

\textbf{Le classi delle azioni} - Il dataset contiene 60 tipi di azioni differenti divise in 3 gruppi principali: 40 azioni \emph{quotidiane} (bere, mangiare, leggere, etc.), 9 azioni \emph{legale alla salute} (starnutire, barcollare, svenire, etc...) e 11 azioni \emph{di coppia} (dare un cazzotto a qualcuno, abbracciarsi, etc...)

\textbf{I soggetti} - Per la realizzazione di questo dataset sono stati utilizzati 40 attori di età compresa dai 10 ai 35 anni. In figura \ref{fig:varietaNTURGB} è possibile vedere come l'insieme delle persone scelte sia estremamente variegato in età, altezza e sesso. Ogni attore è rappresentato con un ID unico in tutto il dataset.

\textbf{I punti di ripresa} - Ogni azione è stata ripresa da 3 telecamere di modo da generare 3 prospettive diverse della stessa azione contemporaneamente. Le 3 telecamere sono state posizionate tutte alla stessa altezza e sempre con la stessa angolazione di ripresa, ovvero $\ang{-45}, \ang{0}$ e $\ang{45}$. 

\begin{figure}[htb!]
\centering
\includegraphics[width=0.85\textwidth]{imgs/disposizioneCamereNTURGB-3}
\caption{Disposizione delle telecamere per ogni setup. La telecamera 1 ha ripreso ogni azione da una posizione centrale mentre le telecamere 2 e 3 hanno ripreso le azioni con un'angolazione di $\ang{45}$. Ogni attore ha ripetuto l'azione 2 volte: una rivolto verso la telecamere di destra e una verso quella di sinistra. }
\label{fig:schemaTelecamere}
\end{figure}

Ad ogni attore è stato inoltre richiesto di ripetere l'azione due volte: una volta verso la telecamere di sinistra ed una verso quella di destra. Cosi facendo le 6 riprese ottenute forniscono 5 prospettive diverse della stessa azione, ovvero  quella frontale (ripresa 2 volte), quella dal profilo sinistro, quella dal profilo destro, quella ad una angolazione di $\ang{45}$ da sinistra e quella ad una angolazione di $\ang{45}$ da destra. Come per gli attori, l'ID assegnato ad ogni telecamera è fisso per tutto il dataset: la telecamera 1 è sempre quella che riprende da $\ang{0}$, la telecamera 2 da $\ang{45}$ e la 3 da $\ang{-45}$. Uno schema della disposizione delle telecamere e della procedura di ripresa è raffigurato in figura \ref{fig:schemaTelecamere}

Per aumentare ulteriormente la variabilità delle riprese, ad ogni \emph{setup} (ovvero ad ogni set di ripresa) le 3 telecamere sono state posizionate ad un'altezza e ad una distanza diversa dall'attore inquadrato. In tabella \ref{tab:puntiDivista} sono elencate le posizioni delle 3 telecamere per ogni setup.

\begin{table}[ht]
\centering
\begin{tabular}{| c | c | c | c | c | c |}
\hline
Setup & \thead{Altezza  \\ (m)} &  \thead{Distanza  \\ (m)} & Setup &\thead{Altezza  \\ (m)}   &   \thead{Distanza  \\ (m)} \\
\hline
1   &    1.7     &   3.5  &   2   &    1.7    &   2.5          \\ \hline
3   &    1.4     &   2.5  &   4  &    1.2    &   3.0          \\ \hline
5   &    1.2     &   3.0  &   6  &    0.8    &   3.5          \\ \hline
7   &    0.5     &   4.5  &   8  &    1.4    &   3.5          \\ \hline
9   &    0.8     &   2.0  &   10  &    1.8    &   3.0          \\ \hline
11   &    1.9     &   3.0  &   12  &    2.0    &   3.0          \\ \hline
13   &    2.1     &   3.0  &   14  &    2.2    &   3.0          \\ \hline
15   &    2.3     &   3.5  &   16  &    2.7    &   3.5          \\ \hline
17   &    2.5     &   3.0  &    &      &            \\
\hline
\end{tabular}
\caption{Altezza e distanza delle 3 telecamere per ogni setup di ripresa. Le altezze e le distanze sono espresse in metri. }
\label{tab:puntiDivista}
\end{table}

\section{Criteri di valutazione}
Per avere una valutazione standard dei risultati ottenuti su questo dataset, gli autori definiscono nel loro lavoro \cite{NTURGB} due particolari criteri di valutazione, di modo da poter allineare anche i risultati futuri e poterli comparare fra loro.

\subsection{Valutazione Cross-subject}
Per questa valutazione, i 40 attori sono stati divisi in due gruppi: \emph{training} e \emph{test} ed ogni gruppo è costituito da 20 attori. Così facendo è stato ottenuto un insieme di training di 40320 video e un insieme di test di 16560 video. 

Gli ID degli attori dell'insieme di training sono: 1, 2, 4, 5, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 27, 28, 31, 34, 35, 38. I rimanenti  fanno parte dell'insieme di test.
\subsection{Valutazione Cross-view}
Per quanto riguarda invece la valutazione \emph{cross-view}, tutti i video ripresi dalle telecamere 2 e 3 costituiscono l'insieme di training, mentre quelli ripresi dalla telecamera 1 l'insieme di test. In altre parole l'insieme di training include tutte le prospettive frontali del soggetto inquadrato e tutte quelle di profilo mentre l'insieme di test include tutte le prospettive a $\ang{45}$ e $-\ang{45}$. Dividendo il dataset con questo criterio si ottiene un insieme di training composto da 37920 video e quello di test da 18960.

%% Esperimenti svolti e risultati %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Esperimenti e risultati}
In questo capitolo verranno mostrati non solo i risultati ottenuti per i vari esperimenti effettuati ma anche il procedimento logico adottato al fine di raggiungere la miglior classificazione possibile.

La legenda di ogni tabella in questo capito sarà la seguente
\begin{itemize}
\item \textbf{rimoz $\boldsymbol{0}$} - rimozione degli zeri
\item \textbf{VIDEO/FRAME} - tecnica del centro del video/frame
\item \textbf{VIDEO/FRAME PERS}  - tecnica del centro del video/frame per ogni posa
\item \textbf{3-5-17 BAR} - tecnica dei 3-5-17 baricentri
\item \textbf{3-5-17 BAR ASS} - tecnica dei 3-5-17 baricentri in valore assoluto
\item \textbf{DIST REL} - tecnica delle distanze relative
\item \textbf{DIST CUM} - tecnica delle distanze cumulate
\item \textbf{NEXT} - tecnica del next frame con S=1
\item \textbf{NEXT 3-7-15} - tecnica del next frame con S=3-7-15
\item \textbf{norm} - normalizzazione globale
\item \textbf{normXY} - normalizzazione separata
\item \textbf{MIN LOSS} - i risultati sono stati ottenuti interropendo l'allenamento secondo il criterio del \emph{minor valore della loss di validazione}, se non specificato allora viene sottinteso del \emph{maggior valore dell'accuratezza di validazione}.
\item \textbf{semplice} - non è stata utilizzata alcuna tecnica aggiuntiva dopo l'estrazione della posa.

\end{itemize}

\section{Una porzione rappresentativa del dataset}

Come prima cosa, considerata la dimensione del dataset (56880 video), è stato scelto un sottoinsieme di azioni di modo da poter lavorare inizialmente su una porzione più piccola, sufficientemente rappresentativa e di difficile categorizzazione, che permettesse una più rapida esplorazione delle tecniche.

A questo scopo sono state scelte 8 azioni con le seguenti caratteristiche:
\begin{itemize}
\item 2 azioni di coppia simili fra loro
\item 3 coppie di azioni individuali simili fra loro
\end{itemize}
Le azioni scelte, fra quelle proposte da NTU-RGB+D, sono state:
\begin{enumerate}
\item dare uno pacca sulla schiena a qualcuno (azione di coppia)
\item toccare la tasca di qualcuno (azione di coppia)
\item leggere
\item giocare col telefonino/tablet
\item applaudire
\item strofinarsi le mani
\item scrivere
\item usare una tastiera
\end{enumerate}

Questa porzione di dataset così creata è sicuramente di difficile categorizzazione, dato che molte delle azioni scelte si differenziano solo per piccoli movimenti delle dita non distinguibili col solo della posa generata da PoseNet e Detectron2.

\section{Fase 1: Test di tutte le tecniche}
Per questa prima fase è stato scelto di testare ogni tecnica del capitolo 4 allenando una semplice rete neurale composta da un livello LSTM ed un livello completamente connesso per l'inferenza finale, ignorando per il momento dropout e regolarizzazione.

I risultati ottenuti sono illustrati in tabella \ref{tab:primaIterazione}.  


\newpage
\newgeometry{left=1cm, right=1.5cm, bottom=0.1cm, top=1.5cm}
\begin{figure}[htb!]
\centering
\includegraphics[width=0.9\textwidth]{imgs/esplorazione1L.jpg}
\caption{Risultati ottenuti per ogni tecnica allenando una rete con un solo livello LSTM. \emph{CV}= Cross View; \emph{CS}= Cross Subject.} 
\label{tab:primaIterazione}
\end{figure}
\restoregeometry
\newpage

%\newgeometry{left=1cm, right=1.5cm, bottom=0.1cm}
%\begin{center}
%\begin{longtable}{| l | c | c | c |}
%\hline
%Tecnica & \thead{Accuratezza \\ Cross  \\ Subject (\%)} &  \thead{Accuratezza \\ Cross  \\ View (\%)} & \thead{Media  \\ (\%) }\\
%\hline\hline
%rimoz  $0$ + 3BAR + norm & 74.92 & 68.48 & 71.70 \\ \hline
%rimoz  $0$ + VIDEO + norm & 76.50 & 66.39 & 71.45 \\ \hline
%rimoz  $0$ + DIST REL + norm & 76.42 & 64.36 & 70.39 \\ \hline
%rimoz  $0$ + FRAME + norm & 74.56 & 65.26 & 69.91 \\ \hline
%rimoz  $0$ + FRAME PERS + norm & 73.14 & 66.21 & 69.68 \\ \hline
%rimoz  $0$ + VIDEO + norm  (MIN LOSS) & 73.26 & 65.94 & 69.60 \\ \hline
%normXY & 71.60 & 66.89 & 69.25 \\ \hline
%rimoz  $0$ + 3BAR + norm  (MIN LOSS) & 72.98 & 65.40 & 69.19 \\ \hline
%rimoz  $0$ + VIDEO PERS + norm & 73.50 & 64.67 & 69.09 \\ \hline
%rimoz  $0$ + 5BAR + norm & 73.54 & 63.32 & 68.43 \\ \hline
%rimoz  $0$ + DIST REL + norm  (MIN LOSS) & 69.62 & 66.17 & 67.89 \\ \hline
%rimoz  $0$ + normXY & 69.66 & 65.94 & 67.80 \\ \hline
%rimoz  $0$ + FRAME + norm  (MIN LOSS) & 68.04 & 65.85 & 66.94 \\ \hline
%rimoz  $0$ + 3BAR ASS + norm & 64.79 & 69.02 & 66.91 \\ \hline
%normXY  (MIN LOSS) & 69.11 & 64.18 & 66.64 \\ \hline
%rimoz  $0$ + VIDEO PERS + norm  (MIN LOSS) & 69.15 & 63.68 & 66.41 \\ \hline
%rimoz  $0$ + FRAME PERS + norm  (MIN LOSS) & 68.28 & 63.68 & 65.98 \\ \hline
%rimoz  $0$ + 17BAR + norm & 69.98 & 61.91 & 65.94 \\ \hline
%rimoz  $0$ + NEXT 3 + norm & 67.37 & 63.68 & 65.52 \\ \hline
%rimoz  $0$ + NEXT 7 + norm & 67.72 & 61.82 & 64.77 \\ \hline
%rimoz  $0$ + NEXT 15 + norm & 69.34 & 59.96 & 64.65 \\ \hline
%rimoz  $0$ + 5BAR + norm  (MIN LOSS) & 69.50 & 58.79 & 64.14 \\ \hline
%rimoz  $0$ + norm & 71.40 & 53.85 & 62.62 \\ \hline
%rimoz  $0$ + 17BAR ASS + norm & 65.90 & 59.15 & 62.53 \\ \hline
%rimoz  $0$ + 3BAR ASS + norm  (MIN LOSS) & 61.04 & 63.09 & 62.06 \\ \hline
%rimoz  $0$ + NEXT 3 + norm  (MIN LOSS) & 63.17 & 59.56 & 61.36 \\ \hline
%rimoz  $0$ + DIST CUM + norm & 65.62 & 56.79 & 61.21 \\ \hline
%rimoz  $0$ + normXY  (MIN LOSS) & 65.82 & 55.93 & 60.88 \\ \hline
%rimoz  $0$ + NEXT + norm & 61.55 & 59.60 & 60.58 \\ \hline
%norm & 65.07 & 54.98 & 60.03 \\ \hline
%rimoz  $0$ + 5BAR ASS + norm & 59.45 & 59.28 & 59.37 \\ \hline
%rimoz  $0$ + DIST CUM + norm  (MIN LOSS) & 62.22 & 54.53 & 58.38 \\ \hline
%norm  (MIN LOSS) & 62.22 & 53.08 & 57.65 \\ \hline
%rimoz  $0$ + 17BAR + norm  (MIN LOSS) & 58.86 & 55.89 & 57.37 \\ \hline
%rimoz  $0$ + NEXT 7 + norm  (MIN LOSS) & 57.12 & 57.43 & 57.27 \\ \hline
%rimoz  $0$ + NEXT 15 + norm  (MIN LOSS) & 56.80 & 57.47 & 57.14 \\ \hline
%rimoz  $0$ + 17BAR ASS + norm  (MIN LOSS) & 60.09 & 53.35 & 56.72 \\ \hline
%rimoz  $0$ + 5BAR ASS + norm  (MIN LOSS) & 58.07 & 52.04 & 55.05 \\ \hline
%rimoz  $0$ + norm  (MIN LOSS) & 56.76 & 53.31 & 55.04 \\ \hline
%rimoz  $0$ + NEXT + norm  (MIN LOSS) & 47.67 & 52.31 & 49.99 \\ \hline
%semplice (MIN LOSS) & 43.63 & 41.49 & 42.56 \\ \hline
%semplice & 43.83 & 41.26 & 42.54 \\ \hline
%\caption{Risultati ottenuti applicando ogni tecnica in maniera ``\emph{naive}" (senza cioè dropout e regolarizzazioni) con una rete neurale composta da un solo livello LSTM e 64 hidden units.}
%\label{tab:primaIterazione}
%\end{longtable}
%\end{center}
%\restoregeometry

Da questa prima classifica dei risultati, apparte avere un'idea su quali sono le tecniche più efficienti, si evince anche qualche altro aspetto importante:\begin{itemize}
\item Categorizzare le azioni utilizzando semplicemente i valori della stima della posa ottenuti da Detectron-2 e Posenet è decisamente non sufficiente, ottenendo un'accuratezza massima del $42,21\%$ (in media)
\item Quanto trattiamo punti sul piano la \emph{normalizzazione separata} ha risultati nettamente migliori rispetto a quella \emph{globale} ($\approx+2\%$ in media).
\item In quasi tutti i casi è preferibile interrompere l'allenamento della rete al \emph{massimo valore di accuratezza di validazione} invece che al \emph{minor valore della loss di validazione}.
\end{itemize} 


\section{Fase 2: Numero di livelli}
A questo punto è interessante capire se aumentare il numero la profondità della rete porta o meno dei benefici. 

Sono state quindi usate le migliori tecniche della fase precedente per allenare 3 diverse reti neurali, così strutturate:
\begin{itemize}
\item \textbf{1 livello} LSTM da 64 hidden units ed un livello completamente connesso ($\approx 35k$ parametri)
\item \textbf{2 livelli} LSTM da 64 hidden units ciascuno ed un livello completamente connesso ($\approx 68k$ parametri)
\item \textbf{3 livelli} LSTM da 64 hidden units ciascuno ed un livello completamente connesso ($\approx 101k$ parametri).
\end{itemize}


%\begin{table}[ht]
%\centering
%\begin{tabular}{| l | c | c | c | c | c | c | c | c | c |}
%\hline
%  \multicolumn{1}{|c|}{Tecnica}  & \multicolumn{3}{c|}{1 livello} & \multicolumn{3}{c|}{ 2 livelli} & \multicolumn{3}{c|}{ 3 livelli} \\
% & \thead{CS  \\ (\%) } & \thead{CV  \\ (\%) } & \thead{Media  \\ (\%) }  & \thead{CS  \\ (\%) }  & \thead{CV  \\ (\%) } & \thead{Media  \\ (\%) } & \thead{CS  \\ (\%) }  &\thead{CV  \\ (\%) } & \thead{Media  \\ (\%) }\\
%\hline\hline
%3BAR & 68,48 & 74,92 & 71,70 & 66,76 & 77,73 & \textbf{72,24} & 66,30 & 73,26 & 69,78 \\ \hline
%VIDEO & 66,40 & 76,50 & \textbf{71,45} & 67,48 & 75,12 & 71,30 & 66,21 & 73,85 & 70,03 \\ \hline
%DIST REL & 64,36 & 76,42 & 70,39 & 64,58 & 79,83 & \textbf{72,21} & 65,49 & 74,92 & 70,21 \\ \hline
%FRAME & 65,26 & 74,57 & 69,91 & 67,75 & 74,76 & \textbf{71,26} & 67,39 & 71,92 & 69,65 \\ \hline
%FRAME PERS & 66,21 & 73,14 & 69,68 & 65,72 & 76,03 & \textbf{70,87} & 66,80 & 74,45 & 70,62 \\ \hline
%\end{tabular}
%\caption{Risultati ottenuti facendo variare la profondità della rete. CS = accuratezza per cross subject, CV = accuratezza per cross view. I valori in grassetto sono i migliori per la tecnica corrispondente.}
%\label{tab:incrementoLivelli}
%\end{table}

Come si può evincere dai risultati in tabella \ref{tab:incrementoLivelli}, incrementare la profondità della rete a due e tre livelli spesso aiuta il processo di classificazione delle azioni, anche se aumentare i livelli porta anche ad un maggiore rischio di overfitting sui dati di addestramento.

\newpage
\newgeometry{left=1cm, right=1.5cm, bottom=0.1cm, top=1.5cm}
\begin{figure}[htb!]
\centering
\includegraphics[width=0.45\textwidth]{imgs/esplorazione3Livelli.jpg}
\caption{Risultati ottenuti facendo variare la profondità della rete. I valori in grassetto sono i migliori per la tecnica corrispondente.}
\label{tab:incrementoLivelli}
\end{figure}
\restoregeometry
\newpage

%\begin{table}[ht]
%\centering
%\begin{tabular}{| c | c | c | c | c | c | c |}


\newgeometry{left=1.5cm, right=1.5cm, bottom=0.1cm}
\begin{longtable}{| c | c | c | l | c | c | c |}
\hline
 \multicolumn{7}{|c|}{Detectron 2} \\
\hline
\thead{Media  \\ (\%) } & \thead{Cross  \\ View \\(\%)} &  \thead{Cross  \\ Subject \\(\%)} &  \multicolumn{1}{c|}{Tecnica} & \thead{Drop  \\ (\%) }  &  Reg & Liv \\
\hline
$84,30$ & $88,89$ & $79,71$ & rimoz $0$ + 3BAR + norm & $15$ & $10^{-5}$ & $4$ \\ \hline

$84,23$ & $88,66$ & $79,81$ &  rimoz $0$ + 3BAR + norm  & $15$ &    & $4$ \\ \hline
$83,86$ & $87,64$ & $80,08$ &  rimoz $0$ + NEXT 3 + norm& $15$ &  $10^{-5}$ & $4$ \\ \hline
$83,76$ & $88,54$ & $78,98$ &  rimoz $0$ + 3BAR + norm (MIN LOSS) & $15$ &  & $4$ \\ \hline

$83,56$ & $88,04$ & $79,08$ &  rimoz $0$ + NEXT 3 + norm & $10$ &  & $4$ \\ \hline
%$82,03$ & $85,5$ & $78,57$ &  rimoz $0$ + normXY & $5$ &  & $3$ \\ \hline
%$80,98$ & $83,67$ & $78,3$ &  rimoz $0$ + normXY  (MIN LOSS) & $5$ &  & $3$ \\ \hline
$80,66$ & $84,69$ & $76,62$ &  rimoz $0$ + 5BAR + norm & $10$ &  & $4$ \\ \hline
$80,61$ & $84,59$ & $76,62$ &  rimoz $0$ + 5BAR + norm  (MIN LOSS) & $10$ &  & $4$ \\ \hline
$80,26$ & $84,54$ & $75,98$ &  rimoz $0$ + NEXT 3 + norm (MIN LOSS) & $15$ &  $10^{-5}$ & $4$ \\ \hline
$80,21$ & $84,83$ & $75,58$ &  rimoz $0$ + 5BAR + norm  (MIN LOSS) & $10$ & $10^{-6}$ & $3$ \\ \hline
$80,04$ & $85,42$ & $74,67$ &  rimoz $0$ + VIDEO + norm  (MIN LOSS) & $5$ &  & $3$ \\ \hline
%$79,91$ & $84,98$ & $74,84$ &  rimoz $0$ + norm & $$ &  & $3$ \\ \hline
$79,9$ & $85,22$ & $74,58$ &  rimoz $0$ + VIDEO + norm & $5$ &  & $3$ \\ \hline
$79,87$ & $84,62$ & $75,12$ &  rimoz $0$ + VIDEO + norm & $5$ & $10^{-6}$ & $3$ \\ \hline
$79,32$ & $83,6$ & $75,04$ &  rimoz $0$ + VIDEO PERS + norm & $5$ & $10^{-6}$ & $4$ \\ \hline
$79,13$ & $83,19$ & $75,06$ &  rimoz $0$ + VIDEO PERS + norm  (MIN LOSS) & $5$ & $10^{-6}$ & $4$ \\ \hline
$78,38$ & $80,79$ & $75,98$ &  rimoz $0$ + FRAME + norm  (MIN LOSS) & $5$ & $10^{-7}$ & $3$ \\ \hline
$78,29$ & $80,7$ & $75,88$ &  rimoz $0$ + FRAME + norm & $5$ & $10^{-7}$ & $3$ \\ \hline
%$77,98$ & $83,34$ & $72,63$ &  rimoz $0$ + norm & $5$ &  & $3$ \\ \hline
$77,88$ & $79,38$ & $76,38$ &  rimoz $0$ + FRAME + norm  (MIN LOSS) & $5$ &  & $3$ \\ \hline
%$77,24$ & $79,15$ & $75,34$ &  rimoz $0$ + normXY & $$ &  & $3$ \\ \hline
$76,69$ & $81,05$ & $72,32$ &  rimoz $0$ + VIDEO PERS + norm & $5$ &  & $3$ \\ \hline
$76,48$ & $80,7$ & $72,27$ &  rimoz $0$ + norm  (MIN LOSS) & $5$ &  & $3$ \\ \hline
$75,89$ & $77,95$ & $73,82$ &  rimoz $0$ + FRAME PERS + norm  (MIN LOSS) & $5$ & $10^{-7}$ & $4$ \\ \hline
$75,76$ & $77,79$ & $73,73$ &  rimoz $0$ + FRAME PERS + norm & $5$ & $10^{-7}$ & $4$ \\ \hline
$71,06$ & $71,98$ & $70,15$ &  rimoz $0$ + 3BAR ASS + norm  (MIN LOSS) & $5$ &  & $3$ \\ \hline
$71,02$ & $71,94$ & $70,1$ &  rimoz $0$ + 3BAR ASS + norm & $5$ &  & $3$ \\ \hline
$70,93$ & $72,34$ & $69,52$ &  rimoz $0$ + 3BAR ASS + norm & $5$ & $10^{-7}$ & $3$ \\ \hline 

\caption{Risultati delle tecniche più promettenti applicate a tutto NTU-RGB+D, utilizzando Detectron 2 per l'estrazione delle pose e interrompendo l'allenamento alla $100$-esima epoca. I risultati sono ordinati per valor medio di accuratezza globale su gli splitting \emph{Cross subject} e \emph{Cross view}. In questa tabella sono rappresentati solo i $3$ migliori risultati per ogni tecnica. \emph{Media} = media delle accuratezze su Cross subject e Cross view; \emph{Cross subject} = accuratezza sullo splitting Cross subject; \emph{Cross view} = accuratezza sullo splitting Cross view; \emph{Drop} = valore percentuale di \emph{dropout} e \emph{dropout ricorrente} durante la fase di allenamento; \emph{Reg} = valore del \emph{regolarizzatore L2} e del \emph{regolarizzatore ricorrente L2}; \emph{Liv} = livelli LSTM della rete.}
\label{tab:dropoutERegolarizzatoreDetectron}
\end{longtable}

\newpage
\begin{longtable}{| c | c | c | l | c | c | c |}
\hline
 \multicolumn{7}{|c|}{PoseNet} \\
\hline
\thead{Media  \\ (\%) } & \thead{Cross  \\ View \\(\%)} &  \thead{Cross  \\ Subject \\(\%)} &  \multicolumn{1}{c|}{Tecnica} & \thead{Drop  \\ (\%) }  &  Reg & Liv \\
\hline
 $76,05$  & $80,09$ &    $72,00$ &   rimoz $0$ + 3BAR + norm & 15 &  $$ & $4$ \\ \hline 
$75,99$  & $79,82$ &    $72,15$ &   rimoz $0$ + NEXT 3 + norm & 15 &  $10^{-6}$ & $4$ \\ \hline 
 $75,84$  & $78,91$ &    $72,77$ &   rimoz $0$ + 3BAR + norm (MIN LOSS) & 10 &  $10^{-6}$ & $4$ \\ \hline 
$75,83$  & $78,69$ &    $72,97$ &   rimoz $0$ + NEXT 3 + norm & 10 &  $10^{-6}$ & $4$ \\ \hline 
 $75,81$  & $79,95$ &    $71,68$ &   rimoz $0$ + 5BAR + norm & 5 &  $10^{-6}$ & $4$ \\ \hline 
 $75,63$  & $78,87$ &    $72,38$ &   rimoz $0$ + 3BAR + norm & 10 &  $10^{-6}$ & $4$ \\ \hline 
 $75,61$  & $78,63$ &    $72,58$ &   rimoz $0$ + NEXT 3 + norm  (MIN LOSS) & 10 &  $10^{-6}$ & $4$ \\ \hline 
 
 $75,59$  & $79,86$ &    $71,32$ &   rimoz $0$ + 5BAR + norm (MIN LOSS) & 5 &  $$ & $3$ \\ \hline 
 $74,48$  & $76,78$ &    $72,17$ &   rimoz $0$ + 5BAR + norm (MIN LOSS) & 10 &  $10^{-7}$ & $3$ \\ \hline 
 $74,03$  & $76,31$ &    $71,75$ &   rimoz $0$ + VIDEO + norm & 5 &  $10^{-6}$ & $3$ \\ \hline 
 $73,68$  & $76,53$ &    $70,83$ &   rimoz $0$ + VIDEO PERS + norm & 5 &  $10^{-6}$ & $4$ \\ \hline 
 $73,57$  & $76,53$ &    $70,60$ &   rimoz $0$ + VIDEO PERS + norm (MIN LOSS) & 5 &  $10^{-6}$ & $4$ \\ \hline 
 $73,54$  & $76,28$ &    $70,79$ &   rimoz $0$ + VIDEO + norm (MIN LOSS) & 5 &  $$ & $4$ \\ \hline 
 $73,50$  & $76,28$ &    $70,72$ &   rimoz $0$ + VIDEO + norm & 5 &  $$ & $4$ \\ \hline 
 $73,19$  & $75,16$ &    $71,21$ &   rimoz $0$ + VIDEO PERS + norm & 10 &  $10^{-6}$ & $4$ \\ \hline 
 $72,71$  & $76,45$ &    $68,98$ &   rimoz $0$ + FRAME + norm & 5 &  $10^{-6}$ & $4$ \\ \hline 
 $72,71$  & $76,44$ &    $68,98$ &   rimoz $0$ + FRAME + norm (MIN LOSS) & 5 &  $10^{-6}$ & $4$ \\ \hline 
 $72,46$  & $75,94$ &    $68,97$ &   rimoz $0$ + FRAME PERS + norm & 5 &  $10^{-7}$ & $3$ \\ \hline 
 $72,40$  & $75,94$ &    $68,85$ &   rimoz $0$ + FRAME PERS + norm (MIN LOSS) & 5 &  $10^{-7}$ & $3$ \\ \hline 
 $72,07$  & $74,25$ &    $69,90$ &   rimoz $0$ + FRAME + norm & 10 &  $10^{-6}$ & $3$ \\ \hline 
 $71,75$  & $73,83$ &    $69,66$ &   rimoz $0$ + FRAME PERS + norm (MIN LOSS) & 10 &  $10^{-7}$ & $3$ \\ \hline 
 $69,92$  & $71,86$ &    $67,98$ &   rimoz $0$ + 3BAR ASS + norm & 10 &  $10^{-6}$ & $3$ \\ \hline 
 $69,77$  & $71,72$ &    $67,82$ &   rimoz $0$ + 3BAR ASS + norm (MIN LOSS) & 10 &  $10^{-6}$ & $3$ \\ \hline 
 $68,75$  & $71,30$ &    $66,21$ &   rimoz $0$ + 3BAR ASS + norm (MIN LOSS) & 15 &  $10^{-5}$ & $3$ \\ \hline 
 \caption{Risultati delle tecniche più promettenti applicate a tutto NTU-RGB+D, utilizzando PoseNet per l'estrazione delle pose e interrompendo l'allenamento alla $100$-esima epoca. I risultati sono ordinati per valor medio di accuratezza globale su gli splitting \emph{Cross subject} e \emph{Cross view}. In questa tabella sono rappresentati solo i $3$ migliori risultati per ogni tecnica. \emph{Media} = media delle accuratezze su Cross subject e Cross view; \emph{Cross subject} = accuratezza sullo splitting Cross subject; \emph{Cross view} = accuratezza sullo splitting Cross view; \emph{Drop} = valore percentuale di \emph{dropout} e \emph{dropout ricorrente} durante la fase di allenamento; \emph{Reg} = valore del \emph{regolarizzatore L2} e del \emph{regolarizzatore ricorrente L2}; \emph{Liv} = livelli LSTM della rete.}
\label{tab:dropoutERegolarizzatorePosenet}
\end{longtable}

\restoregeometry

%\end{tabular}
%\label{tab:dropoutERegolarizzatore}
%\caption{risultati sul dataset completo stoppando l'allenamento alla 100-esima epoca}
%\end{table}



\section{Fase 3: Dropout e regolarizzatori}
Adesso che abbiamo un'idea di quali sono le tecniche più efficienti possiamo ridurre il raggio d'azione e testare solo quest'ultime sul tutto il dataset facendo variare per ognuna di esse il valore del regolarizzatore e del dropout (ricordiamo che per semplicità di strutturazione degli esperimenti il \emph{regolarizzatore ricorrente} ed il \emph{dropout ricorrente} sono stati sempre impostati allo stesso valore del \emph{regolarizzatore} e del \emph{dropout del kernel} rispettivamente).
I risultati ottenuti in questa fase sono elencati in tabella \ref{tab:dropoutERegolarizzatoreDetectron} e \ref{tab:dropoutERegolarizzatorePosenet}


\section{Fase 4: Allenamento completo tecnica migliore}
Quest'ultima fase è dedicata a finalizzare le migliori tecniche trovate, rimuovendo il vincolo delle 100 epoche, fissando il learning rate a $0,001$ oltre la 100-esima epocha e interrompendo l'allenamento al raggiungimento del miglior valore di accuratezza sul dataset di validazione. Inoltre solo per quest'ultima fase è stato deciso di testare anche l'aggiunta della normalizzazione separata prima di ogni tecnica ed in virtù dei risultati ottenuti nella fase precedente dalla tecnica dei 3 baricentri è stato deciso di testare anche la variante dei \emph{3 baricentri globali}, indicato in tabella con \emph{3BAR GLOB}.

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{| c | c | c | l | c | c | c |}
\hline
 \multicolumn{7}{|c|}{Detectron2} \\
\hline
\thead{Media  \\ (\%) } & \thead{Cross  \\ View \\(\%)} &  \thead{Cross  \\ Subject \\(\%)} &  \multicolumn{1}{c|}{Tecnica} & \thead{Drop  \\ (\%) }  &  Reg & Liv \\
\hline
$86,84$  & $91,89$ & $81,78$ & rimoz $0$  + normXY + 3BAR GLOB + norm & $15$ & $10^{-5}$ & $4$ \\ \hline
$86,62$  & $91,72$ & $81,52$ & rimoz $0$ + 3BAR GLOB + norm & $15$ & $10^{-5}$ & $4$ \\ \hline
$86,10$ &  $90,72$ &  $81,47$ & rimoz $0$ + normXY + NEXT 3 + norm & $15$ & $10^{-5}$ & $4$ \\ \hline
$85,70$ &  $89,80$ &  $81,59$ & rimoz $0$ + 3BAR + norm & $15$ & & $4$ \\ \hline
$85,35$ &  $89,22$ &  $80,74$ & rimoz $0$ + NEXT 3 + norm & $15$ & $10^{-5}$ & $4$ \\ \hline
$85,18$ &  $89,02$ &  $81,33$ & rimoz $0$ + normXY + 3BAR + norm & $15$ & & $4$ \\ \hline

\end{tabular}
}
\caption{Migliori risultati ottenuti con Detectron2 rimuovendo il vincolo delle 100 epoche e interrompendo l'addestramento seguendo il criterio d'arresto spiegato nel capitolo 4.}
\label{tab:risultatiFinaliDetectron}
\end{table}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{| c | c | c | l | c | c | c |}
\hline
 \multicolumn{7}{|c|}{PoseNet} \\
\hline
\thead{Media  \\ (\%) } & \thead{Cross  \\ View \\(\%)} &  \thead{Cross  \\ Subject \\(\%)} &  \multicolumn{1}{c|}{Tecnica} & \thead{Drop  \\ (\%) }  &  Reg & Liv \\
\hline
$82,51$  & $87,47$  & $77,55$ & rimoz $0$ + normXY + 3BAR + norm & $10$ &   & $4$ \\ \hline
$82,15$  & $86,98$  & $77,32$ & rimoz $0$ + 3BAR + norm & $10$ &   & $4$ \\ \hline
$81,55$  & $85,60$  & $77,50$ & rimoz $0$ + 3BAR GLOB + norm & $15$ &   & $4$ \\ \hline
$80,01$  & $84,64$  & $75,38$ & rimoz $0$ + normXY + 3BAR GLOB + norm & $15$ &   & $4$ \\ \hline
$78,90$  & $82,74$  & $75,06$ & rimoz $0$ + normXY + NEXT 3 + norm & $15$ &  $10^{-5}$ & $4$ \\ \hline
$77,64$  & $81,74$  & $73,53$ & rimoz $0$ + NEXT 3 + norm & $15$ &  $10^{-5}$ & $4$ \\ \hline

\end{tabular}
}
\caption{Migliori risultati ottenuti con PoseNet rimuovendo il vincolo delle 100 epoche e interrompendo l'addestramento seguendo il criterio d'arresto spiegato nel capitolo 4.}
\label{tab:risultatiFinaliPosenet}
\end{table}

Per determinare quale fosse il picco massimo dell'accuratezza e il picco minimo della loss è stata usata la tecnica \emph{dell'asta}, ovvero è stato fissato un valore arbitrario di $150$ epoche di inattività sull'aggiornamento dei rispettivi estremi per decretarne il vincitore. Terminato questo addestramento è stata addestrata la rete per altre 300 epoche utilizzando anche i dati del dataset di validazione.

L'addestramento cosi strutturato ha richiesto circa 1200 epoche ed i risultati ottenuti al termine di questo lungo procedimento sono riassunti in tabella \ref{tab:risultatiFinaliDetectron} e \ref{tab:risultatiFinaliPosenet}.

\section{Fase 5: Combinazione tecniche migliori}
Adesso che siamo riusciti a trovare quali sono le tecniche migliori su tutto il dataset possiamo combinarle fra loro per verificare se una loro cooperazione può aiutare o meno la classificazione. Il risultato finale sarà quindi la media delle inferenze generate dai due modelli in combinazione. Per ogni combinazione abbiamo ripetuto l'addestramento seguendo lo stesso procedimento adottato per gli altri test ed utilizzando la seguente funzione loss,
$$
\mathcal{L} = \alpha\cdot\mathcal{L}_1+\alpha\cdot\mathcal{L}_2
$$
dove $\mathcal{L}_1$ e $\mathcal{L}_2$ sono le funzioni loss delle due tecniche in combinazione e $\alpha$ è stato impostato a $0.5$.

I risultati così ottenuti sono rappresentati in tabella \ref{tab:combinazioneTecnicheDetectron} e \ref{tab:combinazioneTecnichePosenet}.

% DETECTRON2
\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{| c | c | c | l | c | c | c |}
\hline
 \multicolumn{7}{|c|}{Detectron2} \\
\hline
\thead{Media  \\ (\%) } & \thead{Cross  \\ View \\(\%)} &  \thead{Cross  \\ Subject \\(\%)} &  \multicolumn{1}{c|}{Tecnica} & \thead{Drop  \\ (\%) }  &  Reg & Liv \\
\hline
\multirow{2}{*}{$88,51$}  &  \multirow{2}{*}{$93,69$}  &  \multirow{2}{*}{$83,32$}& rimoz $0$ + normXY + 3BAR GLOB + norm & $15$ & $10^{-5}$ & $4$ \\
 & &  & rimoz $0$ + normXY + NEXT 3 + norm & $15$ & $10^{-5}$ & $4$ \\ \hline
\multirow{2}{*}{$85,05$}  &  \multirow{2}{*}{$90,23$}  &  \multirow{2}{*}{$79,87$}& rimoz $0$ + normXY + 3BAR GLOB + norm & $15$ & $10^{-5}$ & $4$ \\
 & &  & rimoz $0$ + normXY + 3 BAR + norm & $15$ & & $4$ \\ \hline
\end{tabular}
}
\caption{Migliori risultati ottenuti combinando le migliori tecniche per Detectron2.}
\label{tab:combinazioneTecnicheDetectron}
\end{table}

% POSENET
\begin{table}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{| c | c | c | l | c | c | c |}
\hline
 \multicolumn{7}{|c|}{PoseNet} \\
\hline
\thead{Media  \\ (\%) } & \thead{Cross  \\ View \\(\%)} &  \thead{Cross  \\ Subject \\(\%)} &  \multicolumn{1}{c|}{Tecnica} & \thead{Drop  \\ (\%) }  &  Reg & Liv \\
\hline
\multirow{2}{*}{$83,43$}  &  \multirow{2}{*}{$87.79$}  &  \multirow{2}{*}{$79,06$}& rimoz $0$ + normXY + 3BAR PERS + norm & $10$ &  & $4$ \\
 & &  & rimoz $0$ + normXY + NEXT 3 + norm & $15$ & $10^{-5}$ & $4$ \\ \hline
\multirow{2}{*}{$81,56$}  &  \multirow{2}{*}{$85,39$}  &  \multirow{2}{*}{$77,74$}& rimoz $0$ + normXY + 3BAR GLOB + norm & $15$ & $10^{-5}$ & $4$ \\
 & &  & rimoz $0$ + normXY + NEXT 3 + norm & $15$ & $10^{-5}$ & $4$ \\ \hline

\end{tabular}
}
\caption{Migliori risultati ottenuti combinando le migliori tecniche per Posenet.}
\label{tab:combinazioneTecnichePosenet}
\end{table}

\newpage
\ 


%% Esperimenti svolti e risultati %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusioni e sviluppi futuri}
In questo lavoro di tesi ci siamo dedicati al riconoscimento e alla classificazione di azioni umane tramite tecniche di apprendimento profondo per la stima della posa. 

A tale scopo abbiamo deciso di ideare un algoritmo che non avesse bisogno di informazioni iniziali complesse, come ad esempio la posizione dei giunti dei soggetti inquadrati, ma che attraverso l'uso dei soli video RGB fosse in grado di estrapolare tutte le informazioni necessarie.

Al fine di ottenere il miglior algoritmo abbiamo eseguito un serie di esperimenti strutturati secondo un procedimento ben preciso, che permettesse l'esplorazione rapida di ogni tecnica ideata affinando progressivamente i risultati per quelle più promettenti.

Quello che abbiamo ottenuto alla fine del processo sono due algoritmi con una discreta capacità di classificazione della azioni umane e per la loro semplicità anche un'elevata portabilità.

Nello specifico, gli algoritmi che proponiamo sono:
\begin{itemize}
\item il classificatore ``\emph{3BAR-NEXT-Detectron}" con un accuratezza Cross subject di $83,32\%$ e Cross view di $93,69\%$  
\item il classificatore ``\emph{3BAR-NEXT-Posenet}" con un accuratezza Cross subject di $79,06\%$ e Cross view di $87,79\%$.
\end{itemize}

Questi due algoritmi, pur facendo uso dei soli video RGB, hanno prestazioni comparabili a molti altri lavori scientifici facenti uso dalla posizione dei giunti fornita da dataset stesso, inoltre per la loro semplicità sono facilmente migliorabili. 

Una visione globale dei risultati ottenuti con i due algoritmi, comparati a quelli dello stato dell'arte, è sintetizzata in tabella \ref{tab:nostriAlgoritmiNelMondo}.
\begin{table}
\centering
\begin{tabular}{| c | c | c | c | c | c | c |}
\hline
Articolo & Posa & RGB & IR & \thead{Accuratezza  \\ cross-view \\ (\%)} & \thead{Accuratezza  \\ cross-subject \\ (\%)} & Media \\
\hline
Lie Group \cite{LieGroup}   				& \checkmark  &  & & 52.8     &   50.1 & 51.45 \\ \hline
H-RNN \cite{HRNN} 						& \checkmark  &  & & 64.0     &   59.1 & 61.55\\ \hline
Deep LSTM \cite{DeppLSTM} 				& \checkmark  &  & & 67.3     &   60.7 & 64.00\\ \hline
PA-LSTM \cite{DeppLSTM}   				& \checkmark  &  & & 70.3     &   62.9 & 66.60\\ \hline
ST-LSTM+TS \cite{ST-LSTM+TS} 			& \checkmark  &  & & 77.7     &   69.2 & 73.45 \\ \hline
Temporal Conv \cite{TemporalConv}   		& \checkmark  &  &  &83.1     &   74.3 & 78.78\\ \hline
Skelemotion \cite{Skelemotion} 			&  \checkmark &   & & 84.7 	&  76.5 & 80.60\\ \hline 
C-CNN+MTLN \cite{C-CNN+MTLN}  		& \checkmark  &   & & 84.8     &   79.6 & 82.20 \\ \hline
VA-LSTM \cite{VA-LSTM}					& \checkmark  &   & & 87.6     &   79.4 & 83.50 \\ \hline
ST-GCN \cite{ST-GCN}   					& \checkmark  &   & & 88.3     &   81.5 &  84.90\\ \hline
SR-TSL \cite{SR-TSL} 					& \checkmark  &   & & 92.4     &   84.8 & 88.60 \\ \hline
PB-GCN  \cite{PB-GCN}					& \checkmark  &  & & 93.2     &   87.5 & 90.35\\ \hline 
MFAS \cite{MFAS}						&  \checkmark & \checkmark & & - &   90.04 & - \\ \hline
FUSION \cite{FUSION}					&  \checkmark &  & \checkmark& 94.5 &   91.6 & 93.05\\ \hline
PoseMap \cite{PREV1}					& \checkmark & \checkmark & & 95.2     &   91.7 & 93.45\\ \hline
MMTM \cite{MMTM}						&  \checkmark & \checkmark & & -  &   91.99 & - \\ \hline
DSSCA-SSLM \cite{DSSCA-SSLM} 			&  & \checkmark  & & -     	&  74.9 & - \\ \hline 
\rowcolor{highlightColor} $\boldsymbol{3}$\textbf{BAR-NEXT-PoseNet} 	& &  \checkmark  & & \textbf{87.79}     &   \textbf{79.06} & \textbf{83.43}\\ \hline
Chained  \cite{Chained} 					&   & \checkmark  & & -     	&   80.8 & - \\ \hline
\rowcolor{highlightColor} $\boldsymbol{3}$\textbf{BAR-NEXT-Detectron} 	& &  \checkmark  & & \textbf{93.69}     &   \textbf{83.32} & \textbf{88.51}\\ \hline
2D-3D-Softargmax \cite{2D-3D-Softargmax} 	&   & \checkmark  & & -     	&   85.5 & - \\ \hline
Glimpse Clouds \cite{GlimpseClouds}		&   & \checkmark & & 93.2     &   86.6 & 89.90\\ \hline 
Action Machine \cite{PREV2}				&   & \checkmark & & 97.2     &   94.3 & 95.75 \\ \hline
\end{tabular}
\caption{Risultati di \emph{3-BAR-Detectron} e \emph{3-BAR-Posenet} comparati allo stato dell'arte.}
\label{tab:nostriAlgoritmiNelMondo}
\end{table}

\section{Sviluppi futuri}
Gli sviluppi futuri per gli algoritmi proposti sono molteplici. In questo lavoro di tesi ci siamo concentrati principalmente sulla trasformazione dei video RGB in una sequenza di pose successivamente rielaborate ed abbiamo ignorato completamente l'informazione utilizzabile dalla classificazione delle azioni utilizzando direttamente i valori RGB dei video. Potrebbe essere interessante combinare l'informazione proveniente dalle due tipologie di classificazione.

Un altro possibile sviluppo di questo lavoro è testare gli algoritmi proposti anche sui dataset \emph{NTU-RGB+D120} \cite{NTURGB120}, \emph{MSR Daily Activity3D} \cite{MSR} e \emph{Northwestern-UCLA Multiview Action 3D (NUCLA)} \cite{NUCLA} per validare le prestazioni ottenute su NTU-RGB+D oppure analizzare le eventuali differenze.




%--------------------------------------------------------------

\begin{thebibliography}{99}
\bibitem{PosenetArticle} PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model - \emph{George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros Gidaris, Jonathan Tompson, Kevin Murphy} - ECCV 2018

\bibitem{COCO-2016} Coco 2016 keypoint challenge - \emph{Lin, T.Y., Cui, Y., Patterson, G., Ronchi, M.R., Bourdev, L., Girshick, R., Dollr,P.} - ECCV 2016

\bibitem{PosenetLink} PoseNet with TensorFlow.js - \emph{https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5} 

\bibitem{Detectron2Link} Detectron2 - \emph{Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick - https://github.com/facebookresearch/detectron2} - 2019

\bibitem{DetectronLink} Detectron - \emph{Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollàr, Kaiming He - https://github.com/facebookresearch/detectron} - 2018

\bibitem{caffe2} Caffe2 - \emph{https://caffe2.ai/docs}

\bibitem{maskRCNN} Mask R-CNN - \emph{Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick} - IEEE Transactions on Pattern Analysis and Machine Intelligence. PP. 1-1. 10.1109/TPAMI.2018.2844175,  2018

\bibitem{focalLossDetection} Focal Loss for Dense Object Detection - \emph{Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár} - IEEE Transactions on Pattern Analysis and Machine Intelligence. PP. 1-1. 10.1109/TPAMI.2018.2858826, 2018

\bibitem{fasterRCNN} Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - \emph{Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun} - Conference on Neural Information Processing Systems (NIPS), 2015

\bibitem{fastRCNN} Fast R-CNN - \emph{Ross Girshick} - IEEE International Conference on Computer Vision (ICCV). 10.1109/ICCV.2015.169, 2015

\bibitem{RFCN} R-FCN: Object Detection via Region-based Fully Convolutional Networks - \emph{Jifeng Dai, Yi Li, Kaiming He, Jian Sun} - arXiv:1605.06409v2  [cs.CV] 2016

\bibitem{resNetXt} Aggregated Residual Transformations for Deep Neural Networks - \emph{Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He} - 5987-5995. 10.1109/CVPR.2017.634, 2017

\bibitem{resNet} Deep Residual Learning for Image Recognition - \emph{Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun} - 770-778. 10.1109/CVPR.2016.90, 2016

\bibitem{featurePyramid} Feature Pyramid Networks for Object Detection - \emph{Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie} - 936-944. 10.1109/CVPR.2017.106, 2017

\bibitem{vgg16} Very Deep Convolutional Networks for Large-Scale Image Recognition - \emph{Karen Simonyan, Andrew Zisserman} -  arXiv:1409.1556 [cs.CV] 2014

\bibitem{cascadeRCNN} Cascade R-CNN: High Quality Object Detection and Instance Segmentation - \emph{Zhaowei Cai, Nuno Vasconcelos} - IEEE Transactions on Pattern Analysis and Machine Intelligence. PP. 1-1. 10.1109/TPAMI.2019.2956516,  2019

\bibitem{panopticFeatures} Panoptic Feature Pyramid Networks - \emph{Alexander Kirillov, Ross Girshick, Kaiming He, Piotr Dollár} - 6392-6401. 10.1109/CVPR.2019.00656,  2019

\bibitem{tensorMask} TensorMask: A Foundation for Dense Object Segmentation - \emph{Xinlei Chen, Ross Girshick, Kaiming He, Piotr Dollár} - 2061-2069. 10.1109/ICCV.2019.00215, 2019

\bibitem{NTURGB} NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis - \emph{Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang} - 10.1109/CVPR.2016.115, 2016

\bibitem{NTURGB120} NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding - \emph{Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan and Alex C. Kot} - IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2019

\bibitem{LSTM} Long short-term memory - \emph{Sepp Hochreiter; Jürgen Schmidhuber} - Neural computation. 9. 1735-80. 10.1162/neco.1997.9.8.1735, 1997


\bibitem{peepholeLSTM} Recurrent Nets that Time and Count - \emph{Felix A. Gers, Jurgen Schmidhuber} -  Proceedings of the International Joint Conference on Neural Networks. 3. 189 - 194 vol.3. 10.1109/IJCNN.2000.861302, 2000

\bibitem{GRU} Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation - \emph{Kyunghyun Cho; Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio } - 10.3115/v1/D14-1179, 2014

\bibitem{PREV1} Recognizing Human Actions as the Evolution of Pose Estimation Maps  - \emph{Mengyuan Liu, Junsong Yuan} - 1159-1168. 10.1109/CVPR.2018.00127,  2018

\bibitem{PREV2} Action Machine: Rethinking Action Recognition in Trimmed Videos - \emph{Jiagang Zhu, Wei Zou, Liang Xu, Yiming Hu, Zheng Zhu, Manyu Chang, Junjie Huang, Guan Huang, Dalong Du} - arXiv:1812.05770v2  [cs.CV], 2018 

\bibitem{QuoVadis}  Quo vadis, action recognition? A new model and the kinetics dataset. \emph{J. Carreira, A. Zisserman} - 4724-4733. 10.1109/CVPR.2017.502,  2017

\bibitem{Deformable} Deformable convolutional networks.  \emph{J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei.} - DEStech Transactions on Computer Science and Engineering. 10.12783/dtcse/iteee2019/28747, 2017

\bibitem{FUSION} Infrared and 3D skeleton feature fusion for RGB-D action recognition - \emph{Alban Main de Boissiere e Rita Noumeir} - 2020
 
\bibitem{TempConv}Interpretable 3D human action analysis with temporal convolutional networks - \emph{T. S. Kim, A. Reiter} - 1623-1631. 10.1109/CVPRW.2017.207, 2017

\bibitem{RMSProp} RMSProp and equilibrated adaptive learning rates for non-convex optimization - \emph{Yann N. Dauphin, Harm de Vries, Junyoung Chung, Yoshua Bengio} - arXiv. 35, 2015

\bibitem{LieGroup} Human action recognition by representing 3d skeletons as points in a lie group - \emph{R. Vemulapalli, F. Arrate e R. Chellappa} - Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 10.1109/CVPR.2014.82,  2014

\bibitem{HRNN} Hierarchical recurrent neural network for skeleton based action recognition - \emph{Y. Du, W. Wang e L. Wang.} - 1110-1118. 10.1109/CVPR.2015.7298714, 2015

\bibitem{DeppLSTM} Co-occurrence Feature Learning for Skeleton based Action Recognition using Regularized Deep LSTM Networks - \emph{Zhu, Wentao Lan, Cuiling Xing, Junliang  Li, Yanghao Shen, Li Zeng, Wenjun Xie, Xiaohui.} - 2016

\bibitem{ST-LSTM+TS} Spatio-temporal lstm with trust gates for 3d human action recognition - \emph{J. Liu, A. Shahroudy, D. Xu e G. Wang} - 9907. 10.1007/978-3-319-46487-9\_50, 2016 

\bibitem{TemporalConv} Interpretable 3d human action analysis with temporal convolutional networks - \emph{T. S. Kim e A. Reiter} - 1623-1631. 10.1109/CVPRW.2017.207, 2017

\bibitem{C-CNN+MTLN} A new representation of skeleton sequences for 3d action recognition - \emph{Q. Ke, M. Bennamoun, S. An, F. Sohel e F. Boussaid} - 10.1109/CVPR.2017.486, 2017

\bibitem{VA-LSTM} View adaptive recurrent neural networks for high performance human action recognition from skeleton data - \emph{P. Zhang, C. Lan, J. Xing, W. Zeng, J. Xue e N. Zheng} - 2136-2145. 10.1109/ICCV.2017.233, 2017

\bibitem{ST-GCN} Spatial temporal graph convolutional networks for skeleton-based action recognition - \emph{S. Yan, Y. Xiong e D. Lin} - 2018

\bibitem{SR-TSL} Skeletonbased action recognition with spatial reasoning and temporal stack learning - \emph{C. Si, Y. Jing, W. Wang, L. Wang e T. Tan} - 2018

\bibitem{Chained} Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection - \emph{M. Zolfaghari, G. L. Oliveira, N. Sedaghat e T. Brox} - 2923-2932. 10.1109/ICCV.2017.316, 2017

\bibitem{DSSCA-SSLM} Deep multimodal feature analysis for action recognition in rgb+d videos - \emph{A. Shahroudy, T. Ng, Y. Gong e G. Wang} - IEEE transactions on pattern analysis and machine intelligence. PP. 10.1109/TPAMI.2017.2691321, 2017

\bibitem{2D-3D-Softargmax} 2d/3d pose estimation and action recognition using multitask deep learning - \emph{D. C. Luvizon, D. Picard e H. Tabia.} - 5137-5146. 10.1109/CVPR.2018.00539, 2018

\bibitem{GlimpseClouds} Glimpse clouds: Human activity recognition from unstructured feature points - \emph{F. Baradel, C. Wolf, J. Mille e G. W. Taylor} - 469-478. 10.1109/CVPR.2018.00056, 2018

\bibitem{MMTM} MMTM: Multimodal Transfer Module for CNN Fusion - \emph{H. R. V. Joze, A. Shaban, M. L. Iuzzolino, K. Koishida} - 2019

\bibitem{Skelemotion} SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition - \emph{C Caetano, J Sena, F Brémond, J. A. dos Santos, W. R. Schwartz} - 10.1109/AVSS.2019.8909840, 2019

\bibitem{PB-GCN} Part-based Graph Convolutional Network for Action Recognition - \emph{K Thakkar, P. J. Narayanan} - 2018

\bibitem{MFAS} MFAS: Multimodal Fusion Architecture Search - \emph{J. Perez-Rua, V. Vielzeuf, S. Pateux, M. Baccouche, F. Jurie} - 6959-6968. 10.1109/CVPR.2019.00713, 2019

\bibitem{MSR} Mining actionlet ensemble for action recognition with depth cameras - \emph{J. Wang, Z. Liu, Y. Wu, and J. Yuan.} - Proceedings / CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 1290-1297. 10.1109/CVPR.2012.6247813, 2012

\bibitem{NUCLA} Cross-view action modeling, learning and recognition - \emph{J. Wang, X. Nie, Y. Xia, Y. Wu, and S. Zhu.} - Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 10.1109/CVPR.2014.339, 2014

\end{thebibliography}
\end{document}