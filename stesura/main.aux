\relax 
\select@language{italian}
\@writefile{toc}{\select@language{italian}}
\@writefile{lof}{\select@language{italian}}
\@writefile{lot}{\select@language{italian}}
\select@language{italian}
\@writefile{toc}{\select@language{italian}}
\@writefile{lof}{\select@language{italian}}
\@writefile{lot}{\select@language{italian}}
\@writefile{toc}{\contentsline {chapter}{Prefazione}{iii}}
\@writefile{toc}{\contentsline {chapter}{Introduzione}{iv}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}L'apprendimento automatico}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Deep learning}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Le reti neurali}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Un neurone biologico (sinistra) ed un neurone artificiale (destra) a confronto. La struttura del neurone artificiale si ispira molto a quella del neurone biologico. \relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:neurone}{{1.1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Struttura di una rete neurale. Ogni livello della rete, eccetto il primo e l'ultimo, viene chiamato \emph  {hidden layer} ed i neuroni che lo compongono prendono il nome di \emph  {hidden units}. Magiore sar\`a il numero di hidden units e maggiore sar\`a la capacit\`a espressiva della rete. \relax }}{5}}
\newlabel{fig:hiddenLayer}{{1.2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Un esempio di rete convoluzionale per l'analisi di un'immagine. \emph  {Conv} = ``Convolution layer", moltiplicazione vettoriale della porzione di layer al quale ogni neurone \`e collegato; \emph  {Pool} = ``Pooling layer", sintetizzazione della porzione di layer connessa; \emph  {FC} = ``Fully connected layer"; \emph  {Softmax} = operazione conclusiva per la decisione del valore di output.\relax }}{7}}
\newlabel{fig:convNetwork}{{1.3}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Le reti neurali ricorrenti}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Un esempio di rete ricorrente con due hidden layers.\relax }}{8}}
\newlabel{fig:reteRicorrente}{{1.4}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Esempio di ``srotolamento" di una rete ricorrente, ovvero la trasformazione da una rete ricorrente ad una combinazione equivalente di reti neurali semplici.\relax }}{8}}
\newlabel{fig:srotolamentoRNN}{{1.5}{8}}
\citation{LSTM}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Le reti LSTM}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Esempio di ``srotolamento" di una rete LSTM. In questa figura ogni unit\`a di rete ``A'' prende in input al tempo $t$ un elemento $X_t$ della sequenza temporale $X$ e restituisce uno stato in output $h_t$. \emph  {tanh} e $\sigma $ sono le funzioni di tangente iperbolica e sigmoidale rispettivamente, mentre ``$+$" e ``$\times $" sono rispettivamente le operazioni vettoriali di somma e moltiplicazione.\relax }}{10}}
\newlabel{fig:srotolamentoLSTM}{{1.6}{10}}
\newlabel{fig:cellState}{{1.7a}{11}}
\newlabel{sub@fig:cellState}{{a}{11}}
\newlabel{fig:gate}{{1.7b}{11}}
\newlabel{sub@fig:gate}{{b}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces I concetti chiave di una LSTM.\relax }}{11}}
\newlabel{fig:LSTMcellStateAndGate}{{1.7}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Forget gate di una LSTM\relax }}{11}}
\newlabel{fig:forgetGate}{{1.8}{11}}
\newlabel{fig:inputGate1}{{1.9a}{12}}
\newlabel{sub@fig:inputGate1}{{a}{12}}
\newlabel{fig:inputGate2}{{1.9b}{12}}
\newlabel{sub@fig:inputGate2}{{b}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Input gate.\relax }}{12}}
\newlabel{fig:LSTMinputForgetGate}{{1.9}{12}}
\citation{peepholeLSTM}
\citation{GRU}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Output gate di una LSTM\relax }}{13}}
\newlabel{fig:outputGate}{{1.10}{13}}
\citation{TempConv}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Lavori precedenti}{14}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{PREV1}
\citation{PREV2}
\citation{PREV3}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Recognizing Human Actions as the Evolution of Pose Estimation Maps (2018)}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Processo di creazione della heatmap e della posa a partire da un singolo frame del video. Ogni giunto corporeo viene stimato singolarmente per poi essere successivamente combinato agli altri.\relax }}{16}}
\newlabel{fig:heatmapAndPose}{{2.1}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Sintetizzazione temporale delle heatmaps.\relax }}{17}}
\newlabel{fig:sintTempHeatmap}{{2.2}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Sintetizzazione spazio-temporale delle heatmaps.\relax }}{18}}
\newlabel{fig:spatialTemporalRank}{{2.3}{18}}
\citation{NTURGB}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Sintetizzazione spazio-temporale delle heatmaps.\relax }}{19}}
\newlabel{fig:ordineGiunti}{{2.4}{19}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Risultati ottenuti da ``Recognizing Human Actions as the Evolution of Pose Estimation Maps - (2018)"\relax }}{20}}
\newlabel{tab:risultati4}{{2.1}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition (2019)}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Vertex feature encoding and hierarchical temporal modelling in a spatial-temporal graph convolutional network for action recognition (2019)}{20}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Stima della posa}{21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Esempi di stima della posa. In alto tre esempi di stima della posa utilizzando modelli di tipo volumetrico. In basso due esempi di stima della posa ottenuti utilizando modelli di tipo scheletrici. \relax }}{21}}
\citation{PosenetArticle}
\citation{Detectron2Link}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Un esempio di utilizzo in campo medico della stima della posa\relax }}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}PoseNet}{23}}
\citation{COCO-2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Stima dei key-points}{24}}
\citation{PosenetArticle}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Generazione con PoseNet delle heat-maps per ogni tipologia di key-point\relax }}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Esempio di stima degli offset a corto raggio con PoseNet\relax }}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Esempio di stima degli offset a medio raggio con PoseNet. L'intento \`e quello di raggruppare i keypoints appartenenti alla stessa persona.\relax }}{26}}
\newlabel{fig:midRangeOffset}{{3.5}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Raggruppamento dei key-points in istanze di persona}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Struttura ad albero utilizzata da PoseNet per raggruppare i key-points appartenenti alla stessa persona\relax }}{26}}
\newlabel{fig:strutturakeypoints}{{3.6}{26}}
\citation{DetectronLink}
\citation{caffe2}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Combinazione delle fasi adottate da PoseNet per il riconoscimento della posa in un'immagine.\relax }}{27}}
\newlabel{fig:overview-posenet}{{3.7}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Detectron2}{27}}
\citation{maskRCNN}
\citation{focalLossDetection}
\citation{cascadeRCNN}
\citation{panopticFeatures}
\citation{tensorMask}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Tipologie di analisi visive.\relax }}{28}}
\newlabel{fig:typeOfDetection}{{3.8}{28}}
\citation{maskRCNN}
\citation{focalLossDetection}
\citation{fasterRCNN}
\citation{fasterRCNN}
\citation{fastRCNN}
\citation{RFCN}
\citation{resNetXt}
\citation{resNet}
\citation{featurePyramid}
\citation{vgg16}
\citation{maskRCNN}
\citation{fasterRCNN}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Instance segmentation con Detectron2\relax }}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Mask R-CNN}{30}}
\citation{fastRCNN}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Alcuni esempi di instance segmentation con Mask R-CNN\relax }}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Predizione della posa con Mask R-CNN}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Alcuni esempi di pose prediction con Mask R-CNN\relax }}{32}}
\citation{NTURGB}
\citation{Detectron2Link}
\citation{PosenetArticle}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Metodo proposto}{33}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Estrazione delle pose}{33}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Diversi modelli in Detectron 2 per l'estrazione della posa da un'immagine. Il tempo di inferenza viene misurato in \emph  {secondi/immagine}; Box AP = ``Bounding box average precision" ; KP AP = ``Keypoint average precision"\relax }}{34}}
\newlabel{proposteDetectronPosenet}{{4.1}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Assegnazione delle pose}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Un esempio di assegnazione inconsistente delle pose lungo il video. Nell'esempio, l'ordine delle pose restituite dall'algoritmo \`e blu, rosso, verde e viola. Quest'ordine \`e indipendente dai soggetti inquadrati, che possono talvolta essere scambiati fra loro, non essere riconosciuti o addirittura scambiati per dei riflessi.\relax }}{35}}
\newlabel{fig:swapPose}{{4.1}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Rimozione degli zeri}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Esempio di rimozione degli zeri. Nei frame 2-3-4-5 i giunti del braccio sinistro non sono stati riconosciuti. Nei frame 2-3 verranno quindi rimpiazzati con quelli del frame 1, mentre nei frame 4-5 con quelli del frame 6.\relax }}{37}}
\newlabel{fig:rimozioneZeri}{{4.2}{37}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Il dataset NTU RGB+D}{40}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{NTURGB}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}NTU RGB+D in dettaglio}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Alcuni esempi di frames del dataset NTU RGB+D. Nelle prime 4 righe \`e possibile notare la variet\`a degli attori partecipanti al progetto e dei diversi punti di ripresa utilizzati. La quinta riga mostra la variet\`a intra-classe per una stessa azione. L'ultima riga mostra per uno stesso frame i valori RGB, i valori RGB + giunti, mappa di profondit\`a, mappa di profondit\`a + giunti e i valori a infrarossi. \relax }}{42}}
\newlabel{fig:varietaNTURGB}{{5.1}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces La configurazione dei 25 giunti utilizzati in NTU RGB+D. Le etichette utilizzate per i giunti sono: 1 - base della colonna vetrebrale, 2 - punto medio della colonna vertebrale, 3 - collo, 4 - testa, 5 - spalla sinistra, 6 - gomito sinistro, 7 - polso sinistro, 8 - mano sinistra, 9 - spalla destra, 10 - gomito destro, 11 - polso destro, 12 - mano destra, 13 - anca sinistra, 14 - ginocchio sinistro, 15 - caviglia sinistra, 16 - piede sinistro, 17 - anca destra, 18 - ginocchio destro, 19 - caviglia destra, 20 - piede destro, 21 - colonna vertebrale, 22 - punta della mano sinistra, 23 - pollice sinistro, 24 - punta della mano destra, 25 - pollice destro.\relax }}{43}}
\newlabel{fig:giuntiNTURGB}{{5.2}{43}}
\citation{NTURGB}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Disposizione delle telecamere per ogni setup. La telecamera 1 ha ripreso ogni azione da una posizione centrale mentre le telecamere 2 e 3 hanno ripreso le azioni con un'angolazione di $\ang {45}$. Ogni attore ha ripetuto l'azione 2 volte: una rivolto verso la telecamere di destra e una verso quella di sinistra. \relax }}{44}}
\newlabel{fig:schemaTelecamere}{{5.3}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Criteri di valutazione}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Valutazione Cross-subject}{44}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Altezza e distanza delle 3 telecamere per ogni setup di ripresa. Le altezze e le distanze sono espresse in metri. \relax }}{45}}
\newlabel{tab:puntiDivista}{{5.1}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Valutazione Cross-view}{45}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Esperimenti e risultati}{46}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusioni e sviluppi futuri}{47}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{PosenetArticle}{1}
\bibcite{COCO-2016}{2}
\bibcite{PosenetLink}{3}
\bibcite{Detectron2Link}{4}
\bibcite{DetectronLink}{5}
\bibcite{caffe2}{6}
\bibcite{maskRCNN}{7}
\bibcite{focalLossDetection}{8}
\bibcite{fasterRCNN}{9}
\bibcite{fastRCNN}{10}
\bibcite{RFCN}{11}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{48}}
\bibcite{resNetXt}{12}
\bibcite{resNet}{13}
\bibcite{featurePyramid}{14}
\bibcite{vgg16}{15}
\bibcite{cascadeRCNN}{16}
\bibcite{panopticFeatures}{17}
\bibcite{tensorMask}{18}
\bibcite{NTURGB}{19}
\bibcite{LSTM}{20}
\bibcite{peepholeLSTM}{21}
\bibcite{GRU}{22}
\bibcite{PREV1}{23}
\bibcite{PREV2}{24}
\bibcite{PREV3}{25}
\bibcite{TempConv}{26}
